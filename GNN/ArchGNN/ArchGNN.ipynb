{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Run this before starting\n",
    "!python -c \"import torch; print(torch.__version__)\"\n",
    "!python -c \"import torch; print(torch.version.cuda)\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch.nn import Parameter\n",
    "from torch_geometric.nn.conv import MessagePassing\n",
    "from torch_geometric.utils import remove_self_loops, add_self_loops, softmax\n",
    "import torch.optim as optim\n",
    "from torch_geometric.nn.conv import feast_conv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "May-19\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "versj = 1\n",
    "today = datetime.date.today().strftime(\"%b-%d\")\n",
    "print(today)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "versj =2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu.\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device_name = torch.device(\"cuda\")\n",
    "else:\n",
    "    device_name = torch.device('cpu')\n",
    "\n",
    "print(\"Using {}.\".format(device_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test for new dataset\n",
    "def testFinal(model, indata, device):\n",
    "    model.eval()\n",
    "    title = ''\n",
    "    E_A = 0\n",
    "    E_A_Ad = 0\n",
    "    E_A_Ad2 = 0\n",
    "    with torch.no_grad():\n",
    "        for id, data in enumerate(indata):\n",
    "          out = model(data.to(device))\n",
    "          real = data.y.to(device)\n",
    "          errorList = []\n",
    "          error = 0\n",
    "          epsilon = 1e-8\n",
    "          for i in range(len(out)):\n",
    "            e = ((out[i].item() - real[i].item() ) / (epsilon + out[i].item()))*100\n",
    "            errorList.append(e)\n",
    "\n",
    "          errorListAbs = [abs(x) for x in errorList]\n",
    "\n",
    "          errorListAbsAd2 = errorListAbs[8:12]\n",
    "\n",
    "          errorListAbs.sort()\n",
    "          errorListAbsAd = errorListAbs[:-2]\n",
    "\n",
    "          E_A_el = sum(errorListAbs)/len(errorListAbs)\n",
    "          E_A_Ad_el = sum(errorListAbsAd)/len(errorListAbsAd)\n",
    "          E_A_Ad_el2 = sum(errorListAbsAd2)/len(errorListAbsAd2)\n",
    "\n",
    "          E_A += E_A_el\n",
    "          E_A_Ad += E_A_Ad_el\n",
    "          E_A_Ad2 += E_A_Ad_el2\n",
    "          \n",
    "          print([round(x,2) for x in errorList])\n",
    "          print('Avarage Error: '+ str(round(E_A_el,2))+'%, Adjusted Avarage Error: '+ str(round(E_A_Ad_el,2))+'%')\n",
    "          print('Mean Square: '+ str(round(F.mse_loss(out, data.y.to(device)).item(),4))+'\\n')\n",
    "\n",
    "          #errorAvg += sum(errorListAbs)/len(errorListAbs)\n",
    "\n",
    "    print('Avarage Tot. Error: '+ str(round(E_A/len(indata),2))+'%\\n')\n",
    "    print('After removing the two worst errors. Avarage Adjusted Tot. Error: '+ str(round(E_A_Ad/len(indata),2))+'%')\n",
    "    print('Total error for nodes 8, 9, 10, 11. Avarage Adjusted Tot. Error: '+ str(round(E_A_Ad2/len(indata),2))+'%')\n",
    "   \n",
    "#Test for new dataline\n",
    "\n",
    "def testData(tester_model, datasetTest,line):\n",
    "    real = datasetTest[line].y\n",
    "    \n",
    "    out = tester_model(datasetTest[line].to(device_name))\n",
    "    Bool = ''\n",
    "    print('{:<10}{:<10}{:<10}{:<10}'.format('real', 'out', '%error', 'Sign'))\n",
    "    for i in range(len(out)):\n",
    "        error = ((out[i].item() - real[i].item())/ out[i] )*100\n",
    "        if torch.sign(real[i]) == torch.sign(out[i]):\n",
    "            Bool = 'True'\n",
    "        else:\n",
    "            Bool = 'False'\n",
    "        real_formatted = '{:<10.3f}'.format(round(real[i].item(),5))\n",
    "        out_formatted = '{:<10.3f}'.format(round(out[i].item(),5))\n",
    "        error_formatted = '{:<10.2f}'.format(error)\n",
    "        list_formatted = [real_formatted, out_formatted, error_formatted, Bool]\n",
    "        print('{:<10}{:<10}{:<10}{:<10}'.format(*list_formatted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Traindata\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "\n",
    "#Import data function\n",
    "def get_data(filepath):\n",
    "    data = np.loadtxt(filepath)\n",
    "    x = data[:, :20]\n",
    "    z = data[:, 20:40]\n",
    "    m = data[:, 40:60]\n",
    "\n",
    "    dataX = []\n",
    "    for i in range(x.shape[0]):\n",
    "        dataXs = np.column_stack((np.arange(20), x[i], z[i]))\n",
    "        dataX.append(dataXs)\n",
    "\n",
    "    dataY = m\n",
    "    #zipped = list(zip(dataX, dataY))\n",
    "    #np.random.shuffle(zipped)\n",
    "    #dataX, dataY = zip(*zipped)\n",
    "\n",
    "    dataX = np.array(dataX)\n",
    "    dataY = np.array(dataY)\n",
    "\n",
    "    dataEdgeIndex = np.column_stack((np.arange(20)[:-1], np.arange(20)[1:]))\n",
    "    dataEdgeIndex = np.vstack((dataEdgeIndex, dataEdgeIndex[:, ::-1])).T\n",
    "\n",
    "    dataset = [Data(x=torch.from_numpy(x).float(), edge_index=torch.from_numpy(dataEdgeIndex).long(), y=torch.from_numpy(y).float()) for x, y in zip(dataX, dataY)]\n",
    "\n",
    "    return dataset\n",
    "\n",
    "#Getting data\n",
    "datasetTrain = get_data(\"Data\\\\1DatasetTrain.txt\")\n",
    "datasetTest = get_data(\"Data\\\\1DatasetTest.txt\")\n",
    "datasetNew = get_data(\"Data\\\\1DatasetNew.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "4DatasetTrain.txt not found.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m datasetTrain4 \u001b[39m=\u001b[39m get_data(\u001b[39m\"\u001b[39;49m\u001b[39m4DatasetTrain.txt\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m      2\u001b[0m datasetNew4 \u001b[39m=\u001b[39m get_data(\u001b[39m\"\u001b[39m\u001b[39m4DatasetNew.txt\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m datasetTest4 \u001b[39m=\u001b[39m get_data(\u001b[39m\"\u001b[39m\u001b[39m4DatasetTest.txt\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[6], line 10\u001b[0m, in \u001b[0;36mget_data\u001b[1;34m(filepath)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_data\u001b[39m(filepath):\n\u001b[1;32m---> 10\u001b[0m     data \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mloadtxt(filepath)\n\u001b[0;32m     11\u001b[0m     x \u001b[39m=\u001b[39m data[:, :\u001b[39m20\u001b[39m]\n\u001b[0;32m     12\u001b[0m     z \u001b[39m=\u001b[39m data[:, \u001b[39m20\u001b[39m:\u001b[39m40\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\kt\\OneDrive - NTNU\\Master thesis\\03_Herman\\masters2023\\.venv\\Lib\\site-packages\\numpy\\lib\\npyio.py:1356\u001b[0m, in \u001b[0;36mloadtxt\u001b[1;34m(fname, dtype, comments, delimiter, converters, skiprows, usecols, unpack, ndmin, encoding, max_rows, quotechar, like)\u001b[0m\n\u001b[0;32m   1353\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(delimiter, \u001b[39mbytes\u001b[39m):\n\u001b[0;32m   1354\u001b[0m     delimiter \u001b[39m=\u001b[39m delimiter\u001b[39m.\u001b[39mdecode(\u001b[39m'\u001b[39m\u001b[39mlatin1\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m-> 1356\u001b[0m arr \u001b[39m=\u001b[39m _read(fname, dtype\u001b[39m=\u001b[39;49mdtype, comment\u001b[39m=\u001b[39;49mcomment, delimiter\u001b[39m=\u001b[39;49mdelimiter,\n\u001b[0;32m   1357\u001b[0m             converters\u001b[39m=\u001b[39;49mconverters, skiplines\u001b[39m=\u001b[39;49mskiprows, usecols\u001b[39m=\u001b[39;49musecols,\n\u001b[0;32m   1358\u001b[0m             unpack\u001b[39m=\u001b[39;49munpack, ndmin\u001b[39m=\u001b[39;49mndmin, encoding\u001b[39m=\u001b[39;49mencoding,\n\u001b[0;32m   1359\u001b[0m             max_rows\u001b[39m=\u001b[39;49mmax_rows, quote\u001b[39m=\u001b[39;49mquotechar)\n\u001b[0;32m   1361\u001b[0m \u001b[39mreturn\u001b[39;00m arr\n",
      "File \u001b[1;32mc:\\Users\\kt\\OneDrive - NTNU\\Master thesis\\03_Herman\\masters2023\\.venv\\Lib\\site-packages\\numpy\\lib\\npyio.py:975\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(fname, delimiter, comment, quote, imaginary_unit, usecols, skiplines, max_rows, converters, ndmin, unpack, dtype, encoding)\u001b[0m\n\u001b[0;32m    973\u001b[0m     fname \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mfspath(fname)\n\u001b[0;32m    974\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(fname, \u001b[39mstr\u001b[39m):\n\u001b[1;32m--> 975\u001b[0m     fh \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mlib\u001b[39m.\u001b[39;49m_datasource\u001b[39m.\u001b[39;49mopen(fname, \u001b[39m'\u001b[39;49m\u001b[39mrt\u001b[39;49m\u001b[39m'\u001b[39;49m, encoding\u001b[39m=\u001b[39;49mencoding)\n\u001b[0;32m    976\u001b[0m     \u001b[39mif\u001b[39;00m encoding \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    977\u001b[0m         encoding \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(fh, \u001b[39m'\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mlatin1\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\kt\\OneDrive - NTNU\\Master thesis\\03_Herman\\masters2023\\.venv\\Lib\\site-packages\\numpy\\lib\\_datasource.py:193\u001b[0m, in \u001b[0;36mopen\u001b[1;34m(path, mode, destpath, encoding, newline)\u001b[0m\n\u001b[0;32m    156\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    157\u001b[0m \u001b[39mOpen `path` with `mode` and return the file object.\u001b[39;00m\n\u001b[0;32m    158\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    189\u001b[0m \n\u001b[0;32m    190\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    192\u001b[0m ds \u001b[39m=\u001b[39m DataSource(destpath)\n\u001b[1;32m--> 193\u001b[0m \u001b[39mreturn\u001b[39;00m ds\u001b[39m.\u001b[39;49mopen(path, mode, encoding\u001b[39m=\u001b[39;49mencoding, newline\u001b[39m=\u001b[39;49mnewline)\n",
      "File \u001b[1;32mc:\\Users\\kt\\OneDrive - NTNU\\Master thesis\\03_Herman\\masters2023\\.venv\\Lib\\site-packages\\numpy\\lib\\_datasource.py:533\u001b[0m, in \u001b[0;36mDataSource.open\u001b[1;34m(self, path, mode, encoding, newline)\u001b[0m\n\u001b[0;32m    530\u001b[0m     \u001b[39mreturn\u001b[39;00m _file_openers[ext](found, mode\u001b[39m=\u001b[39mmode,\n\u001b[0;32m    531\u001b[0m                               encoding\u001b[39m=\u001b[39mencoding, newline\u001b[39m=\u001b[39mnewline)\n\u001b[0;32m    532\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 533\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mFileNotFoundError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mpath\u001b[39m}\u001b[39;00m\u001b[39m not found.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: 4DatasetTrain.txt not found."
     ]
    }
   ],
   "source": [
    "datasetTrain4 = get_data(\"4DatasetTrain.txt\")\n",
    "datasetNew4 = get_data(\"4DatasetNew.txt\")\n",
    "datasetTest4 = get_data(\"4DatasetTest.txt\")\n",
    "datasetTest2 = get_data(\"data2test.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "1001\n",
      "1000\n",
      "250\n",
      "torch.Size([2, 38])\n",
      "torch.Size([20, 3])\n",
      "torch.Size([20])\n"
     ]
    }
   ],
   "source": [
    "print(len(datasetNew))\n",
    "print(len(datasetTest))\n",
    "\n",
    "#Splitting dataset\n",
    "\n",
    "train_loader = datasetTrain[:1000]\n",
    "test_loader = datasetTrain[-250:]\n",
    "\n",
    "print(len(train_loader))\n",
    "print(len(test_loader))\n",
    "\n",
    "print(datasetTrain[10].edge_index.shape)\n",
    "print(datasetTrain[10].x.shape)\n",
    "print(datasetTrain[10].y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArchNN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, num_classes, heads, t_inv = True):\n",
    "        super(ArchNN, self).__init__()\n",
    "        self.fc0 = nn.Linear(in_channels, 16)\n",
    "        self.conv1 = feast_conv.FeaStConv(16, 32, heads=heads, t_inv=t_inv)\n",
    "        self.conv2 = feast_conv.FeaStConv(32, 64, heads=heads, t_inv=t_inv)\n",
    "        self.conv3 = feast_conv.FeaStConv(64, 128, heads=heads, t_inv=t_inv)\n",
    "        self.conv4 = feast_conv.FeaStConv(128, 256, heads=heads, t_inv=t_inv)\n",
    "        self.conv5 = feast_conv.FeaStConv(256, 128, heads=heads, t_inv=t_inv)\n",
    "        self.fc1 = nn.Linear(128, 64)\n",
    "        self.fc2 = nn.Linear(64, 1)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.conv1.reset_parameters()\n",
    "        self.conv2.reset_parameters()\n",
    "        self.conv3.reset_parameters()\n",
    "        self.conv4.reset_parameters()\n",
    "        self.conv5.reset_parameters()\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = F.leaky_relu(self.fc0(x))\n",
    "        x = F.leaky_relu(self.conv1(x, edge_index))\n",
    "        x = F.leaky_relu(self.conv2(x, edge_index))\n",
    "        x = F.leaky_relu(self.conv3(x, edge_index))\n",
    "        x = F.leaky_relu(self.conv4(x, edge_index))\n",
    "        x = F.leaky_relu(self.conv5(x, edge_index))\n",
    "        x = F.leaky_relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        x = torch.squeeze(x, dim=1)\n",
    "        return x\n",
    "\n",
    "class ArchNN2(torch.nn.Module):\n",
    "    def __init__(self, in_channels, num_classes, heads, t_inv = True):\n",
    "        super(ArchNN2, self).__init__()\n",
    "        self.fc0 = nn.Linear(in_channels, 16)\n",
    "        self.conv1 = feast_conv.FeaStConv(16, 32, heads=heads, t_inv=t_inv)\n",
    "        self.conv2 = feast_conv.FeaStConv(32, 64, heads=heads, t_inv=t_inv)\n",
    "        self.conv3 = feast_conv.FeaStConv(64, 128, heads=heads, t_inv=t_inv)\n",
    "        self.conv4 = feast_conv.FeaStConv(128, 256, heads=heads, t_inv=t_inv)\n",
    "        self.conv5 = feast_conv.FeaStConv(256, 128, heads=heads, t_inv=t_inv)\n",
    "        self.fc1 = nn.Linear(128, 64)\n",
    "        self.fc2 = nn.Linear(64, 1)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.conv1.reset_parameters()\n",
    "        self.conv2.reset_parameters()\n",
    "        self.conv3.reset_parameters()\n",
    "        self.conv4.reset_parameters()\n",
    "        self.conv5.reset_parameters()\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = F.relu(self.fc0(x))\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.relu(self.conv2(x, edge_index))\n",
    "        x = F.relu(self.conv3(x, edge_index))\n",
    "        x = F.relu(self.conv4(x, edge_index))\n",
    "        x = F.relu(self.conv5(x, edge_index))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        x = torch.squeeze(x, dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef train(model, train_loader, optimizer, device):\\n    model.train()\\n\\n    total_loss = 0\\n    for idx, data in enumerate(train_loader):\\n        optimizer.zero_grad()\\n        output = model(data.to(device))\\n        errors = torch.abs(output - data.y.to(device))\\n        error_percentage = (errors / (output))\\n        mean_error_percentage = error_percentage.mean()\\n        mean_error_percentage.backward(retain_graph=True)\\n        optimizer.step()\\n        total_loss += mean_error_percentage\\n        \\n    return total_loss / len(train_loader)\\n\\ndef test(model, test_loader, num_nodes, device):\\n    model.eval()\\n    correct = 0\\n    total_loss = 0\\n    n_graphs = 0\\n    with torch.no_grad():\\n        for idx, data in enumerate(test_loader):\\n            output = model(data.to(device))\\n            errors = torch.abs(output - data.y.to(device))\\n            error_percentage = (errors / (output))\\n            mean_error_percentage = error_percentage.mean()\\n            total_loss += mean_error_percentage\\n    return total_loss / len(test_loader)'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def print_info(info):\n",
    "    message = ('Epoch: {}/{}, Duration: {:.3f}s,'\n",
    "               'Train Loss: {:.4f}, Test Loss:{:.4f}').format(\n",
    "                   info['current_epoch'], info['epochs'], info['t_duration'],\n",
    "                   info['train_loss'], info['test_loss'])\n",
    "    print(message)\n",
    "\n",
    "\n",
    "def run(model, train_loader, test_loader, num_nodes, epochs, optimizer, device):\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        t = time.time()\n",
    "        train_loss = train(model, train_loader, optimizer, device)\n",
    "        t_duration = time.time() - t\n",
    "        test_loss = test(model, test_loader, num_nodes, device)\n",
    "        eval_info = {\n",
    "            'train_loss': train_loss,\n",
    "            'test_loss': test_loss,\n",
    "            'current_epoch': epoch,\n",
    "            'epochs': epochs,\n",
    "            't_duration': t_duration\n",
    "        }\n",
    "\n",
    "        print_info(eval_info)\n",
    "        train_losses.append(train_loss)\n",
    "        test_losses.append(test_loss)\n",
    "    return train_losses, test_losses\n",
    "\n",
    "\n",
    "\n",
    "def train(model, train_loader, optimizer, device):\n",
    "    model.train()\n",
    "\n",
    "    total_loss = 0\n",
    "    for idx, data in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data.to(device))\n",
    "        losses = F.mse_loss(output, data.y.to(device), reduction='none')  # Calculate MSE loss for each item in output and labels\n",
    "        loss = losses.mean()  # Compute mean loss for backpropagation\n",
    "        loss.backward(retain_graph=True)\n",
    "        #loss = model.compute_loss(output, data.y)  # compute loss with L1 regularization\n",
    "        #loss = F.nll_loss(log_probs, data.y)\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "\n",
    "def test(model, test_loader, num_nodes, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total_loss = 0\n",
    "    n_graphs = 0\n",
    "    with torch.no_grad():\n",
    "        for idx, data in enumerate(test_loader):\n",
    "            out = model(data.to(device))\n",
    "            #total_loss += F.nll_loss(out, data.y.to(device)).item()\n",
    "            total_loss += F.mse_loss(out, data.y.to(device)).item()\n",
    "            #pred = out.max(1)[1]\n",
    "            #correct += pred.eq(data.y).sum().item()\n",
    "            #n_graphs += data.num_graphs\n",
    "    return total_loss / len(test_loader)\n",
    "\"\"\"\n",
    "def train(model, train_loader, optimizer, device):\n",
    "    model.train()\n",
    "\n",
    "    total_loss = 0\n",
    "    for idx, data in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data.to(device))\n",
    "        errors = torch.abs(output - data.y.to(device))\n",
    "        error_percentage = (errors / (output))\n",
    "        mean_error_percentage = error_percentage.mean()\n",
    "        mean_error_percentage.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "        total_loss += mean_error_percentage\n",
    "        \n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "def test(model, test_loader, num_nodes, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total_loss = 0\n",
    "    n_graphs = 0\n",
    "    with torch.no_grad():\n",
    "        for idx, data in enumerate(test_loader):\n",
    "            output = model(data.to(device))\n",
    "            errors = torch.abs(output - data.y.to(device))\n",
    "            error_percentage = (errors / (output))\n",
    "            mean_error_percentage = error_percentage.mean()\n",
    "            total_loss += mean_error_percentage\n",
    "    return total_loss / len(test_loader)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/100, Duration: 17.906s,Train Loss: 4.8944, Test Loss:3.9238\n",
      "Epoch: 2/100, Duration: 19.364s,Train Loss: 3.2756, Test Loss:3.7240\n",
      "Epoch: 3/100, Duration: 18.733s,Train Loss: 3.0029, Test Loss:3.2959\n",
      "Epoch: 4/100, Duration: 22.481s,Train Loss: 2.8255, Test Loss:3.0820\n",
      "Epoch: 5/100, Duration: 22.077s,Train Loss: 2.7175, Test Loss:3.0337\n",
      "Epoch: 6/100, Duration: 24.844s,Train Loss: 2.6483, Test Loss:2.7533\n",
      "Epoch: 7/100, Duration: 21.259s,Train Loss: 2.3036, Test Loss:2.2901\n",
      "Epoch: 8/100, Duration: 19.854s,Train Loss: 2.0360, Test Loss:1.7537\n",
      "Epoch: 9/100, Duration: 19.622s,Train Loss: 1.5629, Test Loss:1.3540\n",
      "Epoch: 10/100, Duration: 20.447s,Train Loss: 1.0485, Test Loss:0.9857\n",
      "Epoch: 11/100, Duration: 19.644s,Train Loss: 0.6520, Test Loss:0.6751\n",
      "Epoch: 12/100, Duration: 21.257s,Train Loss: 0.4701, Test Loss:0.4826\n",
      "Epoch: 13/100, Duration: 24.168s,Train Loss: 0.3915, Test Loss:0.3410\n",
      "Epoch: 14/100, Duration: 22.079s,Train Loss: 0.3185, Test Loss:0.2541\n",
      "Epoch: 15/100, Duration: 18.933s,Train Loss: 0.2369, Test Loss:0.1650\n",
      "Epoch: 16/100, Duration: 17.636s,Train Loss: 0.1839, Test Loss:0.1155\n",
      "Epoch: 17/100, Duration: 17.592s,Train Loss: 0.1580, Test Loss:0.0960\n",
      "Epoch: 18/100, Duration: 17.731s,Train Loss: 0.1131, Test Loss:0.0800\n",
      "Epoch: 19/100, Duration: 17.442s,Train Loss: 0.1152, Test Loss:0.0778\n",
      "Epoch: 20/100, Duration: 17.613s,Train Loss: 0.0958, Test Loss:0.0558\n",
      "Epoch: 21/100, Duration: 18.108s,Train Loss: 0.1005, Test Loss:0.0585\n",
      "Epoch: 22/100, Duration: 19.944s,Train Loss: 0.0878, Test Loss:0.0568\n",
      "Epoch: 23/100, Duration: 20.578s,Train Loss: 0.0810, Test Loss:0.0561\n",
      "Epoch: 24/100, Duration: 23.083s,Train Loss: 0.0848, Test Loss:0.0724\n",
      "Epoch: 25/100, Duration: 24.519s,Train Loss: 0.0856, Test Loss:0.0429\n",
      "Epoch: 26/100, Duration: 22.082s,Train Loss: 0.0790, Test Loss:0.0388\n",
      "Epoch: 27/100, Duration: 17.663s,Train Loss: 0.0738, Test Loss:0.0360\n",
      "Epoch: 28/100, Duration: 17.709s,Train Loss: 0.0649, Test Loss:0.0335\n",
      "Epoch: 29/100, Duration: 17.910s,Train Loss: 0.0688, Test Loss:0.0330\n",
      "Epoch: 30/100, Duration: 18.352s,Train Loss: 0.0667, Test Loss:0.0266\n",
      "Epoch: 31/100, Duration: 20.886s,Train Loss: 0.0686, Test Loss:0.0256\n",
      "Epoch: 32/100, Duration: 20.564s,Train Loss: 0.0735, Test Loss:0.0253\n",
      "Epoch: 33/100, Duration: 17.724s,Train Loss: 0.0582, Test Loss:0.0215\n",
      "Epoch: 34/100, Duration: 17.628s,Train Loss: 0.0717, Test Loss:0.0196\n",
      "Epoch: 35/100, Duration: 17.924s,Train Loss: 0.0581, Test Loss:0.0193\n",
      "Epoch: 36/100, Duration: 17.501s,Train Loss: 0.0623, Test Loss:0.0183\n",
      "Epoch: 37/100, Duration: 17.532s,Train Loss: 0.0634, Test Loss:0.0168\n",
      "Epoch: 38/100, Duration: 17.855s,Train Loss: 0.0554, Test Loss:0.0172\n",
      "Epoch: 39/100, Duration: 18.631s,Train Loss: 0.0558, Test Loss:0.0147\n",
      "Epoch: 40/100, Duration: 20.746s,Train Loss: 0.0543, Test Loss:0.0146\n",
      "Epoch: 41/100, Duration: 20.620s,Train Loss: 0.0719, Test Loss:0.0218\n",
      "Epoch: 42/100, Duration: 18.653s,Train Loss: 0.0411, Test Loss:0.0209\n",
      "Epoch: 43/100, Duration: 17.413s,Train Loss: 0.0567, Test Loss:0.0139\n",
      "Epoch: 44/100, Duration: 17.588s,Train Loss: 0.0563, Test Loss:0.0118\n",
      "Epoch: 45/100, Duration: 17.705s,Train Loss: 0.0508, Test Loss:0.0108\n",
      "Epoch: 46/100, Duration: 17.397s,Train Loss: 0.0456, Test Loss:0.0103\n",
      "Epoch: 47/100, Duration: 17.310s,Train Loss: 0.0510, Test Loss:0.0181\n",
      "Epoch: 48/100, Duration: 17.583s,Train Loss: 0.0755, Test Loss:0.0125\n",
      "Epoch: 49/100, Duration: 19.540s,Train Loss: 0.0408, Test Loss:0.0169\n",
      "Epoch: 50/100, Duration: 24.261s,Train Loss: 0.0686, Test Loss:0.0153\n",
      "Epoch: 51/100, Duration: 22.214s,Train Loss: 0.0427, Test Loss:0.0093\n",
      "Epoch: 52/100, Duration: 20.583s,Train Loss: 0.0514, Test Loss:0.0116\n",
      "Epoch: 53/100, Duration: 21.168s,Train Loss: 0.0505, Test Loss:0.0148\n",
      "Epoch: 54/100, Duration: 21.529s,Train Loss: 0.0466, Test Loss:0.0185\n",
      "Epoch: 55/100, Duration: 19.689s,Train Loss: 0.0449, Test Loss:0.0181\n",
      "Epoch: 56/100, Duration: 19.035s,Train Loss: 0.0491, Test Loss:0.0225\n",
      "Epoch: 57/100, Duration: 18.812s,Train Loss: 0.0417, Test Loss:0.0181\n",
      "Epoch: 58/100, Duration: 19.146s,Train Loss: 0.0435, Test Loss:0.0186\n",
      "Epoch: 59/100, Duration: 19.642s,Train Loss: 0.0412, Test Loss:0.0230\n",
      "Epoch: 60/100, Duration: 20.910s,Train Loss: 0.0426, Test Loss:0.0297\n",
      "Epoch: 61/100, Duration: 21.894s,Train Loss: 0.0414, Test Loss:0.0381\n",
      "Epoch: 62/100, Duration: 20.435s,Train Loss: 0.0374, Test Loss:0.0158\n",
      "Epoch: 63/100, Duration: 20.591s,Train Loss: 0.0418, Test Loss:0.0669\n",
      "Epoch: 64/100, Duration: 21.511s,Train Loss: 0.0214, Test Loss:0.0200\n",
      "Epoch: 65/100, Duration: 18.542s,Train Loss: 0.0493, Test Loss:0.0176\n",
      "Epoch: 66/100, Duration: 18.546s,Train Loss: 0.0336, Test Loss:0.0301\n",
      "Epoch: 67/100, Duration: 20.569s,Train Loss: 0.0388, Test Loss:0.0622\n",
      "Epoch: 68/100, Duration: 20.767s,Train Loss: 0.0313, Test Loss:0.0070\n",
      "Epoch: 69/100, Duration: 19.564s,Train Loss: 0.0460, Test Loss:0.0349\n",
      "Epoch: 70/100, Duration: 21.843s,Train Loss: 0.0188, Test Loss:0.0094\n",
      "Epoch: 71/100, Duration: 20.613s,Train Loss: 0.0445, Test Loss:0.0137\n",
      "Epoch: 72/100, Duration: 20.687s,Train Loss: 0.0217, Test Loss:0.0144\n",
      "Epoch: 73/100, Duration: 19.799s,Train Loss: 0.0404, Test Loss:0.0208\n",
      "Epoch: 74/100, Duration: 18.286s,Train Loss: 0.0261, Test Loss:0.0077\n",
      "Epoch: 75/100, Duration: 18.608s,Train Loss: 0.0367, Test Loss:0.0346\n",
      "Epoch: 76/100, Duration: 19.293s,Train Loss: 0.0258, Test Loss:0.0429\n",
      "Epoch: 77/100, Duration: 17.853s,Train Loss: 0.0304, Test Loss:0.0337\n",
      "Epoch: 78/100, Duration: 19.125s,Train Loss: 0.0295, Test Loss:0.0338\n",
      "Epoch: 79/100, Duration: 19.462s,Train Loss: 0.0262, Test Loss:0.0153\n",
      "Epoch: 80/100, Duration: 19.077s,Train Loss: 0.0353, Test Loss:0.0546\n",
      "Epoch: 81/100, Duration: 19.668s,Train Loss: 0.0212, Test Loss:0.0094\n",
      "Epoch: 82/100, Duration: 19.587s,Train Loss: 0.0349, Test Loss:0.0224\n",
      "Epoch: 83/100, Duration: 18.655s,Train Loss: 0.0241, Test Loss:0.0054\n",
      "Epoch: 84/100, Duration: 18.343s,Train Loss: 0.0350, Test Loss:0.0182\n",
      "Epoch: 85/100, Duration: 17.946s,Train Loss: 0.0133, Test Loss:0.0158\n",
      "Epoch: 86/100, Duration: 19.165s,Train Loss: 0.0382, Test Loss:0.0173\n",
      "Epoch: 87/100, Duration: 18.816s,Train Loss: 0.0200, Test Loss:0.0054\n",
      "Epoch: 88/100, Duration: 20.094s,Train Loss: 0.0353, Test Loss:0.0182\n",
      "Epoch: 89/100, Duration: 19.080s,Train Loss: 0.0162, Test Loss:0.0258\n",
      "Epoch: 90/100, Duration: 25.788s,Train Loss: 0.0423, Test Loss:0.0143\n",
      "Epoch: 91/100, Duration: 23.749s,Train Loss: 0.0161, Test Loss:0.0140\n",
      "Epoch: 92/100, Duration: 21.769s,Train Loss: 0.0352, Test Loss:0.0192\n",
      "Epoch: 93/100, Duration: 18.965s,Train Loss: 0.0201, Test Loss:0.0247\n",
      "Epoch: 94/100, Duration: 20.000s,Train Loss: 0.0314, Test Loss:0.0155\n",
      "Epoch: 95/100, Duration: 20.032s,Train Loss: 0.0185, Test Loss:0.0206\n",
      "Epoch: 96/100, Duration: 21.990s,Train Loss: 0.0317, Test Loss:0.0255\n",
      "Epoch: 97/100, Duration: 19.791s,Train Loss: 0.0132, Test Loss:0.0171\n",
      "Epoch: 98/100, Duration: 25.620s,Train Loss: 0.0350, Test Loss:0.0270\n",
      "Epoch: 99/100, Duration: 23.371s,Train Loss: 0.0177, Test Loss:0.0096\n",
      "Epoch: 100/100, Duration: 21.440s,Train Loss: 0.0292, Test Loss:0.0113\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "expected str, bytes or os.PathLike object, not list",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 19\u001b[0m\n\u001b[0;32m     17\u001b[0m testLossPath \u001b[39m=\u001b[39m today \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mArchGNNVersion\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(versj) \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mTestLoss.txt\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     18\u001b[0m np\u001b[39m.\u001b[39msavetxt(trainLossPath, np\u001b[39m.\u001b[39marray(train_losses))\n\u001b[1;32m---> 19\u001b[0m np\u001b[39m.\u001b[39msavetxt(testLossPath, np\u001b[39m.\u001b[39;49mload(test_losses))\n\u001b[0;32m     20\u001b[0m torch\u001b[39m.\u001b[39msave(model1\u001b[39m.\u001b[39mstate_dict(), modelPath)\n\u001b[0;32m     21\u001b[0m versj \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\kt\\OneDrive - NTNU\\Master thesis\\03_Herman\\masters2023\\.venv\\Lib\\site-packages\\numpy\\lib\\npyio.py:405\u001b[0m, in \u001b[0;36mload\u001b[1;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[0;32m    403\u001b[0m     own_fid \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    404\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 405\u001b[0m     fid \u001b[39m=\u001b[39m stack\u001b[39m.\u001b[39menter_context(\u001b[39mopen\u001b[39m(os_fspath(file), \u001b[39m\"\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[0;32m    406\u001b[0m     own_fid \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    408\u001b[0m \u001b[39m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: expected str, bytes or os.PathLike object, not list"
     ]
    }
   ],
   "source": [
    "#runner\n",
    "num_nodes = train_loader[0].x.shape[0]\n",
    "num_features = train_loader[0].x.shape[1]\n",
    "\n",
    "#leaky_relu\n",
    "model1 = ArchNN(num_features, num_nodes, heads=8).to(device_name)\n",
    "\n",
    "optimizer = optim.Adam(model1.parameters(),\n",
    "                       lr=0.0001)\n",
    "\n",
    "train_losses, test_losses = \\\n",
    "run(model1, train_loader, test_loader, num_nodes, 100, optimizer, device_name)\n",
    "\n",
    "#versj defined at the top of the notebook\n",
    "modelPath = \"trained_models\\\\\" + today + \"ArchGNNVersion\" + str(versj) +\".pt\"\n",
    "trainLossPath = \"trained_models\\\\\" + today + \"ArchGNNVersion\" + str(versj) + \"TrainLoss.txt\"\n",
    "testLossPath = \"trained_models\\\\\" + today + \"ArchGNNVersion\" + str(versj) + \"TestLoss.txt\"\n",
    "np.savetxt(trainLossPath, np.array(train_losses))\n",
    "np.savetxt(testLossPath, np.load(test_losses))\n",
    "torch.save(model1.state_dict(), modelPath)\n",
    "versj += 1\n",
    "print(\"saved as: \", modelPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved as:  May-19ArchGNNVersion1.pt\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"saved as: May-19ArchGNNVersion1.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/100, Duration: 20.105s,Train Loss: 5.4293, Test Loss:4.4863\n",
      "Epoch: 2/100, Duration: 24.151s,Train Loss: 3.6564, Test Loss:3.7175\n",
      "Epoch: 3/100, Duration: 21.958s,Train Loss: 3.1565, Test Loss:3.3926\n",
      "Epoch: 4/100, Duration: 21.947s,Train Loss: 2.9077, Test Loss:3.1633\n",
      "Epoch: 5/100, Duration: 21.081s,Train Loss: 2.7485, Test Loss:2.9449\n",
      "Epoch: 6/100, Duration: 19.851s,Train Loss: 2.5428, Test Loss:2.6715\n",
      "Epoch: 7/100, Duration: 25.160s,Train Loss: 2.1954, Test Loss:2.0479\n",
      "Epoch: 8/100, Duration: 24.894s,Train Loss: 1.3667, Test Loss:1.0465\n",
      "Epoch: 9/100, Duration: 27.326s,Train Loss: 0.7322, Test Loss:0.5538\n",
      "Epoch: 10/100, Duration: 30.844s,Train Loss: 0.4431, Test Loss:0.3601\n",
      "Epoch: 11/100, Duration: 27.563s,Train Loss: 0.3092, Test Loss:0.2376\n",
      "Epoch: 12/100, Duration: 26.324s,Train Loss: 0.2445, Test Loss:0.1744\n",
      "Epoch: 13/100, Duration: 24.663s,Train Loss: 0.1955, Test Loss:0.1299\n",
      "Epoch: 14/100, Duration: 23.615s,Train Loss: 0.1596, Test Loss:0.0969\n",
      "Epoch: 15/100, Duration: 21.440s,Train Loss: 0.1441, Test Loss:0.0772\n",
      "Epoch: 16/100, Duration: 21.788s,Train Loss: 0.1388, Test Loss:0.0668\n",
      "Epoch: 17/100, Duration: 24.221s,Train Loss: 0.1131, Test Loss:0.0539\n",
      "Epoch: 18/100, Duration: 22.392s,Train Loss: 0.1169, Test Loss:0.0499\n",
      "Epoch: 19/100, Duration: 21.933s,Train Loss: 0.1017, Test Loss:0.0439\n",
      "Epoch: 20/100, Duration: 22.081s,Train Loss: 0.1086, Test Loss:0.0401\n",
      "Epoch: 21/100, Duration: 21.424s,Train Loss: 0.1100, Test Loss:0.0390\n",
      "Epoch: 22/100, Duration: 21.593s,Train Loss: 0.1063, Test Loss:0.0425\n",
      "Epoch: 23/100, Duration: 21.711s,Train Loss: 0.1060, Test Loss:0.0408\n",
      "Epoch: 24/100, Duration: 21.882s,Train Loss: 0.0930, Test Loss:0.0359\n",
      "Epoch: 25/100, Duration: 21.692s,Train Loss: 0.0930, Test Loss:0.0322\n",
      "Epoch: 26/100, Duration: 21.756s,Train Loss: 0.0883, Test Loss:0.0313\n",
      "Epoch: 27/100, Duration: 23.368s,Train Loss: 0.0828, Test Loss:0.0264\n",
      "Epoch: 28/100, Duration: 22.201s,Train Loss: 0.0818, Test Loss:0.0262\n",
      "Epoch: 29/100, Duration: 28.221s,Train Loss: 0.0764, Test Loss:0.0246\n",
      "Epoch: 30/100, Duration: 27.742s,Train Loss: 0.0753, Test Loss:0.0237\n",
      "Epoch: 31/100, Duration: 27.998s,Train Loss: 0.0693, Test Loss:0.0217\n",
      "Epoch: 32/100, Duration: 44.235s,Train Loss: 0.0682, Test Loss:0.0222\n",
      "Epoch: 33/100, Duration: 48.159s,Train Loss: 0.0646, Test Loss:0.0204\n",
      "Epoch: 34/100, Duration: 42.414s,Train Loss: 0.0648, Test Loss:0.0209\n",
      "Epoch: 35/100, Duration: 41.887s,Train Loss: 0.0581, Test Loss:0.0196\n",
      "Epoch: 36/100, Duration: 37.386s,Train Loss: 0.0592, Test Loss:0.0203\n",
      "Epoch: 37/100, Duration: 37.337s,Train Loss: 0.0585, Test Loss:0.0187\n",
      "Epoch: 38/100, Duration: 39.328s,Train Loss: 0.0566, Test Loss:0.0193\n",
      "Epoch: 39/100, Duration: 41.290s,Train Loss: 0.0553, Test Loss:0.0198\n",
      "Epoch: 40/100, Duration: 38.196s,Train Loss: 0.0569, Test Loss:0.0192\n",
      "Epoch: 41/100, Duration: 37.744s,Train Loss: 0.0547, Test Loss:0.0195\n",
      "Epoch: 42/100, Duration: 37.552s,Train Loss: 0.0521, Test Loss:0.0191\n",
      "Epoch: 43/100, Duration: 42.385s,Train Loss: 0.0537, Test Loss:0.0197\n",
      "Epoch: 44/100, Duration: 40.799s,Train Loss: 0.0541, Test Loss:0.0171\n",
      "Epoch: 45/100, Duration: 37.800s,Train Loss: 0.0506, Test Loss:0.0164\n",
      "Epoch: 46/100, Duration: 38.583s,Train Loss: 0.0500, Test Loss:0.0165\n",
      "Epoch: 47/100, Duration: 41.204s,Train Loss: 0.0494, Test Loss:0.0166\n",
      "Epoch: 48/100, Duration: 42.305s,Train Loss: 0.0472, Test Loss:0.0160\n",
      "Epoch: 49/100, Duration: 40.094s,Train Loss: 0.0489, Test Loss:0.0162\n",
      "Epoch: 50/100, Duration: 49.095s,Train Loss: 0.0465, Test Loss:0.0165\n",
      "Epoch: 51/100, Duration: 45.914s,Train Loss: 0.0449, Test Loss:0.0165\n",
      "Epoch: 52/100, Duration: 46.216s,Train Loss: 0.0454, Test Loss:0.0177\n",
      "Epoch: 53/100, Duration: 50.805s,Train Loss: 0.0435, Test Loss:0.0159\n",
      "Epoch: 54/100, Duration: 45.669s,Train Loss: 0.0456, Test Loss:0.0170\n",
      "Epoch: 55/100, Duration: 42.217s,Train Loss: 0.0423, Test Loss:0.0158\n",
      "Epoch: 56/100, Duration: 42.147s,Train Loss: 0.0446, Test Loss:0.0170\n",
      "Epoch: 57/100, Duration: 41.774s,Train Loss: 0.0409, Test Loss:0.0161\n",
      "Epoch: 58/100, Duration: 42.754s,Train Loss: 0.0382, Test Loss:0.0169\n",
      "Epoch: 59/100, Duration: 39.755s,Train Loss: 0.0432, Test Loss:0.0180\n",
      "Epoch: 60/100, Duration: 38.866s,Train Loss: 0.0431, Test Loss:0.0164\n",
      "Epoch: 61/100, Duration: 39.066s,Train Loss: 0.0418, Test Loss:0.0163\n",
      "Epoch: 62/100, Duration: 39.229s,Train Loss: 0.0415, Test Loss:0.0159\n",
      "Epoch: 63/100, Duration: 50.515s,Train Loss: 0.0422, Test Loss:0.0138\n",
      "Epoch: 64/100, Duration: 49.530s,Train Loss: 0.0387, Test Loss:0.0154\n",
      "Epoch: 65/100, Duration: 43.051s,Train Loss: 0.0408, Test Loss:0.0139\n",
      "Epoch: 66/100, Duration: 42.705s,Train Loss: 0.0417, Test Loss:0.0139\n",
      "Epoch: 67/100, Duration: 39.801s,Train Loss: 0.0391, Test Loss:0.0130\n",
      "Epoch: 68/100, Duration: 38.991s,Train Loss: 0.0389, Test Loss:0.0129\n",
      "Epoch: 69/100, Duration: 39.236s,Train Loss: 0.0359, Test Loss:0.0141\n",
      "Epoch: 70/100, Duration: 38.687s,Train Loss: 0.0359, Test Loss:0.0127\n",
      "Epoch: 71/100, Duration: 38.478s,Train Loss: 0.0347, Test Loss:0.0121\n",
      "Epoch: 72/100, Duration: 38.644s,Train Loss: 0.0356, Test Loss:0.0123\n",
      "Epoch: 73/100, Duration: 38.807s,Train Loss: 0.0359, Test Loss:0.0130\n",
      "Epoch: 74/100, Duration: 32.092s,Train Loss: 0.0338, Test Loss:0.0123\n",
      "Epoch: 75/100, Duration: 27.656s,Train Loss: 0.0328, Test Loss:0.0113\n",
      "Epoch: 76/100, Duration: 29.547s,Train Loss: 0.0424, Test Loss:0.0134\n",
      "Epoch: 77/100, Duration: 26.117s,Train Loss: 0.0269, Test Loss:0.0175\n",
      "Epoch: 78/100, Duration: 23.649s,Train Loss: 0.0335, Test Loss:0.0217\n",
      "Epoch: 79/100, Duration: 23.609s,Train Loss: 0.0327, Test Loss:0.0209\n",
      "Epoch: 80/100, Duration: 23.761s,Train Loss: 0.0329, Test Loss:0.0134\n",
      "Epoch: 81/100, Duration: 23.489s,Train Loss: 0.0310, Test Loss:0.0164\n",
      "Epoch: 82/100, Duration: 25.546s,Train Loss: 0.0324, Test Loss:0.0148\n"
     ]
    }
   ],
   "source": [
    "#runner\n",
    "num_nodes = train_loader[0].x.shape[0]\n",
    "num_features = train_loader[0].x.shape[1]\n",
    "\n",
    "#relu\n",
    "model2 = ArchNN2(num_features, num_nodes, heads=8).to(device_name)\n",
    "\n",
    "optimizer = optim.Adam(model2.parameters(),\n",
    "                       lr=0.0001)\n",
    "\n",
    "train_losses, test_losses = \\\n",
    "run(model2, train_loader, test_loader, num_nodes, 100, optimizer, device_name)\n",
    "\n",
    "#versj defined at the top of the notebook\n",
    "modelPath = \"trained_models\\\\\" + today + \"ArchGNNReLUVersion\" + str(versj) +\".pt\"\n",
    "trainLossPath = \"trained_models\\\\\" + today + \"ArchGNNReLUVersion\" + str(versj) + \"TrainLoss.txt\"\n",
    "testLossPath = \"trained_models\\\\\" + today + \"ArchGNNReLUVersion\" + str(versj) + \"TestLoss.txt\"\n",
    "np.savetxt(trainLossPath, np.array(train_losses))\n",
    "np.savetxt(testLossPath, np.array(test_losses))\n",
    "torch.save(model1.state_dict(), modelPath) #this is wrong!! remember after\n",
    "versj += 1\n",
    "print(\"saved as: \", modelPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#best one with 1dataset train 4000 and test 1000, Apr-25ver2.pt\n",
    "\n",
    "class Arch2NN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, num_classes, heads, t_inv = True):\n",
    "        super(Arch2NN, self).__init__()\n",
    "        self.fc0 = nn.Linear(in_channels, 16)\n",
    "        #self.pool = nn.MaxPool1d(kernel_size=2)  # Add a pooling layer\n",
    "        self.conv1 = feast_conv.FeaStConv(16, 32, heads=heads, t_inv=t_inv)\n",
    "        self.conv2 = feast_conv.FeaStConv(32, 64, heads=heads, t_inv=t_inv)\n",
    "        self.conv3 = feast_conv.FeaStConv(64, 128, heads=heads, t_inv=t_inv)\n",
    "        self.conv4 = feast_conv.FeaStConv(128, 256, heads=heads, t_inv=t_inv)\n",
    "        self.conv5 = feast_conv.FeaStConv(256, 512, heads=heads, t_inv=t_inv)\n",
    "        self.conv6 = feast_conv.FeaStConv(512, 128, heads=heads, t_inv=t_inv)\n",
    "        #self.conv7 = feast_conv.FeaStConv(256, 128, heads=heads, t_inv=t_inv)\n",
    "        self.fc1 = nn.Linear(128, 64)\n",
    "        self.fc2 = nn.Linear(64, 1)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.conv1.reset_parameters()\n",
    "        self.conv2.reset_parameters()\n",
    "        self.conv3.reset_parameters()\n",
    "        self.conv4.reset_parameters()\n",
    "        self.conv5.reset_parameters()\n",
    "        self.conv6.reset_parameters()\n",
    "        #self.conv7.reset_parameters()\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = F.leaky_relu(self.fc0(x))\n",
    "        #x = self.pool(x)\n",
    "        x = F.leaky_relu(self.conv1(x, edge_index))\n",
    "        x = F.leaky_relu(self.conv2(x, edge_index))\n",
    "        x = F.leaky_relu(self.conv3(x, edge_index))\n",
    "        x = F.leaky_relu(self.conv4(x, edge_index))\n",
    "        x = F.leaky_relu(self.conv5(x, edge_index))\n",
    "        x = F.leaky_relu(self.conv6(x, edge_index))\n",
    "        #x = F.elu(self.conv7(x, edge_index))\n",
    "        x = F.leaky_relu(self.fc1(x))\n",
    "        #x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        #F.log_softmax(x, dijm=1)\n",
    "        x = torch.squeeze(x, dim=1)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/300, Duration: 394.561s,Train Loss: 0.0947, Test Loss:0.0623\n",
      "Epoch: 2/300, Duration: 400.452s,Train Loss: 0.0221, Test Loss:0.0106\n",
      "Epoch: 3/300, Duration: 402.195s,Train Loss: 0.0085, Test Loss:0.0039\n",
      "Epoch: 4/300, Duration: 403.763s,Train Loss: 0.0061, Test Loss:0.0021\n",
      "Epoch: 5/300, Duration: 370.298s,Train Loss: 0.0045, Test Loss:0.0032\n",
      "Epoch: 6/300, Duration: 405.726s,Train Loss: 0.0036, Test Loss:0.0030\n",
      "Epoch: 7/300, Duration: 397.639s,Train Loss: 0.0035, Test Loss:0.0015\n",
      "Epoch: 8/300, Duration: 430.281s,Train Loss: 0.0030, Test Loss:0.0013\n",
      "Epoch: 9/300, Duration: 389.513s,Train Loss: 0.0029, Test Loss:0.0010\n",
      "Epoch: 10/300, Duration: 391.166s,Train Loss: 0.0025, Test Loss:0.0014\n",
      "Epoch: 11/300, Duration: 399.412s,Train Loss: 0.0024, Test Loss:0.0008\n",
      "Epoch: 12/300, Duration: 373.854s,Train Loss: 0.0023, Test Loss:0.0006\n",
      "Epoch: 13/300, Duration: 365.286s,Train Loss: 0.0019, Test Loss:0.0007\n",
      "Epoch: 14/300, Duration: 366.415s,Train Loss: 0.0020, Test Loss:0.0006\n",
      "Epoch: 15/300, Duration: 418.218s,Train Loss: 0.0020, Test Loss:0.0006\n",
      "Epoch: 16/300, Duration: 393.096s,Train Loss: 0.0018, Test Loss:0.0007\n",
      "Epoch: 17/300, Duration: 411.807s,Train Loss: 0.0016, Test Loss:0.0007\n",
      "Epoch: 18/300, Duration: 384.559s,Train Loss: 0.0017, Test Loss:0.0006\n",
      "Epoch: 19/300, Duration: 360.123s,Train Loss: 0.0015, Test Loss:0.0004\n",
      "Epoch: 20/300, Duration: 360.329s,Train Loss: 0.0015, Test Loss:0.0009\n",
      "Epoch: 21/300, Duration: 359.565s,Train Loss: 0.0015, Test Loss:0.0010\n",
      "Epoch: 22/300, Duration: 378.291s,Train Loss: 0.0014, Test Loss:0.0005\n",
      "Epoch: 23/300, Duration: 375.785s,Train Loss: 0.0014, Test Loss:0.0007\n",
      "Epoch: 24/300, Duration: 375.094s,Train Loss: 0.0013, Test Loss:0.0006\n",
      "Epoch: 25/300, Duration: 372.880s,Train Loss: 0.0014, Test Loss:0.0005\n",
      "Epoch: 26/300, Duration: 373.912s,Train Loss: 0.0013, Test Loss:0.0006\n",
      "Epoch: 27/300, Duration: 372.845s,Train Loss: 0.0011, Test Loss:0.0007\n",
      "Epoch: 28/300, Duration: 374.935s,Train Loss: 0.0011, Test Loss:0.0009\n",
      "Epoch: 29/300, Duration: 375.131s,Train Loss: 0.0012, Test Loss:0.0010\n",
      "Epoch: 30/300, Duration: 374.470s,Train Loss: 0.0011, Test Loss:0.0009\n",
      "Epoch: 31/300, Duration: 372.881s,Train Loss: 0.0010, Test Loss:0.0004\n",
      "Epoch: 32/300, Duration: 375.018s,Train Loss: 0.0010, Test Loss:0.0005\n",
      "Epoch: 33/300, Duration: 373.507s,Train Loss: 0.0009, Test Loss:0.0007\n",
      "Epoch: 34/300, Duration: 374.113s,Train Loss: 0.0010, Test Loss:0.0009\n",
      "Epoch: 35/300, Duration: 372.048s,Train Loss: 0.0009, Test Loss:0.0008\n",
      "Epoch: 36/300, Duration: 372.221s,Train Loss: 0.0010, Test Loss:0.0003\n",
      "Epoch: 37/300, Duration: 373.923s,Train Loss: 0.0008, Test Loss:0.0006\n",
      "Epoch: 38/300, Duration: 372.764s,Train Loss: 0.0009, Test Loss:0.0005\n",
      "Epoch: 39/300, Duration: 372.107s,Train Loss: 0.0009, Test Loss:0.0004\n",
      "Epoch: 40/300, Duration: 371.485s,Train Loss: 0.0008, Test Loss:0.0003\n",
      "Epoch: 41/300, Duration: 371.564s,Train Loss: 0.0008, Test Loss:0.0009\n",
      "Epoch: 42/300, Duration: 373.553s,Train Loss: 0.0008, Test Loss:0.0007\n",
      "Epoch: 43/300, Duration: 371.021s,Train Loss: 0.0008, Test Loss:0.0010\n",
      "Epoch: 44/300, Duration: 371.646s,Train Loss: 0.0008, Test Loss:0.0004\n",
      "Epoch: 45/300, Duration: 371.357s,Train Loss: 0.0008, Test Loss:0.0004\n",
      "Epoch: 46/300, Duration: 372.923s,Train Loss: 0.0008, Test Loss:0.0002\n",
      "Epoch: 47/300, Duration: 373.392s,Train Loss: 0.0007, Test Loss:0.0004\n",
      "Epoch: 48/300, Duration: 371.233s,Train Loss: 0.0007, Test Loss:0.0002\n",
      "Epoch: 49/300, Duration: 373.226s,Train Loss: 0.0007, Test Loss:0.0005\n",
      "Epoch: 50/300, Duration: 371.467s,Train Loss: 0.0007, Test Loss:0.0006\n",
      "Epoch: 51/300, Duration: 384.962s,Train Loss: 0.0008, Test Loss:0.0002\n",
      "Epoch: 52/300, Duration: 370.203s,Train Loss: 0.0007, Test Loss:0.0003\n",
      "Epoch: 53/300, Duration: 371.790s,Train Loss: 0.0007, Test Loss:0.0002\n",
      "Epoch: 54/300, Duration: 370.875s,Train Loss: 0.0006, Test Loss:0.0006\n",
      "Epoch: 55/300, Duration: 370.657s,Train Loss: 0.0007, Test Loss:0.0006\n",
      "Epoch: 56/300, Duration: 371.319s,Train Loss: 0.0006, Test Loss:0.0004\n",
      "Epoch: 57/300, Duration: 370.088s,Train Loss: 0.0006, Test Loss:0.0003\n",
      "Epoch: 58/300, Duration: 369.608s,Train Loss: 0.0006, Test Loss:0.0004\n",
      "Epoch: 59/300, Duration: 371.012s,Train Loss: 0.0006, Test Loss:0.0006\n",
      "Epoch: 60/300, Duration: 371.349s,Train Loss: 0.0007, Test Loss:0.0004\n",
      "Epoch: 61/300, Duration: 370.556s,Train Loss: 0.0006, Test Loss:0.0004\n",
      "Epoch: 62/300, Duration: 371.017s,Train Loss: 0.0006, Test Loss:0.0005\n",
      "Epoch: 63/300, Duration: 371.433s,Train Loss: 0.0006, Test Loss:0.0003\n",
      "Epoch: 64/300, Duration: 371.120s,Train Loss: 0.0006, Test Loss:0.0002\n",
      "Epoch: 65/300, Duration: 371.526s,Train Loss: 0.0005, Test Loss:0.0004\n",
      "Epoch: 66/300, Duration: 369.839s,Train Loss: 0.0006, Test Loss:0.0001\n",
      "Epoch: 67/300, Duration: 373.088s,Train Loss: 0.0005, Test Loss:0.0002\n",
      "Epoch: 68/300, Duration: 380.479s,Train Loss: 0.0005, Test Loss:0.0002\n",
      "Epoch: 69/300, Duration: 370.765s,Train Loss: 0.0006, Test Loss:0.0003\n",
      "Epoch: 70/300, Duration: 370.878s,Train Loss: 0.0005, Test Loss:0.0002\n",
      "Epoch: 71/300, Duration: 370.938s,Train Loss: 0.0005, Test Loss:0.0002\n",
      "Epoch: 72/300, Duration: 371.734s,Train Loss: 0.0005, Test Loss:0.0002\n",
      "Epoch: 73/300, Duration: 373.038s,Train Loss: 0.0005, Test Loss:0.0002\n",
      "Epoch: 74/300, Duration: 373.370s,Train Loss: 0.0005, Test Loss:0.0003\n",
      "Epoch: 75/300, Duration: 371.432s,Train Loss: 0.0005, Test Loss:0.0002\n",
      "Epoch: 76/300, Duration: 370.294s,Train Loss: 0.0005, Test Loss:0.0004\n",
      "Epoch: 77/300, Duration: 381.122s,Train Loss: 0.0005, Test Loss:0.0003\n",
      "Epoch: 78/300, Duration: 370.026s,Train Loss: 0.0005, Test Loss:0.0003\n",
      "Epoch: 79/300, Duration: 370.912s,Train Loss: 0.0004, Test Loss:0.0002\n",
      "Epoch: 80/300, Duration: 371.420s,Train Loss: 0.0004, Test Loss:0.0002\n",
      "Epoch: 81/300, Duration: 370.746s,Train Loss: 0.0005, Test Loss:0.0003\n",
      "Epoch: 82/300, Duration: 371.371s,Train Loss: 0.0005, Test Loss:0.0004\n",
      "Epoch: 83/300, Duration: 372.184s,Train Loss: 0.0005, Test Loss:0.0004\n",
      "Epoch: 84/300, Duration: 371.135s,Train Loss: 0.0005, Test Loss:0.0002\n",
      "Epoch: 85/300, Duration: 372.086s,Train Loss: 0.0004, Test Loss:0.0002\n",
      "Epoch: 86/300, Duration: 380.218s,Train Loss: 0.0004, Test Loss:0.0003\n",
      "Epoch: 87/300, Duration: 370.809s,Train Loss: 0.0004, Test Loss:0.0003\n",
      "Epoch: 88/300, Duration: 370.790s,Train Loss: 0.0004, Test Loss:0.0002\n",
      "Epoch: 89/300, Duration: 371.076s,Train Loss: 0.0005, Test Loss:0.0003\n",
      "Epoch: 90/300, Duration: 371.267s,Train Loss: 0.0004, Test Loss:0.0002\n",
      "Epoch: 91/300, Duration: 370.896s,Train Loss: 0.0004, Test Loss:0.0003\n",
      "Epoch: 92/300, Duration: 370.149s,Train Loss: 0.0004, Test Loss:0.0002\n",
      "Epoch: 93/300, Duration: 371.371s,Train Loss: 0.0004, Test Loss:0.0004\n",
      "Epoch: 94/300, Duration: 369.869s,Train Loss: 0.0004, Test Loss:0.0002\n",
      "Epoch: 95/300, Duration: 370.907s,Train Loss: 0.0004, Test Loss:0.0003\n",
      "Epoch: 96/300, Duration: 370.828s,Train Loss: 0.0004, Test Loss:0.0003\n",
      "Epoch: 97/300, Duration: 369.380s,Train Loss: 0.0004, Test Loss:0.0002\n",
      "Epoch: 98/300, Duration: 370.827s,Train Loss: 0.0004, Test Loss:0.0002\n",
      "Epoch: 99/300, Duration: 372.179s,Train Loss: 0.0004, Test Loss:0.0001\n",
      "Epoch: 100/300, Duration: 370.752s,Train Loss: 0.0004, Test Loss:0.0002\n",
      "Epoch: 101/300, Duration: 370.171s,Train Loss: 0.0004, Test Loss:0.0002\n",
      "Epoch: 102/300, Duration: 371.779s,Train Loss: 0.0004, Test Loss:0.0003\n",
      "Epoch: 103/300, Duration: 369.690s,Train Loss: 0.0004, Test Loss:0.0003\n",
      "Epoch: 104/300, Duration: 370.015s,Train Loss: 0.0004, Test Loss:0.0003\n",
      "Epoch: 105/300, Duration: 370.242s,Train Loss: 0.0004, Test Loss:0.0002\n",
      "Epoch: 106/300, Duration: 370.037s,Train Loss: 0.0004, Test Loss:0.0002\n",
      "Epoch: 107/300, Duration: 389.549s,Train Loss: 0.0004, Test Loss:0.0003\n",
      "Epoch: 108/300, Duration: 371.153s,Train Loss: 0.0004, Test Loss:0.0002\n",
      "Epoch: 109/300, Duration: 370.631s,Train Loss: 0.0004, Test Loss:0.0001\n",
      "Epoch: 110/300, Duration: 369.170s,Train Loss: 0.0003, Test Loss:0.0003\n",
      "Epoch: 111/300, Duration: 370.252s,Train Loss: 0.0003, Test Loss:0.0003\n",
      "Epoch: 112/300, Duration: 374.125s,Train Loss: 0.0003, Test Loss:0.0003\n",
      "Epoch: 113/300, Duration: 371.304s,Train Loss: 0.0003, Test Loss:0.0002\n",
      "Epoch: 114/300, Duration: 370.104s,Train Loss: 0.0004, Test Loss:0.0003\n",
      "Epoch: 115/300, Duration: 369.794s,Train Loss: 0.0004, Test Loss:0.0003\n",
      "Epoch: 116/300, Duration: 370.165s,Train Loss: 0.0003, Test Loss:0.0004\n",
      "Epoch: 117/300, Duration: 371.359s,Train Loss: 0.0004, Test Loss:0.0002\n",
      "Epoch: 118/300, Duration: 373.144s,Train Loss: 0.0003, Test Loss:0.0004\n",
      "Epoch: 119/300, Duration: 372.937s,Train Loss: 0.0003, Test Loss:0.0001\n",
      "Epoch: 120/300, Duration: 371.868s,Train Loss: 0.0003, Test Loss:0.0002\n",
      "Epoch: 121/300, Duration: 378.752s,Train Loss: 0.0004, Test Loss:0.0002\n",
      "Epoch: 122/300, Duration: 371.666s,Train Loss: 0.0003, Test Loss:0.0003\n",
      "Epoch: 123/300, Duration: 370.352s,Train Loss: 0.0003, Test Loss:0.0004\n",
      "Epoch: 124/300, Duration: 370.534s,Train Loss: 0.0003, Test Loss:0.0003\n",
      "Epoch: 125/300, Duration: 370.391s,Train Loss: 0.0003, Test Loss:0.0001\n",
      "Epoch: 126/300, Duration: 371.162s,Train Loss: 0.0003, Test Loss:0.0002\n",
      "Epoch: 127/300, Duration: 371.547s,Train Loss: 0.0004, Test Loss:0.0002\n",
      "Epoch: 128/300, Duration: 371.046s,Train Loss: 0.0003, Test Loss:0.0003\n",
      "Epoch: 129/300, Duration: 370.215s,Train Loss: 0.0003, Test Loss:0.0003\n",
      "Epoch: 130/300, Duration: 369.741s,Train Loss: 0.0003, Test Loss:0.0002\n",
      "Epoch: 131/300, Duration: 372.193s,Train Loss: 0.0003, Test Loss:0.0002\n",
      "Epoch: 132/300, Duration: 371.088s,Train Loss: 0.0003, Test Loss:0.0003\n",
      "Epoch: 133/300, Duration: 370.783s,Train Loss: 0.0003, Test Loss:0.0002\n",
      "Epoch: 134/300, Duration: 370.577s,Train Loss: 0.0003, Test Loss:0.0002\n",
      "Epoch: 135/300, Duration: 370.672s,Train Loss: 0.0003, Test Loss:0.0002\n",
      "Epoch: 136/300, Duration: 372.425s,Train Loss: 0.0003, Test Loss:0.0002\n",
      "Epoch: 137/300, Duration: 370.513s,Train Loss: 0.0003, Test Loss:0.0001\n",
      "Epoch: 138/300, Duration: 369.603s,Train Loss: 0.0003, Test Loss:0.0002\n",
      "Epoch: 139/300, Duration: 370.966s,Train Loss: 0.0003, Test Loss:0.0002\n",
      "Epoch: 140/300, Duration: 370.406s,Train Loss: 0.0003, Test Loss:0.0003\n",
      "Epoch: 141/300, Duration: 371.462s,Train Loss: 0.0003, Test Loss:0.0001\n",
      "Epoch: 142/300, Duration: 370.158s,Train Loss: 0.0003, Test Loss:0.0003\n",
      "Epoch: 143/300, Duration: 370.358s,Train Loss: 0.0003, Test Loss:0.0001\n",
      "Epoch: 144/300, Duration: 370.753s,Train Loss: 0.0003, Test Loss:0.0002\n",
      "Epoch: 145/300, Duration: 371.750s,Train Loss: 0.0003, Test Loss:0.0001\n",
      "Epoch: 146/300, Duration: 371.712s,Train Loss: 0.0003, Test Loss:0.0003\n",
      "Epoch: 147/300, Duration: 376.706s,Train Loss: 0.0003, Test Loss:0.0002\n",
      "Epoch: 148/300, Duration: 367.990s,Train Loss: 0.0003, Test Loss:0.0003\n",
      "Epoch: 149/300, Duration: 366.613s,Train Loss: 0.0003, Test Loss:0.0003\n",
      "Epoch: 150/300, Duration: 367.779s,Train Loss: 0.0003, Test Loss:0.0003\n",
      "Epoch: 151/300, Duration: 370.684s,Train Loss: 0.0003, Test Loss:0.0002\n",
      "Epoch: 152/300, Duration: 369.755s,Train Loss: 0.0003, Test Loss:0.0001\n",
      "Epoch: 153/300, Duration: 371.794s,Train Loss: 0.0003, Test Loss:0.0003\n",
      "Epoch: 154/300, Duration: 372.157s,Train Loss: 0.0003, Test Loss:0.0003\n",
      "Epoch: 155/300, Duration: 370.850s,Train Loss: 0.0003, Test Loss:0.0002\n",
      "Epoch: 156/300, Duration: 371.769s,Train Loss: 0.0003, Test Loss:0.0002\n",
      "Epoch: 157/300, Duration: 371.262s,Train Loss: 0.0003, Test Loss:0.0002\n",
      "Epoch: 158/300, Duration: 369.986s,Train Loss: 0.0003, Test Loss:0.0003\n",
      "Epoch: 159/300, Duration: 370.034s,Train Loss: 0.0003, Test Loss:0.0004\n",
      "Epoch: 160/300, Duration: 370.567s,Train Loss: 0.0003, Test Loss:0.0003\n",
      "Epoch: 161/300, Duration: 369.508s,Train Loss: 0.0003, Test Loss:0.0002\n",
      "Epoch: 162/300, Duration: 371.671s,Train Loss: 0.0003, Test Loss:0.0002\n",
      "Epoch: 163/300, Duration: 370.718s,Train Loss: 0.0002, Test Loss:0.0001\n",
      "Epoch: 164/300, Duration: 370.086s,Train Loss: 0.0003, Test Loss:0.0001\n",
      "Epoch: 165/300, Duration: 372.344s,Train Loss: 0.0003, Test Loss:0.0001\n",
      "Epoch: 166/300, Duration: 372.563s,Train Loss: 0.0003, Test Loss:0.0003\n",
      "Epoch: 167/300, Duration: 372.789s,Train Loss: 0.0002, Test Loss:0.0002\n",
      "Epoch: 168/300, Duration: 369.757s,Train Loss: 0.0003, Test Loss:0.0001\n",
      "Epoch: 169/300, Duration: 364.016s,Train Loss: 0.0002, Test Loss:0.0002\n",
      "Epoch: 170/300, Duration: 363.508s,Train Loss: 0.0002, Test Loss:0.0003\n",
      "Epoch: 171/300, Duration: 355.385s,Train Loss: 0.0003, Test Loss:0.0003\n",
      "Epoch: 172/300, Duration: 397.342s,Train Loss: 0.0003, Test Loss:0.0002\n",
      "Epoch: 173/300, Duration: 356.661s,Train Loss: 0.0002, Test Loss:0.0001\n",
      "Epoch: 174/300, Duration: 365.258s,Train Loss: 0.0003, Test Loss:0.0002\n",
      "Epoch: 175/300, Duration: 366.143s,Train Loss: 0.0002, Test Loss:0.0001\n",
      "Epoch: 176/300, Duration: 351.906s,Train Loss: 0.0003, Test Loss:0.0001\n",
      "Epoch: 177/300, Duration: 337.332s,Train Loss: 0.0003, Test Loss:0.0003\n",
      "Epoch: 178/300, Duration: 362.950s,Train Loss: 0.0002, Test Loss:0.0001\n",
      "Epoch: 179/300, Duration: 359.002s,Train Loss: 0.0003, Test Loss:0.0001\n",
      "Epoch: 180/300, Duration: 362.776s,Train Loss: 0.0002, Test Loss:0.0001\n",
      "Epoch: 181/300, Duration: 368.337s,Train Loss: 0.0002, Test Loss:0.0002\n",
      "Epoch: 182/300, Duration: 360.751s,Train Loss: 0.0002, Test Loss:0.0001\n",
      "Epoch: 183/300, Duration: 361.069s,Train Loss: 0.0003, Test Loss:0.0002\n",
      "Epoch: 184/300, Duration: 381.750s,Train Loss: 0.0002, Test Loss:0.0002\n",
      "Epoch: 185/300, Duration: 358.613s,Train Loss: 0.0002, Test Loss:0.0001\n",
      "Epoch: 186/300, Duration: 364.633s,Train Loss: 0.0002, Test Loss:0.0001\n",
      "Epoch: 187/300, Duration: 366.643s,Train Loss: 0.0002, Test Loss:0.0001\n",
      "Epoch: 188/300, Duration: 359.946s,Train Loss: 0.0002, Test Loss:0.0002\n",
      "Epoch: 189/300, Duration: 359.465s,Train Loss: 0.0002, Test Loss:0.0001\n",
      "Epoch: 190/300, Duration: 362.724s,Train Loss: 0.0002, Test Loss:0.0002\n",
      "Epoch: 191/300, Duration: 360.094s,Train Loss: 0.0002, Test Loss:0.0001\n",
      "Epoch: 192/300, Duration: 357.343s,Train Loss: 0.0002, Test Loss:0.0001\n",
      "Epoch: 193/300, Duration: 359.140s,Train Loss: 0.0002, Test Loss:0.0001\n",
      "Epoch: 194/300, Duration: 360.889s,Train Loss: 0.0002, Test Loss:0.0001\n",
      "Epoch: 195/300, Duration: 361.317s,Train Loss: 0.0002, Test Loss:0.0002\n",
      "Epoch: 196/300, Duration: 377.508s,Train Loss: 0.0002, Test Loss:0.0001\n",
      "Epoch: 197/300, Duration: 358.838s,Train Loss: 0.0002, Test Loss:0.0001\n",
      "Epoch: 198/300, Duration: 354.637s,Train Loss: 0.0002, Test Loss:0.0001\n",
      "Epoch: 199/300, Duration: 353.178s,Train Loss: 0.0002, Test Loss:0.0001\n",
      "Epoch: 200/300, Duration: 350.258s,Train Loss: 0.0002, Test Loss:0.0002\n",
      "Epoch: 201/300, Duration: 349.931s,Train Loss: 0.0002, Test Loss:0.0001\n",
      "Epoch: 202/300, Duration: 353.787s,Train Loss: 0.0002, Test Loss:0.0001\n",
      "Epoch: 203/300, Duration: 358.134s,Train Loss: 0.0002, Test Loss:0.0001\n",
      "Epoch: 204/300, Duration: 351.987s,Train Loss: 0.0002, Test Loss:0.0002\n",
      "Epoch: 205/300, Duration: 346.151s,Train Loss: 0.0002, Test Loss:0.0002\n",
      "Epoch: 206/300, Duration: 343.731s,Train Loss: 0.0002, Test Loss:0.0001\n",
      "Epoch: 207/300, Duration: 358.049s,Train Loss: 0.0002, Test Loss:0.0002\n",
      "Epoch: 208/300, Duration: 354.487s,Train Loss: 0.0002, Test Loss:0.0002\n",
      "Epoch: 209/300, Duration: 362.899s,Train Loss: 0.0002, Test Loss:0.0001\n",
      "Epoch: 210/300, Duration: 395.562s,Train Loss: 0.0002, Test Loss:0.0002\n",
      "Epoch: 211/300, Duration: 366.182s,Train Loss: 0.0002, Test Loss:0.0001\n",
      "Epoch: 212/300, Duration: 374.884s,Train Loss: 0.0002, Test Loss:0.0001\n",
      "Epoch: 213/300, Duration: 396.319s,Train Loss: 0.0002, Test Loss:0.0001\n",
      "Epoch: 214/300, Duration: 408.623s,Train Loss: 0.0002, Test Loss:0.0001\n",
      "Epoch: 215/300, Duration: 379.467s,Train Loss: 0.0002, Test Loss:0.0001\n",
      "Epoch: 216/300, Duration: 350.106s,Train Loss: 0.0002, Test Loss:0.0002\n",
      "Epoch: 217/300, Duration: 345.439s,Train Loss: 0.0002, Test Loss:0.0001\n",
      "Epoch: 218/300, Duration: 394.037s,Train Loss: 0.0002, Test Loss:0.0001\n",
      "Epoch: 219/300, Duration: 426.762s,Train Loss: 0.0002, Test Loss:0.0002\n",
      "Epoch: 220/300, Duration: 430.650s,Train Loss: 0.0002, Test Loss:0.0001\n",
      "Epoch: 221/300, Duration: 429.119s,Train Loss: 0.0002, Test Loss:0.0002\n",
      "Epoch: 222/300, Duration: 352.548s,Train Loss: 0.0002, Test Loss:0.0002\n",
      "Epoch: 223/300, Duration: 374.437s,Train Loss: 0.0002, Test Loss:0.0002\n",
      "Epoch: 224/300, Duration: 367.163s,Train Loss: 0.0002, Test Loss:0.0002\n",
      "Epoch: 225/300, Duration: 374.881s,Train Loss: 0.0002, Test Loss:0.0002\n",
      "Epoch: 226/300, Duration: 379.317s,Train Loss: 0.0002, Test Loss:0.0001\n",
      "Epoch: 227/300, Duration: 364.908s,Train Loss: 0.0002, Test Loss:0.0001\n",
      "Epoch: 228/300, Duration: 365.791s,Train Loss: 0.0002, Test Loss:0.0001\n",
      "Epoch: 229/300, Duration: 367.161s,Train Loss: 0.0002, Test Loss:0.0001\n",
      "Epoch: 230/300, Duration: 368.282s,Train Loss: 0.0002, Test Loss:0.0001\n",
      "Epoch: 231/300, Duration: 387.288s,Train Loss: 0.0002, Test Loss:0.0001\n",
      "Epoch: 232/300, Duration: 364.794s,Train Loss: 0.0002, Test Loss:0.0001\n",
      "Epoch: 233/300, Duration: 370.634s,Train Loss: 0.0002, Test Loss:0.0001\n",
      "Epoch: 234/300, Duration: 358.345s,Train Loss: 0.0002, Test Loss:0.0001\n",
      "Epoch: 235/300, Duration: 365.052s,Train Loss: 0.0002, Test Loss:0.0002\n",
      "Epoch: 236/300, Duration: 357.440s,Train Loss: 0.0002, Test Loss:0.0001\n",
      "Epoch: 237/300, Duration: 364.839s,Train Loss: 0.0002, Test Loss:0.0001\n",
      "Epoch: 238/300, Duration: 359.460s,Train Loss: 0.0002, Test Loss:0.0001\n",
      "Epoch: 239/300, Duration: 360.547s,Train Loss: 0.0002, Test Loss:0.0002\n",
      "Epoch: 240/300, Duration: 369.672s,Train Loss: 0.0002, Test Loss:0.0002\n",
      "Epoch: 241/300, Duration: 378.169s,Train Loss: 0.0002, Test Loss:0.0001\n",
      "Epoch: 242/300, Duration: 373.839s,Train Loss: 0.0002, Test Loss:0.0001\n",
      "Epoch: 243/300, Duration: 361.666s,Train Loss: 0.0002, Test Loss:0.0002\n",
      "Epoch: 244/300, Duration: 370.703s,Train Loss: 0.0002, Test Loss:0.0001\n",
      "Epoch: 245/300, Duration: 358.327s,Train Loss: 0.0002, Test Loss:0.0001\n",
      "Epoch: 246/300, Duration: 379.676s,Train Loss: 0.0002, Test Loss:0.0001\n",
      "Epoch: 247/300, Duration: 377.088s,Train Loss: 0.0002, Test Loss:0.0001\n",
      "Epoch: 248/300, Duration: 387.641s,Train Loss: 0.0002, Test Loss:0.0002\n",
      "Epoch: 249/300, Duration: 381.224s,Train Loss: 0.0002, Test Loss:0.0001\n",
      "Epoch: 250/300, Duration: 399.730s,Train Loss: 0.0002, Test Loss:0.0001\n",
      "Epoch: 251/300, Duration: 354.005s,Train Loss: 0.0002, Test Loss:0.0002\n",
      "Epoch: 252/300, Duration: 352.731s,Train Loss: 0.0002, Test Loss:0.0001\n",
      "Epoch: 253/300, Duration: 351.960s,Train Loss: 0.0002, Test Loss:0.0002\n",
      "Epoch: 254/300, Duration: 350.611s,Train Loss: 0.0002, Test Loss:0.0001\n",
      "Epoch: 255/300, Duration: 352.688s,Train Loss: 0.0002, Test Loss:0.0001\n",
      "Epoch: 256/300, Duration: 353.089s,Train Loss: 0.0002, Test Loss:0.0001\n",
      "Epoch: 257/300, Duration: 351.669s,Train Loss: 0.0002, Test Loss:0.0001\n",
      "Epoch: 258/300, Duration: 351.409s,Train Loss: 0.0002, Test Loss:0.0001\n",
      "Epoch: 259/300, Duration: 351.776s,Train Loss: 0.0002, Test Loss:0.0001\n",
      "Epoch: 260/300, Duration: 351.954s,Train Loss: 0.0002, Test Loss:0.0001\n",
      "Epoch: 261/300, Duration: 354.126s,Train Loss: 0.0002, Test Loss:0.0001\n",
      "Epoch: 262/300, Duration: 351.587s,Train Loss: 0.0002, Test Loss:0.0002\n",
      "Epoch: 263/300, Duration: 352.215s,Train Loss: 0.0002, Test Loss:0.0001\n",
      "Epoch: 264/300, Duration: 353.119s,Train Loss: 0.0002, Test Loss:0.0002\n",
      "Epoch: 265/300, Duration: 361.455s,Train Loss: 0.0002, Test Loss:0.0001\n",
      "Epoch: 266/300, Duration: 351.719s,Train Loss: 0.0002, Test Loss:0.0001\n",
      "Epoch: 267/300, Duration: 351.909s,Train Loss: 0.0002, Test Loss:0.0001\n",
      "Epoch: 268/300, Duration: 351.866s,Train Loss: 0.0002, Test Loss:0.0001\n",
      "Epoch: 269/300, Duration: 352.415s,Train Loss: 0.0002, Test Loss:0.0003\n",
      "Epoch: 270/300, Duration: 352.604s,Train Loss: 0.0002, Test Loss:0.0001\n",
      "Epoch: 271/300, Duration: 351.693s,Train Loss: 0.0002, Test Loss:0.0001\n",
      "Epoch: 272/300, Duration: 352.943s,Train Loss: 0.0002, Test Loss:0.0001\n",
      "Epoch: 273/300, Duration: 352.999s,Train Loss: 0.0002, Test Loss:0.0002\n",
      "Epoch: 274/300, Duration: 351.284s,Train Loss: 0.0002, Test Loss:0.0002\n",
      "Epoch: 275/300, Duration: 351.812s,Train Loss: 0.0002, Test Loss:0.0001\n",
      "Epoch: 276/300, Duration: 352.089s,Train Loss: 0.0002, Test Loss:0.0002\n",
      "Epoch: 277/300, Duration: 351.598s,Train Loss: 0.0002, Test Loss:0.0001\n",
      "Epoch: 278/300, Duration: 351.678s,Train Loss: 0.0002, Test Loss:0.0001\n",
      "Epoch: 279/300, Duration: 351.541s,Train Loss: 0.0002, Test Loss:0.0001\n",
      "Epoch: 280/300, Duration: 352.270s,Train Loss: 0.0002, Test Loss:0.0001\n",
      "Epoch: 281/300, Duration: 351.049s,Train Loss: 0.0002, Test Loss:0.0001\n",
      "Epoch: 282/300, Duration: 352.403s,Train Loss: 0.0002, Test Loss:0.0001\n",
      "Epoch: 283/300, Duration: 352.151s,Train Loss: 0.0002, Test Loss:0.0002\n",
      "Epoch: 284/300, Duration: 352.131s,Train Loss: 0.0002, Test Loss:0.0001\n",
      "Epoch: 285/300, Duration: 350.919s,Train Loss: 0.0002, Test Loss:0.0001\n",
      "Epoch: 286/300, Duration: 351.574s,Train Loss: 0.0002, Test Loss:0.0001\n",
      "Epoch: 287/300, Duration: 352.592s,Train Loss: 0.0002, Test Loss:0.0001\n",
      "Epoch: 288/300, Duration: 351.752s,Train Loss: 0.0002, Test Loss:0.0001\n",
      "Epoch: 289/300, Duration: 351.722s,Train Loss: 0.0002, Test Loss:0.0001\n",
      "Epoch: 290/300, Duration: 351.346s,Train Loss: 0.0002, Test Loss:0.0001\n",
      "Epoch: 291/300, Duration: 353.425s,Train Loss: 0.0002, Test Loss:0.0001\n",
      "Epoch: 292/300, Duration: 356.203s,Train Loss: 0.0002, Test Loss:0.0001\n",
      "Epoch: 293/300, Duration: 351.362s,Train Loss: 0.0002, Test Loss:0.0001\n",
      "Epoch: 294/300, Duration: 352.192s,Train Loss: 0.0001, Test Loss:0.0002\n",
      "Epoch: 295/300, Duration: 351.536s,Train Loss: 0.0002, Test Loss:0.0001\n",
      "Epoch: 296/300, Duration: 353.409s,Train Loss: 0.0002, Test Loss:0.0001\n",
      "Epoch: 297/300, Duration: 352.034s,Train Loss: 0.0002, Test Loss:0.0001\n",
      "Epoch: 298/300, Duration: 352.516s,Train Loss: 0.0002, Test Loss:0.0002\n",
      "Epoch: 299/300, Duration: 351.074s,Train Loss: 0.0002, Test Loss:0.0001\n",
      "Epoch: 300/300, Duration: 352.373s,Train Loss: 0.0002, Test Loss:0.0001\n",
      "saved as:  Apr-26ver1.pt\n"
     ]
    }
   ],
   "source": [
    "#runner\n",
    "num_nodes = train_loader[0].x.shape[0]\n",
    "num_features = train_loader[0].x.shape[1]\n",
    "\n",
    "model2 = Arch2NN(num_features, num_nodes, heads=8).to(device_name)\n",
    "\n",
    "optimizer = optim.Adam(model2.parameters(),\n",
    "                       lr=0.0001)\n",
    "\n",
    "\n",
    "run(model2, train_loader, test_loader, num_nodes, 300, optimizer, device_name)\n",
    "\n",
    "#versj defined at the top of the notebook\n",
    "modelPath = today + \"ver\" + str(versj) +\".pt\"\n",
    "torch.save(model2.state_dict(), modelPath)\n",
    "versj += 1\n",
    "print(\"saved as: \", modelPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load model\n",
    "model5_dict = torch.load(\"Apr-25ver1.pt\")\n",
    "model6_dict = torch.load(\"Apr-25ver2.pt\")\n",
    "model7_dict = torch.load(\"Apr-26ver1.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(model5.state_dict(), \"Apr-21ver1_state_dict.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Arch2NN(\n",
       "  (fc0): Linear(in_features=3, out_features=16, bias=True)\n",
       "  (conv1): FeaStConv(16, 32, heads=8)\n",
       "  (conv2): FeaStConv(32, 64, heads=8)\n",
       "  (conv3): FeaStConv(64, 128, heads=8)\n",
       "  (conv4): FeaStConv(128, 256, heads=8)\n",
       "  (conv5): FeaStConv(256, 512, heads=8)\n",
       "  (conv6): FeaStConv(512, 128, heads=8)\n",
       "  (fc1): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (fc2): Linear(in_features=64, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_nodes = train_loader[0].x.shape[0]\n",
    "num_features = train_loader[0].x.shape[1]\n",
    "\n",
    "tester_model2 = Arch2NN(num_features, num_nodes, heads=8)\n",
    "tester_model2.load_state_dict(model6_dict)\n",
    "tester_model2.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.share_memory of Arch2NN(\n",
       "  (fc0): Linear(in_features=3, out_features=16, bias=True)\n",
       "  (conv1): FeaStConv(16, 32, heads=8)\n",
       "  (conv2): FeaStConv(32, 64, heads=8)\n",
       "  (conv3): FeaStConv(64, 128, heads=8)\n",
       "  (conv4): FeaStConv(128, 256, heads=8)\n",
       "  (conv5): FeaStConv(256, 512, heads=8)\n",
       "  (conv6): FeaStConv(512, 128, heads=8)\n",
       "  (fc1): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (fc2): Linear(in_features=64, out_features=1, bias=True)\n",
       ")>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tester_model1 = ArchNN(num_features, num_nodes, heads=8)\n",
    "\n",
    "tester_model3 = Arch2NN(num_features, num_nodes, heads=8)\n",
    "tester_model1.load_state_dict(model5_dict)\n",
    "tester_model3.load_state_dict(model7_dict)\n",
    "tester_model1.eval()\n",
    "tester_model3.eval()\n",
    "tester_model3.share_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-3.1303,  2.3542,  1.5644,  0.8224,  0.1171, -0.4801, -1.0095, -1.4173,\n",
      "        -1.6761, -1.8224, -1.8244, -1.6886, -1.3957, -1.0161, -0.4973,  0.1045,\n",
      "         0.8055,  1.5466,  2.3431,  3.1168], grad_fn=<SqueezeBackward1>)\n",
      "tensor([-3.1242,  2.3410,  1.5610,  0.8124,  0.1205, -0.4919, -1.0057, -1.4050,\n",
      "        -1.6780, -1.8165, -1.8165, -1.6780, -1.4050, -1.0057, -0.4919,  0.1205,\n",
      "         0.8124,  1.5610,  2.3410,  3.1242])\n",
      "tensor([-3.1318,  2.3414,  1.5673,  0.8168,  0.1115, -0.5086, -1.0223, -1.4418,\n",
      "        -1.6993, -1.8431, -1.8433, -1.7036, -1.4348, -1.0281, -0.5020,  0.1201,\n",
      "         0.8166,  1.5717,  2.3660,  3.1554], grad_fn=<SqueezeBackward1>)\n",
      "tensor([-3.1242,  2.3410,  1.5610,  0.8124,  0.1205, -0.4919, -1.0057, -1.4050,\n",
      "        -1.6780, -1.8165, -1.8165, -1.6780, -1.4050, -1.0057, -0.4919,  0.1205,\n",
      "         0.8124,  1.5610,  2.3410,  3.1242])\n",
      "tensor([ 2.6688, -0.6248,  0.6508,  1.2117,  1.2339,  0.9167,  0.2930, -0.3265,\n",
      "        -0.8476, -1.1294, -1.1046, -0.8399, -0.3201,  0.3003,  0.9151,  1.2960,\n",
      "         1.2993,  0.6095, -0.5557, -2.3887], grad_fn=<SqueezeBackward1>)\n",
      "tensor([ 27.8597,  -6.6907,   6.8243,  13.1813,  13.5436,   9.5651,   3.1650,\n",
      "         -3.7210,  -9.3840, -12.5538, -12.5538,  -9.3840,  -3.7210,   3.1650,\n",
      "          9.5651,  13.5436,  13.1813,   6.8243,  -6.6907, -27.8597])\n"
     ]
    }
   ],
   "source": [
    "tester_model1.eval()\n",
    "print(tester_model1(datasetTest[200]))\n",
    "print(datasetTest[200].y)\n",
    "\n",
    "tester_model2.eval()\n",
    "print(tester_model2(datasetTest[200]))\n",
    "print(datasetTest[200].y)\n",
    "\n",
    "tester_model3.eval()\n",
    "print(tester_model3(datasetTest2[0]))\n",
    "print(datasetTest2[0].y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-943.92, -970.89, -948.52, -987.8, -997.61, -943.48, -980.23, -1039.57, -1007.18, -1011.52, -1036.51, -1017.27, -1062.31, -953.94, -945.2, -945.05, -914.52, -1019.61, -1104.04, -1066.3]\n",
      "Avarage Error: 994.77%, Adjusted Avarage Error: 984.73%\n",
      "Mean Square: 130.8571\n",
      "\n",
      "[-951.31, -977.89, -960.97, -1002.53, -1007.59, -948.62, -987.03, -1057.16, -1021.67, -1031.18, -1061.54, -1037.75, -1086.54, -966.99, -947.21, -944.99, -910.13, -1024.24, -1127.92, -1083.64]\n",
      "Avarage Error: 1006.85%, Adjusted Avarage Error: 995.69%\n",
      "Mean Square: 139.2679\n",
      "\n",
      "[-959.2, -987.29, -973.06, -1017.36, -1018.11, -954.3, -994.52, -1074.95, -1035.72, -1046.5, -1077.21, -1060.06, -1113.77, -982.0, -946.93, -943.89, -904.25, -1023.76, -1158.22, -1100.32]\n",
      "Avarage Error: 1018.57%, Adjusted Avarage Error: 1005.52%\n",
      "Mean Square: 148.0589\n",
      "\n",
      "[-967.55, -997.65, -986.45, -1032.59, -1029.2, -959.43, -1003.24, -1091.49, -1050.67, -1064.13, -1095.75, -1085.83, -1140.44, -996.17, -948.28, -940.86, -893.69, -1020.78, -1189.63, -1116.28]\n",
      "Avarage Error: 1030.51%, Adjusted Avarage Error: 1015.56%\n",
      "Mean Square: 157.259\n",
      "\n",
      "[-976.46, -1009.0, -1000.95, -1048.97, -1040.84, -962.09, -1013.81, -1106.56, -1067.55, -1072.1, -1099.55, -1121.89, -1169.79, -1008.43, -951.39, -918.6, -878.35, -1020.08, -1222.44, -1132.42]\n",
      "Avarage Error: 1041.06%, Adjusted Avarage Error: 1023.84%\n",
      "Mean Square: 166.8201\n",
      "\n",
      "[-985.98, -1021.31, -1015.22, -1066.13, -1052.83, -965.43, -1025.31, -1121.68, -1087.67, -1077.22, -1100.45, -1170.92, -1220.53, -1019.98, -953.76, -896.63, -859.71, -1017.53, -1263.66, -1149.06]\n",
      "Avarage Error: 1053.55%, Adjusted Avarage Error: 1032.6%\n",
      "Mean Square: 176.8167\n",
      "\n",
      "[-996.34, -1034.62, -1030.22, -1083.18, -1065.04, -969.95, -1037.53, -1137.29, -1107.44, -1088.73, -1106.22, -1229.43, -1279.97, -1019.58, -955.01, -882.5, -841.02, -1009.08, -1292.8, -1166.74]\n",
      "Avarage Error: 1066.63%, Adjusted Avarage Error: 1042.22%\n",
      "Mean Square: 187.2974\n",
      "\n",
      "[-1007.1, -1048.11, -1046.03, -1100.8, -1077.03, -974.17, -1048.46, -1155.93, -1130.02, -1101.02, -1110.77, -1276.0, -1327.1, -1017.37, -957.69, -870.6, -822.59, -1000.56, -1319.73, -1185.37]\n",
      "Avarage Error: 1078.82%, Adjusted Avarage Error: 1051.64%\n",
      "Mean Square: 198.241\n",
      "\n",
      "[-1018.36, -1061.19, -1062.08, -1118.74, -1089.27, -979.84, -1058.39, -1181.96, -1174.34, -1123.75, -1111.57, -1307.54, -1357.73, -1015.01, -961.65, -862.33, -806.34, -993.12, -1346.95, -1204.46]\n",
      "Avarage Error: 1091.73%, Adjusted Avarage Error: 1062.78%\n",
      "Mean Square: 209.6978\n",
      "\n",
      "[-1029.81, -1075.29, -1078.46, -1137.44, -1101.49, -985.63, -1074.47, -1208.05, -1207.73, -1224.63, -1144.54, -1366.32, -1397.02, -1013.25, -962.45, -853.03, -793.97, -989.55, -1374.47, -1223.67]\n",
      "Avarage Error: 1112.06%, Adjusted Avarage Error: 1081.65%\n",
      "Mean Square: 221.8263\n",
      "\n",
      "Avarage Tot. Error: 1049.46%\n",
      "\n",
      "After removing the two worst errors. Avarage Adjusted Tot. Error: 1029.62%\n",
      "Total error for nodes 8, 9, 10, 11. Avarage Adjusted Tot. Error: 1108.7%\n"
     ]
    }
   ],
   "source": [
    "testFinal(tester_model3, datasetTest2, device_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[86.29, 85.38, 88.6, 88.33, 86.96, 86.97, 154.74, 94.72, 89.06, 395.3, 81.07, 160.95, 95.86, 85.02, 84.8, 84.81, 78.52, 75.03, 86.72, 82.46]\n",
      "Avarage Error: 108.58%, Adjusted Avarage Error: 89.74%\n",
      "Mean Square: 25.618\n",
      "\n",
      "[86.2, 85.26, 88.57, 88.28, 86.89, 86.89, 153.49, 94.72, 88.98, 396.95, 80.93, 159.33, 95.83, 84.8, 84.65, 84.73, 78.19, 74.29, 86.8, 82.3]\n",
      "Avarage Error: 108.41%, Adjusted Avarage Error: 89.55%\n",
      "Mean Square: 25.9762\n",
      "\n",
      "[86.12, 85.15, 88.54, 88.24, 86.82, 86.8, 152.38, 94.7, 88.91, 399.48, 80.8, 158.18, 95.81, 84.57, 84.51, 84.66, 77.91, 73.55, 86.87, 82.15]\n",
      "Avarage Error: 108.31%, Adjusted Avarage Error: 89.36%\n",
      "Mean Square: 26.3286\n",
      "\n",
      "[86.03, 85.04, 88.51, 88.19, 86.75, 86.71, 151.46, 94.66, 88.83, 405.11, 80.67, 157.22, 95.78, 84.29, 84.37, 84.59, 77.65, 72.77, 86.98, 81.99]\n",
      "Avarage Error: 108.38%, Adjusted Avarage Error: 89.18%\n",
      "Mean Square: 26.6836\n",
      "\n",
      "[85.95, 84.92, 88.47, 88.14, 86.68, 86.62, 150.64, 94.62, 88.75, 411.5, 80.53, 156.41, 95.75, 83.98, 84.24, 84.52, 77.42, 71.93, 87.1, 81.84]\n",
      "Avarage Error: 108.5%, Adjusted Avarage Error: 89.01%\n",
      "Mean Square: 27.0444\n",
      "\n",
      "[85.87, 84.81, 88.44, 88.09, 86.61, 86.53, 149.85, 94.59, 88.67, 420.93, 80.4, 155.49, 95.73, 83.49, 84.1, 84.46, 77.18, 70.72, 87.21, 81.68]\n",
      "Avarage Error: 108.74%, Adjusted Avarage Error: 88.8%\n",
      "Mean Square: 27.4029\n",
      "\n",
      "[85.79, 84.69, 88.4, 88.04, 86.55, 86.44, 149.35, 94.55, 88.6, 432.92, 80.27, 154.7, 95.7, 82.87, 83.96, 84.39, 76.93, 69.16, 87.32, 81.52]\n",
      "Avarage Error: 109.11%, Adjusted Avarage Error: 88.58%\n",
      "Mean Square: 27.7541\n",
      "\n",
      "[85.7, 84.58, 88.36, 87.99, 86.48, 86.35, 148.94, 94.5, 88.52, 447.57, 80.15, 154.12, 95.67, 82.3, 83.83, 84.33, 76.68, 67.45, 87.41, 81.35]\n",
      "Avarage Error: 109.61%, Adjusted Avarage Error: 88.37%\n",
      "Mean Square: 28.1026\n",
      "\n",
      "[85.62, 84.46, 88.33, 87.94, 86.41, 86.26, 148.52, 94.44, 88.44, 469.15, 80.02, 153.15, 95.64, 81.88, 83.71, 84.27, 76.44, 65.51, 87.46, 81.19]\n",
      "Avarage Error: 110.44%, Adjusted Avarage Error: 88.14%\n",
      "Mean Square: 28.4496\n",
      "\n",
      "[99.8, 100.42, 98.79, 99.52, 99.74, 100.25, 98.21, 99.22, 99.36, 99.41, 99.43, 99.44, 99.52, 99.68, 99.92, 100.16, 100.4, 100.63, 100.8, 100.9]\n",
      "Avarage Error: 99.78%, Adjusted Avarage Error: 99.66%\n",
      "Mean Square: 1.4735\n",
      "\n",
      "[99.75, 101.07, 99.24, 99.47, 99.7, 100.15, 102.24, 97.54, 98.56, 98.76, 98.85, 99.03, 99.29, 99.59, 99.92, 100.22, 100.51, 100.75, 100.91, 100.98]\n",
      "Avarage Error: 99.83%, Adjusted Avarage Error: 99.62%\n",
      "Mean Square: 1.2299\n",
      "\n",
      "[99.7, 103.29, 99.21, 99.36, 99.62, 100.11, 100.86, 112.8, 96.3, 97.46, 97.92, 98.5, 99.07, 99.51, 99.93, 100.29, 100.61, 100.86, 101.01, 101.04]\n",
      "Avarage Error: 100.37%, Adjusted Avarage Error: 99.52%\n",
      "Mean Square: 1.1007\n",
      "\n",
      "[99.67, 92.19, 99.12, 99.15, 99.5, 100.08, 101.11, 154.44, 94.34, 95.15, 96.84, 98.06, 98.86, 99.43, 99.94, 100.41, 100.79, 101.05, 101.19, 101.15]\n",
      "Avarage Error: 101.62%, Adjusted Avarage Error: 98.71%\n",
      "Mean Square: 0.9524\n",
      "\n",
      "[99.65, 97.56, 99.0, 98.95, 99.35, 100.04, 101.88, 88.73, 90.08, 91.78, 95.68, 97.46, 98.59, 99.34, 99.98, 100.57, 101.06, 101.33, 101.41, 101.24]\n",
      "Avarage Error: 98.18%, Adjusted Avarage Error: 97.8%\n",
      "Mean Square: 0.8263\n",
      "\n",
      "[99.65, 98.31, 98.85, 98.74, 99.11, 99.94, 103.85, 83.74, 74.07, 80.54, 93.59, 96.78, 98.37, 99.28, 100.04, 100.79, 101.4, 101.72, 101.66, 101.29]\n",
      "Avarage Error: 96.59%, Adjusted Avarage Error: 95.9%\n",
      "Mean Square: 0.7174\n",
      "\n",
      "[99.66, 98.63, 98.62, 98.53, 98.74, 99.77, 108.64, -175.61, 210.33, 47.38, 90.9, 96.07, 98.21, 99.25, 100.11, 101.06, 101.9, 102.2, 101.97, 101.31]\n",
      "Avarage Error: 106.44%, Adjusted Avarage Error: 96.83%\n",
      "Mean Square: 0.6226\n",
      "\n",
      "[99.68, 98.69, 98.35, 98.24, 98.25, 99.5, 114.72, 122.95, 136.17, -52.44, 88.95, 95.87, 98.07, 99.25, 100.21, 101.35, 102.49, 102.82, 102.38, 101.33]\n",
      "Avarage Error: 100.59%, Adjusted Avarage Error: 97.37%\n",
      "Mean Square: 0.5349\n",
      "\n",
      "[99.73, 98.74, 98.11, 97.89, 97.73, 99.07, 108.62, 114.86, 133.24, -200.27, 90.96, 95.97, 98.04, 99.26, 100.33, 101.67, 103.22, 103.64, 102.81, 101.23]\n",
      "Avarage Error: 107.27%, Adjusted Avarage Error: 100.66%\n",
      "Mean Square: 0.4645\n",
      "\n",
      "[99.8, 98.76, 97.87, 97.45, 96.95, 98.3, 105.16, 113.01, 129.22, 40.65, 92.55, 96.33, 98.07, 99.27, 100.44, 101.95, 103.8, 104.52, 103.13, 100.9]\n",
      "Avarage Error: 98.91%, Adjusted Avarage Error: 96.44%\n",
      "Mean Square: 0.4133\n",
      "\n",
      "[99.91, 98.75, 97.61, 96.93, 96.0, 97.42, 104.0, 113.2, 128.87, 77.87, 93.85, 96.62, 98.08, 99.31, 100.56, 102.12, 104.2, 105.16, 103.28, 100.37]\n",
      "Avarage Error: 100.71%, Adjusted Avarage Error: 98.45%\n",
      "Mean Square: 0.3758\n",
      "\n",
      "[100.07, 98.73, 97.31, 96.3, 94.89, 96.76, 103.52, 113.03, 144.52, 87.5, 94.54, 96.7, 98.06, 99.34, 100.72, 102.42, 104.71, 105.63, 103.33, 99.74]\n",
      "Avarage Error: 101.89%, Adjusted Avarage Error: 98.9%\n",
      "Mean Square: 0.3403\n",
      "\n",
      "[100.31, 98.69, 97.03, 95.44, 93.85, 96.31, 103.5, 112.8, 11.14, 91.19, 95.06, 96.67, 98.0, 99.36, 100.89, 102.7, 104.8, 105.94, 103.26, 99.05]\n",
      "Avarage Error: 95.3%, Adjusted Avarage Error: 93.74%\n",
      "Mean Square: 0.309\n",
      "\n",
      "[100.65, 98.67, 96.7, 94.46, 92.79, 95.67, 103.36, 116.18, 77.97, 92.09, 95.12, 96.57, 97.92, 99.38, 101.11, 103.08, 105.13, 106.29, 103.28, 98.24]\n",
      "Avarage Error: 98.73%, Adjusted Avarage Error: 97.34%\n",
      "Mean Square: 0.2776\n",
      "\n",
      "[101.15, 98.67, 96.33, 93.57, 92.02, 95.03, 102.73, 124.1, 85.65, 92.56, 94.92, 96.39, 97.8, 99.39, 101.4, 103.63, 105.8, 106.68, 103.16, 97.33]\n",
      "Avarage Error: 99.41%, Adjusted Avarage Error: 97.64%\n",
      "Mean Square: 0.2462\n",
      "\n",
      "[101.94, 98.66, 95.95, 92.74, 91.36, 94.47, 102.21, 143.94, 87.05, 92.83, 94.7, 96.15, 97.63, 99.41, 101.74, 104.3, 106.45, 107.21, 103.08, 96.21]\n",
      "Avarage Error: 100.4%, Adjusted Avarage Error: 97.61%\n",
      "Mean Square: 0.2174\n",
      "\n",
      "[103.26, 98.66, 95.58, 91.83, 90.57, 93.91, 101.86, 1736.78, 87.34, 93.01, 94.49, 95.92, 97.44, 99.43, 102.18, 105.09, 107.64, 108.11, 103.17, 94.73]\n",
      "Avarage Error: 180.05%, Adjusted Avarage Error: 97.56%\n",
      "Mean Square: 0.1911\n",
      "\n",
      "[105.47, 98.72, 95.22, 90.92, 89.78, 93.89, 101.5, 19.31, 88.74, 92.94, 94.28, 95.67, 97.25, 99.47, 102.76, 106.14, 109.08, 109.21, 103.11, 92.97]\n",
      "Avarage Error: 94.32%, Adjusted Avarage Error: 92.67%\n",
      "Mean Square: 0.172\n",
      "\n",
      "[109.98, 98.78, 94.84, 90.11, 89.09, 93.9, 101.15, 31.02, 89.83, 92.69, 94.04, 95.37, 97.02, 99.52, 103.35, 107.4, 110.4, 110.57, 102.99, 90.57]\n",
      "Avarage Error: 95.13%, Adjusted Avarage Error: 93.42%\n",
      "Mean Square: 0.1567\n",
      "\n",
      "[121.41, 98.86, 94.47, 89.39, 88.62, 93.98, 100.89, -13.2, 90.44, 92.43, 93.76, 95.08, 96.74, 99.59, 104.19, 109.64, 112.3, 110.99, 102.43, 89.09]\n",
      "Avarage Error: 94.88%, Adjusted Avarage Error: 92.43%\n",
      "Mean Square: 0.1453\n",
      "\n",
      "[201.84, 98.96, 94.07, 88.86, 88.56, 94.01, 100.67, 29.85, 90.88, 92.23, 93.47, 94.78, 96.46, 99.67, 105.58, 113.35, 115.19, 110.91, 101.77, 87.83]\n",
      "Avarage Error: 99.95%, Adjusted Avarage Error: 93.44%\n",
      "Mean Square: 0.1367\n",
      "\n",
      "[14.72, 99.11, 93.64, 88.47, 88.74, 93.89, 100.42, 67.75, 91.1, 92.11, 93.2, 94.49, 96.18, 99.76, 107.72, 121.77, 121.3, 111.05, 101.21, 87.06]\n",
      "Avarage Error: 93.19%, Adjusted Avarage Error: 90.04%\n",
      "Mean Square: 0.1309\n",
      "\n",
      "[56.25, 99.33, 93.31, 88.17, 88.78, 93.79, 100.16, 79.09, 91.11, 92.0, 92.99, 94.21, 95.88, 99.89, 110.99, 158.4, 132.65, 111.71, 100.77, 86.16]\n",
      "Avarage Error: 98.28%, Adjusted Avarage Error: 93.03%\n",
      "Mean Square: 0.1276\n",
      "\n",
      "[67.5, 99.63, 93.02, 88.15, 88.67, 93.66, 99.89, 82.93, 91.07, 91.92, 92.77, 93.9, 95.54, 100.08, 118.2, -179.4, 161.34, 113.17, 100.38, 84.08]\n",
      "Avarage Error: 101.76%, Adjusted Avarage Error: 94.14%\n",
      "Mean Square: 0.1253\n",
      "\n",
      "[72.64, 100.03, 92.66, 88.12, 88.55, 93.47, 99.63, 84.86, 90.95, 91.77, 92.54, 93.56, 95.16, 100.34, 130.24, 43.18, 336.0, 116.82, 99.97, 81.55]\n",
      "Avarage Error: 104.6%, Adjusted Avarage Error: 90.32%\n",
      "Mean Square: 0.1239\n",
      "\n",
      "[76.9, 100.54, 92.32, 88.12, 88.57, 93.27, 99.38, 85.67, 90.79, 91.58, 92.3, 93.16, 94.66, 100.69, 172.57, 67.82, -32.59, 126.7, 99.52, 78.56]\n",
      "Avarage Error: 93.28%, Adjusted Avarage Error: 87.02%\n",
      "Mean Square: 0.1234\n",
      "\n",
      "[80.1, 101.2, 92.03, 88.18, 88.75, 92.97, 99.14, 86.31, 90.61, 91.39, 92.05, 92.79, 94.19, 101.16, -520.28, 75.17, 34.23, 147.97, 99.08, 80.55]\n",
      "Avarage Error: 112.41%, Adjusted Avarage Error: 87.77%\n",
      "Mean Square: 0.1284\n",
      "\n",
      "[82.4, 102.07, 91.78, 88.22, 88.93, 92.58, 98.87, 86.94, 90.47, 91.21, 91.8, 92.4, 93.71, 101.86, 35.63, 80.39, 59.66, 213.71, 98.57, 83.85]\n",
      "Avarage Error: 93.25%, Adjusted Avarage Error: 86.07%\n",
      "Mean Square: 0.1374\n",
      "\n",
      "[84.06, 103.34, 91.62, 88.23, 89.11, 92.2, 98.54, 87.48, 90.31, 91.03, 91.58, 92.04, 93.27, 102.88, 66.48, 83.52, 68.51, 584.09, 98.01, 84.07]\n",
      "Avarage Error: 114.02%, Adjusted Avarage Error: 88.5%\n",
      "Mean Square: 0.1472\n",
      "\n",
      "[85.4, 105.49, 91.46, 88.35, 89.12, 91.82, 98.15, 88.11, 90.18, 90.85, 91.38, 91.68, 92.76, 104.85, 77.07, 85.47, 73.54, 1267.66, 97.47, 83.66]\n",
      "Avarage Error: 149.22%, Adjusted Avarage Error: 89.52%\n",
      "Mean Square: 0.1591\n",
      "\n",
      "[86.47, 110.16, 91.3, 88.54, 89.22, 91.41, 97.65, 89.02, 90.05, 90.7, 91.21, 91.31, 92.24, 109.35, 81.83, 86.57, 76.83, 388.62, 96.94, 83.84]\n",
      "Avarage Error: 106.16%, Adjusted Avarage Error: 90.25%\n",
      "Mean Square: 0.1749\n",
      "\n",
      "[87.34, 124.12, 91.15, 88.81, 89.26, 90.98, 97.03, 89.65, 89.93, 90.56, 91.05, 90.92, 91.54, 139.34, 84.51, 87.0, 78.85, 221.27, 96.47, 84.91]\n",
      "Avarage Error: 100.23%, Adjusted Avarage Error: 91.34%\n",
      "Mean Square: 0.1954\n",
      "\n",
      "[87.89, 265.18, 91.03, 89.09, 89.29, 90.44, 96.31, 90.22, 89.8, 90.42, 90.89, 90.58, 91.0, 27.04, 86.04, 87.48, 80.87, -218.35, 96.08, 85.37]\n",
      "Avarage Error: 101.67%, Adjusted Avarage Error: 86.1%\n",
      "Mean Square: 0.2178\n",
      "\n",
      "[88.3, 35.83, 90.93, 89.27, 89.32, 89.83, 95.36, 90.53, 89.73, 90.31, 90.75, 90.29, 90.64, 76.39, 87.13, 87.71, 82.47, 39.95, 95.71, 85.83]\n",
      "Avarage Error: 83.82%, Adjusted Avarage Error: 82.51%\n",
      "Mean Square: 0.2436\n",
      "\n",
      "[88.6, 65.73, 90.88, 89.41, 89.38, 89.25, 94.06, 90.59, 89.73, 90.25, 90.63, 90.06, 90.43, 82.52, 87.96, 87.87, 83.87, 64.69, 95.63, 86.27]\n",
      "Avarage Error: 86.89%, Adjusted Avarage Error: 86.01%\n",
      "Mean Square: 0.2743\n",
      "\n",
      "[88.89, 75.56, 90.83, 89.52, 89.42, 88.99, 92.31, 90.68, 89.75, 90.21, 90.52, 89.84, 90.28, 85.24, 88.72, 88.03, 85.27, 74.95, 95.31, 86.54]\n",
      "Avarage Error: 88.04%, Adjusted Avarage Error: 87.4%\n",
      "Mean Square: 0.3093\n",
      "\n",
      "[89.13, 80.55, 90.74, 89.6, 89.44, 89.04, 91.46, 90.7, 89.8, 90.17, 90.42, 89.69, 90.15, 86.18, 89.33, 88.18, 86.45, 80.19, 94.82, 86.41]\n",
      "Avarage Error: 88.62%, Adjusted Avarage Error: 88.12%\n",
      "Mean Square: 0.346\n",
      "\n",
      "[89.3, 83.48, 90.64, 89.67, 89.5, 89.09, 90.84, 90.73, 89.84, 90.12, 90.33, 89.63, 90.01, 87.84, 89.65, 88.3, 87.19, 82.94, 94.13, 86.47]\n",
      "Avarage Error: 88.99%, Adjusted Avarage Error: 88.6%\n",
      "Mean Square: 0.3861\n",
      "\n",
      "[89.42, 85.32, 90.52, 89.72, 89.56, 89.15, 90.31, 90.76, 89.86, 90.08, 90.25, 89.58, 89.72, 88.26, 89.61, 88.33, 87.73, 85.78, 93.02, 87.0]\n",
      "Avarage Error: 89.2%, Adjusted Avarage Error: 88.9%\n",
      "Mean Square: 0.432\n",
      "\n",
      "[89.51, 86.62, 90.41, 89.75, 89.61, 89.27, 89.61, 90.78, 89.89, 90.04, 90.17, 89.62, 89.32, 88.15, 89.65, 88.35, 88.18, 87.66, 92.27, 87.63]\n",
      "Avarage Error: 89.33%, Adjusted Avarage Error: 89.08%\n",
      "Mean Square: 0.4859\n",
      "\n",
      "[89.57, 87.55, 90.31, 89.76, 89.65, 89.35, 89.07, 90.77, 89.89, 90.04, 90.09, 89.65, 89.16, 88.06, 89.73, 88.8, 88.81, 88.78, 91.75, 88.58]\n",
      "Avarage Error: 89.47%, Adjusted Avarage Error: 89.27%\n",
      "Mean Square: 0.5557\n",
      "\n",
      "[89.65, 88.18, 90.22, 89.78, 89.68, 89.38, 88.88, 90.62, 89.92, 90.03, 90.03, 89.72, 89.41, 87.74, 89.69, 89.14, 89.27, 89.46, 90.98, 89.07]\n",
      "Avarage Error: 89.54%, Adjusted Avarage Error: 89.4%\n",
      "Mean Square: 0.6261\n",
      "\n",
      "[89.76, 88.61, 90.14, 89.81, 89.73, 89.45, 89.01, 90.48, 89.93, 90.0, 89.98, 89.79, 89.57, 88.15, 89.66, 89.29, 89.47, 89.69, 90.55, 89.26]\n",
      "Avarage Error: 89.62%, Adjusted Avarage Error: 89.52%\n",
      "Mean Square: 0.6945\n",
      "\n",
      "[89.79, 88.97, 90.1, 89.84, 89.77, 89.55, 89.21, 90.4, 89.93, 89.98, 89.97, 89.83, 89.75, 88.87, 89.65, 89.48, 89.64, 89.83, 90.42, 89.48]\n",
      "Avarage Error: 89.72%, Adjusted Avarage Error: 89.65%\n",
      "Mean Square: 0.7694\n",
      "\n",
      "[89.83, 89.22, 90.08, 89.92, 89.81, 89.66, 89.48, 90.34, 89.93, 89.97, 89.97, 89.85, 89.86, 89.19, 89.69, 89.67, 89.84, 89.94, 90.39, 89.7]\n",
      "Avarage Error: 89.82%, Adjusted Avarage Error: 89.76%\n",
      "Mean Square: 0.8521\n",
      "\n",
      "[89.86, 89.44, 90.06, 89.96, 89.85, 89.74, 89.65, 90.27, 89.92, 89.95, 89.98, 89.9, 89.88, 89.4, 89.76, 89.82, 90.0, 90.02, 90.3, 89.88]\n",
      "Avarage Error: 89.88%, Adjusted Avarage Error: 89.84%\n",
      "Mean Square: 0.94\n",
      "\n",
      "[89.9, 89.59, 90.04, 89.98, 89.87, 89.8, 89.63, 90.14, 89.93, 89.95, 90.01, 89.94, 89.91, 89.55, 89.84, 89.86, 90.1, 90.05, 90.28, 89.97]\n",
      "Avarage Error: 89.92%, Adjusted Avarage Error: 89.89%\n",
      "Mean Square: 1.0288\n",
      "\n",
      "[89.93, 89.67, 90.04, 90.0, 89.89, 89.84, 89.62, 89.97, 89.93, 89.95, 90.04, 89.94, 89.91, 89.73, 89.93, 89.88, 90.11, 90.02, 90.26, 90.02]\n",
      "Avarage Error: 89.93%, Adjusted Avarage Error: 89.91%\n",
      "Mean Square: 1.1187\n",
      "\n",
      "[89.95, 89.7, 90.05, 90.0, 89.91, 89.89, 89.67, 89.9, 89.94, 89.97, 90.05, 89.95, 89.94, 90.06, 90.01, 89.9, 90.11, 90.04, 90.2, 90.06]\n",
      "Avarage Error: 89.96%, Adjusted Avarage Error: 89.94%\n",
      "Mean Square: 1.2145\n",
      "\n",
      "[89.97, 89.72, 90.07, 90.0, 89.94, 89.95, 89.7, 89.85, 89.96, 89.99, 90.04, 89.93, 89.96, 90.44, 90.05, 89.92, 90.09, 90.06, 90.12, 90.09]\n",
      "Avarage Error: 89.99%, Adjusted Avarage Error: 89.96%\n",
      "Mean Square: 1.3151\n",
      "\n",
      "[90.0, 89.75, 90.08, 90.01, 89.95, 89.96, 89.69, 89.85, 89.97, 89.99, 90.03, 89.91, 89.97, 90.34, 90.05, 89.95, 90.08, 90.08, 90.08, 90.1]\n",
      "Avarage Error: 89.99%, Adjusted Avarage Error: 89.97%\n",
      "Mean Square: 1.4197\n",
      "\n",
      "[90.0, 89.8, 90.08, 90.01, 89.98, 89.93, 89.69, 89.84, 89.97, 89.99, 90.04, 89.93, 90.02, 90.15, 90.03, 89.97, 90.08, 90.14, 90.07, 90.12]\n",
      "Avarage Error: 89.99%, Adjusted Avarage Error: 89.97%\n",
      "Mean Square: 1.5303\n",
      "\n",
      "[90.01, 89.84, 90.07, 89.99, 89.98, 89.93, 89.78, 89.86, 89.99, 89.99, 90.05, 89.94, 90.03, 90.04, 90.01, 89.99, 90.09, 90.17, 90.06, 90.13]\n",
      "Avarage Error: 90.0%, Adjusted Avarage Error: 89.98%\n",
      "Mean Square: 1.6466\n",
      "\n",
      "[90.01, 89.86, 90.07, 89.98, 89.98, 89.94, 89.85, 89.87, 90.0, 90.0, 90.05, 89.95, 89.99, 89.99, 89.99, 90.01, 90.09, 90.17, 90.04, 90.12]\n",
      "Avarage Error: 90.0%, Adjusted Avarage Error: 89.98%\n",
      "Mean Square: 1.7664\n",
      "\n",
      "[90.0, 89.87, 90.06, 89.97, 89.98, 89.95, 89.89, 89.89, 90.02, 89.98, 90.05, 89.96, 89.96, 90.05, 89.98, 90.02, 90.08, 90.16, 90.05, 90.11]\n",
      "Avarage Error: 90.0%, Adjusted Avarage Error: 89.99%\n",
      "Mean Square: 1.8919\n",
      "\n",
      "[90.0, 89.88, 90.05, 89.97, 89.99, 89.96, 89.97, 89.93, 90.04, 89.99, 90.03, 89.95, 89.9, 90.19, 89.99, 90.02, 90.08, 90.15, 90.04, 90.1]\n",
      "Avarage Error: 90.01%, Adjusted Avarage Error: 89.99%\n",
      "Mean Square: 2.0247\n",
      "\n",
      "[90.0, 89.87, 90.04, 89.97, 89.98, 89.98, 90.04, 89.96, 90.05, 89.99, 90.01, 89.94, 89.89, 90.22, 89.99, 90.01, 90.08, 90.14, 90.02, 90.1]\n",
      "Avarage Error: 90.01%, Adjusted Avarage Error: 90.0%\n",
      "Mean Square: 2.164\n",
      "\n",
      "[90.0, 89.85, 90.04, 89.97, 89.97, 90.0, 90.09, 89.96, 90.05, 89.98, 90.0, 89.94, 89.87, 90.17, 90.0, 90.01, 90.07, 90.13, 90.0, 90.09]\n",
      "Avarage Error: 90.01%, Adjusted Avarage Error: 89.99%\n",
      "Mean Square: 2.3095\n",
      "\n",
      "[90.0, 89.83, 90.03, 89.98, 89.97, 90.03, 90.09, 90.0, 90.07, 89.96, 90.0, 89.95, 89.89, 90.1, 90.02, 90.02, 90.06, 90.11, 89.98, 90.08]\n",
      "Avarage Error: 90.01%, Adjusted Avarage Error: 90.0%\n",
      "Mean Square: 2.4635\n",
      "\n",
      "[90.01, 89.82, 90.04, 89.98, 89.98, 90.03, 90.03, 90.06, 90.09, 89.96, 89.99, 89.98, 89.91, 89.96, 90.04, 90.02, 90.05, 90.1, 89.96, 90.07]\n",
      "Avarage Error: 90.0%, Adjusted Avarage Error: 89.99%\n",
      "Mean Square: 2.6256\n",
      "\n",
      "[90.01, 89.8, 90.05, 89.99, 89.99, 90.03, 90.0, 90.07, 90.11, 89.96, 90.0, 90.0, 89.97, 89.91, 90.02, 90.02, 90.03, 90.08, 89.95, 90.05]\n",
      "Avarage Error: 90.0%, Adjusted Avarage Error: 89.99%\n",
      "Mean Square: 2.7936\n",
      "\n",
      "[90.01, 89.8, 90.07, 90.0, 90.01, 90.03, 90.02, 90.06, 90.11, 89.97, 90.0, 89.99, 90.02, 89.83, 89.99, 90.03, 90.03, 90.08, 89.94, 90.04]\n",
      "Avarage Error: 90.0%, Adjusted Avarage Error: 89.99%\n",
      "Mean Square: 2.972\n",
      "\n",
      "[90.02, 89.8, 90.07, 90.01, 90.03, 90.02, 90.05, 90.06, 90.1, 89.99, 90.0, 89.98, 90.05, 89.83, 89.98, 90.04, 90.03, 90.08, 89.94, 90.04]\n",
      "Avarage Error: 90.01%, Adjusted Avarage Error: 90.0%\n",
      "Mean Square: 3.1611\n",
      "\n",
      "[90.02, 89.82, 90.06, 90.01, 90.05, 90.02, 90.06, 90.07, 90.1, 90.01, 90.03, 89.99, 90.07, 89.95, 90.0, 90.05, 90.02, 90.06, 89.96, 90.03]\n",
      "Avarage Error: 90.02%, Adjusted Avarage Error: 90.01%\n",
      "Mean Square: 3.3602\n",
      "\n",
      "[90.02, 89.83, 90.04, 90.03, 90.06, 90.03, 90.11, 90.07, 90.09, 90.02, 90.04, 90.02, 90.08, 90.05, 90.01, 90.05, 90.02, 90.05, 89.98, 90.03]\n",
      "Avarage Error: 90.03%, Adjusted Avarage Error: 90.02%\n",
      "Mean Square: 3.5672\n",
      "\n",
      "[90.03, 89.85, 90.03, 90.03, 90.07, 90.05, 90.16, 90.06, 90.08, 90.03, 90.05, 90.05, 90.09, 90.11, 90.03, 90.04, 90.02, 90.03, 90.0, 90.03]\n",
      "Avarage Error: 90.04%, Adjusted Avarage Error: 90.03%\n",
      "Mean Square: 3.784\n",
      "\n",
      "[90.04, 89.87, 90.01, 90.03, 90.07, 90.06, 90.19, 90.05, 90.06, 90.03, 90.06, 90.07, 90.07, 90.23, 90.06, 90.04, 90.02, 90.02, 90.01, 90.02]\n",
      "Avarage Error: 90.05%, Adjusted Avarage Error: 90.03%\n",
      "Mean Square: 4.0103\n",
      "\n",
      "[90.05, 89.89, 90.0, 90.02, 90.06, 90.06, 90.19, 90.05, 90.05, 90.02, 90.06, 90.06, 90.08, 90.19, 90.05, 90.03, 90.02, 90.03, 90.01, 90.03]\n",
      "Avarage Error: 90.05%, Adjusted Avarage Error: 90.03%\n",
      "Mean Square: 4.2447\n",
      "\n",
      "[90.06, 89.91, 90.0, 90.02, 90.04, 90.07, 90.21, 90.04, 90.05, 90.01, 90.06, 90.05, 90.08, 90.16, 90.01, 90.01, 90.01, 90.04, 90.01, 90.03]\n",
      "Avarage Error: 90.04%, Adjusted Avarage Error: 90.03%\n",
      "Mean Square: 4.4862\n",
      "\n",
      "[90.06, 89.93, 90.01, 90.01, 90.03, 90.07, 90.22, 90.04, 90.03, 90.02, 90.05, 90.03, 90.07, 90.14, 89.97, 90.0, 90.0, 90.04, 90.0, 90.02]\n",
      "Avarage Error: 90.04%, Adjusted Avarage Error: 90.02%\n",
      "Mean Square: 4.7339\n",
      "\n",
      "[90.05, 89.93, 90.02, 90.0, 90.02, 90.07, 90.22, 90.04, 90.02, 90.02, 90.05, 90.03, 90.06, 90.1, 89.97, 89.99, 89.99, 90.03, 89.99, 90.02]\n",
      "Avarage Error: 90.03%, Adjusted Avarage Error: 90.02%\n",
      "Mean Square: 4.9916\n",
      "\n",
      "[90.04, 89.93, 90.03, 90.0, 90.01, 90.08, 90.23, 90.04, 90.02, 90.03, 90.05, 90.02, 90.04, 90.06, 89.96, 89.98, 89.98, 90.01, 89.96, 90.0]\n",
      "Avarage Error: 90.02%, Adjusted Avarage Error: 90.01%\n",
      "Mean Square: 5.2564\n",
      "\n",
      "[90.03, 89.93, 90.04, 90.0, 90.01, 90.08, 90.24, 90.03, 90.03, 90.02, 90.05, 90.01, 90.02, 90.03, 89.95, 89.97, 89.96, 89.99, 89.93, 89.99]\n",
      "Avarage Error: 90.02%, Adjusted Avarage Error: 90.0%\n",
      "Mean Square: 5.5296\n",
      "\n",
      "[90.01, 89.93, 90.05, 90.0, 89.99, 90.06, 90.21, 90.01, 90.03, 90.01, 90.05, 90.01, 90.0, 90.04, 89.96, 89.97, 89.93, 89.97, 89.9, 89.97]\n",
      "Avarage Error: 90.01%, Adjusted Avarage Error: 89.99%\n",
      "Mean Square: 5.8103\n",
      "\n",
      "[90.0, 89.93, 90.05, 89.99, 89.99, 90.04, 90.15, 90.0, 90.02, 90.01, 90.05, 90.02, 90.0, 90.05, 90.0, 89.98, 89.91, 89.93, 89.89, 89.96]\n",
      "Avarage Error: 90.0%, Adjusted Avarage Error: 89.99%\n",
      "Mean Square: 6.1057\n",
      "\n",
      "[89.98, 89.92, 90.06, 89.99, 90.0, 90.01, 90.1, 89.99, 90.03, 89.99, 90.06, 90.03, 90.02, 90.07, 90.02, 90.0, 89.89, 89.91, 89.87, 89.94]\n",
      "Avarage Error: 89.99%, Adjusted Avarage Error: 89.98%\n",
      "Mean Square: 6.4096\n",
      "\n",
      "[89.95, 89.92, 90.06, 89.98, 90.0, 89.98, 90.05, 89.99, 90.03, 89.97, 90.07, 90.04, 90.03, 90.08, 90.03, 90.01, 89.88, 89.9, 89.85, 89.93]\n",
      "Avarage Error: 89.99%, Adjusted Avarage Error: 89.98%\n",
      "Mean Square: 6.7241\n",
      "\n",
      "[89.92, 89.9, 90.06, 89.99, 90.0, 89.97, 90.03, 90.01, 90.04, 89.97, 90.07, 90.05, 90.05, 90.07, 90.03, 90.03, 89.88, 89.92, 89.83, 89.92]\n",
      "Avarage Error: 89.99%, Adjusted Avarage Error: 89.98%\n",
      "Mean Square: 7.0555\n",
      "\n",
      "[89.9, 89.88, 90.06, 90.0, 90.0, 89.97, 90.03, 90.04, 90.04, 89.98, 90.08, 90.05, 90.07, 90.06, 90.03, 90.04, 89.89, 89.96, 89.82, 89.93]\n",
      "Avarage Error: 89.99%, Adjusted Avarage Error: 89.98%\n",
      "Mean Square: 7.4088\n",
      "\n",
      "[89.87, 89.85, 90.04, 89.99, 90.01, 89.98, 90.04, 90.07, 90.04, 90.01, 90.07, 90.05, 90.09, 90.04, 90.03, 90.04, 89.91, 90.01, 89.82, 89.94]\n",
      "Avarage Error: 90.0%, Adjusted Avarage Error: 89.99%\n",
      "Mean Square: 7.778\n",
      "\n",
      "[89.84, 89.82, 90.02, 89.99, 90.01, 89.98, 90.03, 90.07, 90.04, 90.0, 90.06, 90.05, 90.11, 90.01, 90.02, 90.04, 89.95, 90.04, 89.84, 89.96]\n",
      "Avarage Error: 89.99%, Adjusted Avarage Error: 89.98%\n",
      "Mean Square: 8.1533\n",
      "\n",
      "[89.8, 89.8, 90.0, 89.99, 90.01, 89.98, 90.0, 90.06, 90.04, 89.97, 90.04, 90.05, 90.11, 90.01, 90.0, 90.01, 89.89, 89.96, 89.79, 89.89]\n",
      "Avarage Error: 89.97%, Adjusted Avarage Error: 89.96%\n",
      "Mean Square: 8.4807\n",
      "\n",
      "[89.76, 89.78, 89.96, 89.98, 89.99, 89.97, 89.96, 90.04, 90.03, 89.94, 90.02, 90.06, 90.09, 90.02, 89.97, 89.96, 89.79, 89.85, 89.71, 89.79]\n",
      "Avarage Error: 89.93%, Adjusted Avarage Error: 89.92%\n",
      "Mean Square: 8.786\n",
      "\n",
      "[89.72, 89.75, 89.92, 89.97, 89.97, 89.95, 89.93, 90.02, 90.01, 89.92, 90.0, 90.06, 90.07, 90.03, 89.94, 89.91, 89.69, 89.73, 89.63, 89.69]\n",
      "Avarage Error: 89.9%, Adjusted Avarage Error: 89.88%\n",
      "Mean Square: 9.0954\n",
      "\n",
      "[89.68, 89.72, 89.89, 89.96, 89.95, 89.93, 89.84, 90.01, 89.99, 89.91, 89.96, 90.06, 90.04, 90.02, 89.89, 89.85, 89.58, 89.62, 89.54, 89.59]\n",
      "Avarage Error: 89.85%, Adjusted Avarage Error: 89.83%\n",
      "Mean Square: 9.4053\n",
      "\n",
      "[89.63, 89.68, 89.85, 89.95, 89.92, 89.91, 89.75, 90.03, 89.97, 89.91, 89.91, 90.04, 90.0, 90.0, 89.83, 89.79, 89.46, 89.49, 89.44, 89.47]\n",
      "Avarage Error: 89.8%, Adjusted Avarage Error: 89.78%\n",
      "Mean Square: 9.7173\n",
      "\n",
      "[89.59, 89.65, 89.81, 89.93, 89.9, 89.89, 89.7, 90.08, 89.95, 89.9, 89.86, 90.01, 89.96, 89.98, 89.77, 89.73, 89.35, 89.37, 89.34, 89.36]\n",
      "Avarage Error: 89.76%, Adjusted Avarage Error: 89.72%\n",
      "Mean Square: 10.0311\n",
      "\n",
      "[89.55, 89.61, 89.77, 89.91, 89.88, 89.88, 89.67, 90.15, 89.92, 89.92, 89.78, 89.98, 89.92, 89.96, 89.71, 89.67, 89.23, 89.24, 89.24, 89.24]\n",
      "Avarage Error: 89.71%, Adjusted Avarage Error: 89.67%\n",
      "Mean Square: 10.3516\n",
      "\n",
      "[89.51, 89.57, 89.74, 89.89, 89.85, 89.86, 89.63, 90.22, 89.89, 89.89, 89.69, 89.94, 89.88, 89.93, 89.64, 89.6, 89.11, 89.12, 89.14, 89.13]\n",
      "Avarage Error: 89.66%, Adjusted Avarage Error: 89.62%\n",
      "Mean Square: 10.6653\n",
      "\n",
      "[89.46, 89.52, 89.7, 89.87, 89.82, 89.83, 89.59, 90.27, 89.88, 89.82, 89.54, 89.85, 89.83, 89.91, 89.58, 89.52, 88.98, 88.99, 89.02, 89.0]\n",
      "Avarage Error: 89.6%, Adjusted Avarage Error: 89.55%\n",
      "Mean Square: 10.9603\n",
      "\n",
      "[89.42, 89.48, 89.66, 89.85, 89.79, 89.81, 89.56, 90.32, 89.87, 89.66, 89.29, 89.68, 89.78, 89.88, 89.5, 89.45, 88.84, 88.85, 88.91, 88.87]\n",
      "Avarage Error: 89.52%, Adjusted Avarage Error: 89.46%\n",
      "Mean Square: 11.2225\n",
      "\n",
      "[89.37, 89.43, 89.62, 89.82, 89.76, 89.78, 89.54, 90.35, 89.87, 89.38, 88.68, 89.62, 89.71, 89.84, 89.37, 89.36, 88.71, 88.72, 88.8, 88.74]\n",
      "Avarage Error: 89.42%, Adjusted Avarage Error: 89.35%\n",
      "Mean Square: 11.4384\n",
      "\n",
      "[89.32, 89.38, 89.58, 89.8, 89.72, 89.75, 89.52, 90.41, 89.93, 89.07, 87.64, 89.77, 89.64, 89.8, 89.23, 89.27, 88.59, 88.6, 88.69, 88.61]\n",
      "Avarage Error: 89.32%, Adjusted Avarage Error: 89.22%\n",
      "Mean Square: 11.645\n",
      "\n",
      "[89.27, 89.32, 89.54, 89.78, 89.69, 89.71, 89.49, 90.48, 90.01, 88.68, 86.73, 90.14, 89.57, 89.75, 89.1, 89.18, 88.45, 88.46, 88.58, 88.47]\n",
      "Avarage Error: 89.22%, Adjusted Avarage Error: 89.1%\n",
      "Mean Square: 11.8961\n",
      "\n",
      "[89.21, 89.26, 89.5, 89.75, 89.65, 89.68, 89.42, 90.55, 90.18, 88.24, 86.52, 90.36, 89.49, 89.72, 88.97, 89.09, 88.32, 88.33, 88.48, 88.34]\n",
      "Avarage Error: 89.15%, Adjusted Avarage Error: 89.01%\n",
      "Mean Square: 12.1924\n",
      "\n",
      "[89.13, 89.19, 89.45, 89.73, 89.61, 89.65, 89.3, 90.6, 90.41, 87.65, 86.76, 90.54, 89.4, 89.7, 88.84, 89.0, 88.18, 88.19, 88.38, 88.2]\n",
      "Avarage Error: 89.09%, Adjusted Avarage Error: 88.93%\n",
      "Mean Square: 12.513\n",
      "\n",
      "[89.06, 89.11, 89.4, 89.7, 89.57, 89.61, 89.17, 90.64, 90.65, 86.84, 87.46, 90.62, 89.31, 89.67, 88.73, 88.9, 88.04, 88.04, 88.28, 88.07]\n",
      "Avarage Error: 89.04%, Adjusted Avarage Error: 88.87%\n",
      "Mean Square: 12.8639\n",
      "\n",
      "[88.98, 89.04, 89.35, 89.67, 89.53, 89.57, 89.04, 90.68, 90.87, 85.84, 88.26, 90.65, 89.22, 89.66, 88.62, 88.8, 87.9, 87.89, 88.19, 87.94]\n",
      "Avarage Error: 88.98%, Adjusted Avarage Error: 88.79%\n",
      "Mean Square: 13.2419\n",
      "\n",
      "[88.91, 88.97, 89.29, 89.64, 89.49, 89.53, 88.9, 90.7, 90.92, 84.99, 89.3, 90.71, 89.14, 89.64, 88.53, 88.7, 87.76, 87.73, 88.1, 87.81]\n",
      "Avarage Error: 88.94%, Adjusted Avarage Error: 88.73%\n",
      "Mean Square: 13.6927\n",
      "\n",
      "[88.83, 88.89, 89.24, 89.61, 89.44, 89.49, 88.74, 90.72, 90.92, 84.47, 90.19, 90.81, 89.05, 89.62, 88.43, 88.6, 87.61, 87.56, 88.01, 87.68]\n",
      "Avarage Error: 88.9%, Adjusted Avarage Error: 88.68%\n",
      "Mean Square: 14.1991\n",
      "\n",
      "[88.76, 88.81, 89.18, 89.57, 89.39, 89.44, 88.57, 90.74, 90.91, 84.47, 90.64, 90.95, 88.94, 89.61, 88.34, 88.5, 87.46, 87.38, 87.91, 87.54]\n",
      "Avarage Error: 88.86%, Adjusted Avarage Error: 88.63%\n",
      "Mean Square: 14.6891\n",
      "\n",
      "[88.69, 88.73, 89.12, 89.54, 89.34, 89.4, 88.41, 90.77, 90.92, 84.86, 90.98, 91.04, 88.83, 89.59, 88.27, 88.4, 87.3, 87.19, 87.82, 87.41]\n",
      "Avarage Error: 88.83%, Adjusted Avarage Error: 88.59%\n",
      "Mean Square: 15.196\n",
      "\n",
      "[88.62, 88.65, 89.06, 89.5, 89.29, 89.35, 88.24, 90.8, 90.95, 85.37, 91.28, 91.13, 88.69, 89.57, 88.21, 88.29, 87.12, 86.99, 87.72, 87.27]\n",
      "Avarage Error: 88.8%, Adjusted Avarage Error: 88.54%\n",
      "Mean Square: 15.722\n",
      "\n",
      "[88.54, 88.56, 89.01, 89.46, 89.24, 89.3, 88.04, 90.83, 90.97, 85.96, 91.52, 91.19, 88.55, 89.55, 88.16, 88.18, 86.94, 86.77, 87.62, 87.12]\n",
      "Avarage Error: 88.78%, Adjusted Avarage Error: 88.49%\n",
      "Mean Square: 16.2541\n",
      "\n",
      "[88.47, 88.48, 88.96, 89.42, 89.19, 89.25, 87.78, 90.85, 90.97, 86.52, 91.57, 91.22, 88.4, 89.51, 88.1, 88.07, 86.76, 86.55, 87.54, 86.98]\n",
      "Avarage Error: 88.73%, Adjusted Avarage Error: 88.43%\n",
      "Mean Square: 16.7232\n",
      "\n",
      "[88.4, 88.39, 88.91, 89.39, 89.13, 89.2, 87.36, 90.87, 90.96, 86.94, 91.39, 91.25, 88.25, 89.47, 88.03, 87.96, 86.57, 86.32, 87.47, 86.84]\n",
      "Avarage Error: 88.65%, Adjusted Avarage Error: 88.36%\n",
      "Mean Square: 17.0889\n",
      "\n",
      "[88.32, 88.31, 88.87, 89.35, 89.07, 89.15, 86.88, 90.9, 90.96, 86.83, 90.95, 91.23, 88.09, 89.42, 87.95, 87.84, 86.38, 86.09, 87.4, 86.69]\n",
      "Avarage Error: 88.53%, Adjusted Avarage Error: 88.25%\n",
      "Mean Square: 17.2841\n",
      "\n",
      "[88.25, 88.22, 88.83, 89.32, 89.01, 89.1, 86.34, 90.94, 90.93, 83.82, 89.68, 91.06, 87.92, 89.37, 87.86, 87.72, 86.17, 85.83, 87.33, 86.54]\n",
      "Avarage Error: 88.21%, Adjusted Avarage Error: 87.9%\n",
      "Mean Square: 16.9553\n",
      "\n",
      "[88.17, 88.13, 88.8, 89.28, 88.95, 89.05, 85.73, 90.99, 90.91, 69.66, 88.57, 90.71, 87.78, 89.32, 87.77, 87.61, 85.97, 85.58, 87.25, 86.38]\n",
      "Avarage Error: 87.33%, Adjusted Avarage Error: 86.93%\n",
      "Mean Square: 16.6289\n",
      "\n",
      "[88.1, 88.04, 88.77, 89.25, 88.89, 88.99, 85.02, 91.05, 90.87, 370.27, 87.85, 90.0, 86.69, 89.27, 87.67, 87.5, 85.76, 85.32, 87.17, 86.23]\n",
      "Avarage Error: 102.14%, Adjusted Avarage Error: 87.86%\n",
      "Mean Square: 16.6324\n",
      "\n",
      "[88.02, 87.96, 88.74, 89.21, 88.82, 88.94, 84.04, 91.15, 90.83, 134.26, 87.29, 88.1, 84.9, 89.22, 87.58, 87.38, 85.56, 85.07, 87.08, 86.08]\n",
      "Avarage Error: 90.01%, Adjusted Avarage Error: 87.49%\n",
      "Mean Square: 16.8387\n",
      "\n",
      "[87.95, 87.87, 88.71, 89.18, 88.76, 88.89, 82.72, 91.28, 90.79, 125.65, 86.79, 84.01, 86.21, 89.15, 87.48, 87.27, 85.35, 84.82, 87.0, 85.92]\n",
      "Avarage Error: 89.29%, Adjusted Avarage Error: 87.16%\n",
      "Mean Square: 16.9922\n",
      "\n",
      "[87.87, 87.77, 88.69, 89.15, 88.7, 88.83, 81.0, 91.47, 90.75, 124.16, 86.31, 75.72, 87.56, 89.09, 87.39, 87.16, 85.1, 84.55, 86.92, 85.76]\n",
      "Avarage Error: 88.7%, Adjusted Avarage Error: 86.57%\n",
      "Mean Square: 17.1358\n",
      "\n",
      "[87.8, 87.68, 88.67, 89.12, 88.64, 88.77, 78.49, 91.75, 90.7, 124.25, 85.79, 52.68, 89.5, 89.02, 87.32, 87.04, 84.83, 84.24, 86.84, 85.6]\n",
      "Avarage Error: 87.44%, Adjusted Avarage Error: 85.15%\n",
      "Mean Square: 17.3552\n",
      "\n",
      "[87.72, 87.58, 88.65, 89.1, 88.59, 88.71, 75.45, 92.06, 90.65, 125.46, 85.27, -10.45, 92.18, 88.94, 87.24, 86.92, 84.55, 83.93, 86.77, 85.44]\n",
      "Avarage Error: 85.28%, Adjusted Avarage Error: 82.67%\n",
      "Mean Square: 17.7075\n",
      "\n",
      "[87.65, 87.49, 88.64, 89.08, 88.53, 88.65, 71.78, 92.42, 90.59, 128.75, 84.76, -193.91, 93.49, 88.85, 87.18, 86.81, 84.26, 83.6, 86.71, 85.28]\n",
      "Avarage Error: 94.42%, Adjusted Avarage Error: 86.99%\n",
      "Mean Square: 18.0514\n",
      "\n",
      "[87.57, 87.39, 88.62, 89.06, 88.47, 88.59, 67.38, 92.83, 90.54, 133.84, 84.29, -3398.55, 94.61, 88.76, 87.12, 86.69, 83.97, 83.24, 86.66, 85.12]\n",
      "Avarage Error: 254.66%, Adjusted Avarage Error: 86.72%\n",
      "Mean Square: 18.4808\n",
      "\n",
      "[87.48, 87.28, 88.61, 89.03, 88.41, 88.52, 61.33, 93.22, 90.48, 142.09, 83.84, 644.88, 95.25, 88.66, 87.08, 86.57, 83.67, 82.89, 86.61, 84.95]\n",
      "Avarage Error: 117.04%, Adjusted Avarage Error: 86.33%\n",
      "Mean Square: 18.9192\n",
      "\n",
      "[87.4, 87.17, 88.61, 89.0, 88.35, 88.46, 52.84, 93.45, 90.41, 152.36, 83.54, 380.52, 95.62, 88.55, 87.04, 86.45, 83.36, 82.53, 86.56, 84.78]\n",
      "Avarage Error: 103.85%, Adjusted Avarage Error: 85.79%\n",
      "Mean Square: 19.3501\n",
      "\n",
      "[87.32, 87.06, 88.61, 88.97, 88.28, 88.39, 40.28, 93.67, 90.35, 165.89, 83.36, 296.66, 95.82, 88.45, 86.98, 86.33, 83.07, 82.2, 86.52, 84.62]\n",
      "Avarage Error: 99.64%, Adjusted Avarage Error: 85.02%\n",
      "Mean Square: 19.7732\n",
      "\n",
      "[87.24, 86.95, 88.61, 88.94, 88.21, 88.32, 21.33, 93.83, 90.28, 184.18, 83.19, 261.27, 95.95, 88.33, 86.91, 86.22, 82.79, 81.89, 86.51, 84.45]\n",
      "Avarage Error: 97.77%, Adjusted Avarage Error: 83.89%\n",
      "Mean Square: 20.1758\n",
      "\n",
      "[87.16, 86.84, 88.61, 88.91, 88.14, 88.25, -5.78, 93.98, 90.22, 208.6, 83.04, 241.54, 96.02, 88.21, 86.84, 86.1, 82.51, 81.57, 86.53, 84.28]\n",
      "Avarage Error: 97.16%, Adjusted Avarage Error: 82.94%\n",
      "Mean Square: 20.57\n",
      "\n",
      "[87.08, 86.73, 88.6, 88.87, 88.07, 88.19, -48.42, 94.11, 90.15, 229.99, 82.88, 230.77, 96.06, 88.08, 86.77, 85.97, 82.23, 81.22, 86.53, 84.12]\n",
      "Avarage Error: 99.74%, Adjusted Avarage Error: 85.23%\n",
      "Mean Square: 20.9608\n",
      "\n",
      "[87.0, 86.61, 88.59, 88.84, 88.0, 88.12, -132.61, 94.23, 90.08, 250.65, 82.72, 220.07, 96.07, 87.95, 86.66, 85.85, 81.96, 80.83, 86.49, 83.97]\n",
      "Avarage Error: 104.37%, Adjusted Avarage Error: 89.81%\n",
      "Mean Square: 21.348\n",
      "\n",
      "[86.92, 86.5, 88.59, 88.8, 87.92, 88.05, -320.01, 94.37, 90.01, 265.68, 82.56, 210.57, 96.08, 87.81, 86.55, 85.74, 81.71, 80.43, 86.46, 83.82]\n",
      "Avarage Error: 113.93%, Adjusted Avarage Error: 94.05%\n",
      "Mean Square: 21.7442\n",
      "\n",
      "[86.83, 86.38, 88.58, 88.77, 87.85, 87.98, -1097.83, 94.48, 89.94, 267.82, 82.41, 201.4, 96.08, 87.66, 86.42, 85.63, 81.43, 80.02, 86.43, 83.67]\n",
      "Avarage Error: 152.38%, Adjusted Avarage Error: 93.44%\n",
      "Mean Square: 22.134\n",
      "\n",
      "[86.75, 86.27, 88.57, 88.73, 87.78, 87.91, 1693.25, 94.57, 89.87, 265.25, 82.26, 193.33, 96.06, 87.49, 86.28, 85.53, 81.12, 79.55, 86.41, 83.52]\n",
      "Avarage Error: 181.52%, Adjusted Avarage Error: 92.89%\n",
      "Mean Square: 22.5168\n",
      "\n",
      "[86.67, 86.15, 88.55, 88.69, 87.7, 87.84, 543.47, 94.65, 89.8, 262.96, 82.11, 185.68, 96.04, 87.32, 86.15, 85.43, 80.82, 79.08, 86.4, 83.37]\n",
      "Avarage Error: 123.44%, Adjusted Avarage Error: 92.36%\n",
      "Mean Square: 22.8996\n",
      "\n",
      "[86.58, 86.03, 88.54, 88.65, 87.63, 87.77, 363.11, 94.71, 89.72, 256.25, 81.93, 178.88, 96.02, 87.17, 86.0, 85.33, 80.54, 78.6, 86.38, 83.21]\n",
      "Avarage Error: 113.65%, Adjusted Avarage Error: 91.87%\n",
      "Mean Square: 23.2767\n",
      "\n",
      "[86.5, 85.92, 88.53, 88.61, 87.56, 87.69, 290.48, 94.74, 89.65, 247.67, 81.76, 173.24, 96.0, 87.06, 85.87, 85.24, 80.31, 78.09, 86.37, 83.06]\n",
      "Avarage Error: 109.22%, Adjusted Avarage Error: 91.45%\n",
      "Mean Square: 23.6542\n",
      "\n",
      "[86.41, 85.8, 88.51, 88.57, 87.48, 87.62, 250.54, 94.76, 89.58, 239.84, 81.58, 168.04, 95.97, 86.95, 85.73, 85.16, 80.08, 77.51, 86.38, 82.91]\n",
      "Avarage Error: 106.47%, Adjusted Avarage Error: 91.06%\n",
      "Mean Square: 24.0299\n",
      "\n",
      "[86.32, 85.69, 88.49, 88.53, 87.41, 87.55, 225.54, 94.78, 89.5, 233.34, 81.42, 163.17, 95.94, 86.69, 85.6, 85.08, 79.86, 76.88, 86.4, 82.76]\n",
      "Avarage Error: 104.55%, Adjusted Avarage Error: 90.67%\n",
      "Mean Square: 24.4061\n",
      "\n",
      "[86.23, 85.57, 88.47, 88.49, 87.34, 87.48, 209.61, 94.78, 89.43, 228.08, 81.25, 158.89, 95.91, 86.43, 85.49, 85.0, 79.65, 76.24, 86.41, 82.61]\n",
      "Avarage Error: 103.17%, Adjusted Avarage Error: 90.31%\n",
      "Mean Square: 24.7836\n",
      "\n",
      "[86.15, 85.45, 88.44, 88.45, 87.27, 87.4, 198.67, 94.78, 89.35, 223.73, 81.09, 155.25, 95.88, 86.11, 85.4, 84.92, 79.44, 75.6, 86.44, 82.46]\n",
      "Avarage Error: 102.12%, Adjusted Avarage Error: 89.99%\n",
      "Mean Square: 25.1673\n",
      "\n",
      "[86.06, 85.33, 88.42, 88.41, 87.19, 87.32, 189.49, 94.78, 89.28, 220.29, 80.93, 152.82, 95.85, 85.79, 85.32, 84.85, 79.26, 74.99, 86.49, 82.31]\n",
      "Avarage Error: 101.26%, Adjusted Avarage Error: 89.74%\n",
      "Mean Square: 25.5559\n",
      "\n",
      "[85.97, 85.22, 88.4, 88.37, 87.12, 87.24, 184.46, 94.77, 89.2, 217.56, 80.77, 151.03, 95.82, 85.47, 85.22, 84.78, 79.06, 74.34, 86.56, 82.17]\n",
      "Avarage Error: 100.68%, Adjusted Avarage Error: 89.53%\n",
      "Mean Square: 25.9429\n",
      "\n",
      "[85.88, 85.1, 88.37, 88.32, 87.05, 87.15, 180.44, 94.76, 89.12, 215.36, 80.61, 150.36, 95.79, 85.15, 85.13, 84.71, 78.84, 73.61, 86.61, 82.03]\n",
      "Avarage Error: 100.22%, Adjusted Avarage Error: 89.37%\n",
      "Mean Square: 26.3216\n",
      "\n",
      "[85.79, 84.98, 88.35, 88.28, 86.98, 87.07, 177.28, 94.74, 89.04, 213.68, 80.46, 149.94, 95.76, 84.82, 85.07, 84.64, 78.64, 72.85, 86.68, 81.88]\n",
      "Avarage Error: 99.85%, Adjusted Avarage Error: 89.22%\n",
      "Mean Square: 26.7055\n",
      "\n",
      "[85.69, 84.86, 88.32, 88.23, 86.91, 86.98, 174.89, 94.73, 88.96, 212.53, 80.31, 149.4, 95.73, 84.34, 85.02, 84.57, 78.44, 72.04, 86.74, 81.73]\n",
      "Avarage Error: 99.52%, Adjusted Avarage Error: 89.06%\n",
      "Mean Square: 27.0921\n",
      "\n",
      "[85.61, 84.74, 88.29, 88.19, 86.83, 86.89, 172.77, 94.71, 88.89, 212.13, 80.16, 149.21, 95.7, 83.67, 85.01, 84.51, 78.25, 71.17, 86.81, 81.59]\n",
      "Avarage Error: 99.26%, Adjusted Avarage Error: 88.9%\n",
      "Mean Square: 27.4841\n",
      "\n",
      "[85.51, 84.62, 88.26, 88.14, 86.76, 86.8, 170.97, 94.69, 88.81, 212.02, 80.01, 149.21, 95.67, 83.16, 85.03, 84.46, 78.1, 70.25, 86.88, 81.44]\n",
      "Avarage Error: 99.04%, Adjusted Avarage Error: 88.77%\n",
      "Mean Square: 27.8837\n",
      "\n",
      "[85.42, 84.5, 88.23, 88.1, 86.69, 86.72, 169.68, 94.68, 88.73, 211.82, 79.88, 149.2, 95.64, 82.61, 85.06, 84.4, 77.96, 69.32, 86.92, 81.29]\n",
      "Avarage Error: 98.84%, Adjusted Avarage Error: 88.63%\n",
      "Mean Square: 28.286\n",
      "\n",
      "[85.33, 84.37, 88.2, 88.05, 86.63, 86.63, 168.5, 94.66, 88.65, 212.04, 79.74, 148.94, 95.61, 82.14, 85.11, 84.33, 77.83, 68.35, 86.95, 81.14]\n",
      "Avarage Error: 98.66%, Adjusted Avarage Error: 88.48%\n",
      "Mean Square: 28.6915\n",
      "\n",
      "[85.24, 84.25, 88.17, 88.0, 86.56, 86.54, 167.31, 94.64, 88.57, 212.64, 79.61, 148.67, 95.58, 81.57, 85.16, 84.25, 77.7, 67.22, 86.97, 80.99]\n",
      "Avarage Error: 98.48%, Adjusted Avarage Error: 88.32%\n",
      "Mean Square: 29.0967\n",
      "\n",
      "[85.14, 84.12, 88.14, 87.95, 86.49, 86.46, 166.12, 94.63, 88.48, 213.55, 79.48, 148.96, 95.55, 80.89, 85.19, 84.16, 77.57, 65.86, 86.97, 80.84]\n",
      "Avarage Error: 98.33%, Adjusted Avarage Error: 88.16%\n",
      "Mean Square: 29.4898\n",
      "\n",
      "[85.05, 84.0, 88.1, 87.91, 86.43, 86.37, 164.84, 94.61, 88.4, 214.32, 79.35, 149.33, 95.52, 80.03, 85.16, 84.05, 77.42, 64.4, 86.95, 80.68]\n",
      "Avarage Error: 98.15%, Adjusted Avarage Error: 87.99%\n",
      "Mean Square: 29.8703\n",
      "\n",
      "[99.8, 100.43, 98.95, 99.53, 99.74, 100.2, 97.51, 99.13, 99.33, 99.39, 99.41, 99.44, 99.53, 99.69, 99.92, 100.17, 100.4, 100.63, 100.79, 100.88]\n",
      "Avarage Error: 99.74%, Adjusted Avarage Error: 99.62%\n",
      "Mean Square: 1.4491\n",
      "\n",
      "[99.76, 101.14, 99.26, 99.48, 99.69, 100.13, 101.67, 96.79, 98.44, 98.69, 98.8, 98.99, 99.28, 99.59, 99.92, 100.23, 100.52, 100.75, 100.9, 100.95]\n",
      "Avarage Error: 99.75%, Adjusted Avarage Error: 99.56%\n",
      "Mean Square: 1.2055\n",
      "\n",
      "[99.72, 104.0, 99.23, 99.36, 99.6, 100.09, 100.78, 110.68, 96.11, 97.28, 97.79, 98.45, 99.05, 99.51, 99.94, 100.31, 100.62, 100.86, 100.99, 101.0]\n",
      "Avarage Error: 100.27%, Adjusted Avarage Error: 99.48%\n",
      "Mean Square: 1.0785\n",
      "\n",
      "[99.69, 94.49, 99.14, 99.13, 99.48, 100.06, 101.11, 177.61, 94.16, 94.74, 96.64, 97.96, 98.84, 99.43, 99.96, 100.44, 100.81, 101.05, 101.17, 101.09]\n",
      "Avarage Error: 102.85%, Adjusted Avarage Error: 98.79%\n",
      "Mean Square: 0.9321\n",
      "\n",
      "[99.68, 97.87, 99.01, 98.93, 99.32, 99.99, 101.91, 89.61, 88.46, 89.97, 95.28, 97.28, 98.57, 99.35, 100.01, 100.6, 101.1, 101.34, 101.38, 101.16]\n",
      "Avarage Error: 98.04%, Adjusted Avarage Error: 97.64%\n",
      "Mean Square: 0.8071\n",
      "\n",
      "[99.68, 98.47, 98.86, 98.73, 99.06, 99.87, 103.99, 81.32, 60.05, 74.93, 92.46, 96.61, 98.34, 99.3, 100.07, 100.84, 101.44, 101.74, 101.61, 101.18]\n",
      "Avarage Error: 95.43%, Adjusted Avarage Error: 94.6%\n",
      "Mean Square: 0.7007\n",
      "\n",
      "[99.7, 98.72, 98.61, 98.51, 98.63, 99.67, 109.52, 205.09, 155.08, -0.47, 89.34, 95.82, 98.18, 99.28, 100.16, 101.13, 101.97, 102.21, 101.92, 101.17]\n",
      "Avarage Error: 102.76%, Adjusted Avarage Error: 94.17%\n",
      "Mean Square: 0.6045\n",
      "\n",
      "[99.73, 98.78, 98.35, 98.2, 98.14, 99.35, 113.95, 118.48, 129.97, -859.57, 87.95, 95.77, 98.06, 99.28, 100.26, 101.43, 102.59, 102.86, 102.31, 101.13]\n",
      "Avarage Error: 140.31%, Adjusted Avarage Error: 100.92%\n",
      "Mean Square: 0.519\n",
      "\n",
      "[99.79, 98.83, 98.11, 97.83, 97.54, 98.83, 106.75, 113.76, 129.79, -800.62, 90.59, 95.95, 98.05, 99.3, 100.39, 101.76, 103.33, 103.73, 102.73, 100.95]\n",
      "Avarage Error: 136.93%, Adjusted Avarage Error: 100.46%\n",
      "Mean Square: 0.4503\n",
      "\n",
      "[99.88, 98.85, 97.87, 97.37, 96.7, 97.96, 104.48, 112.23, 126.51, 32.91, 92.51, 96.34, 98.09, 99.32, 100.51, 102.04, 103.89, 104.61, 103.01, 100.54]\n",
      "Avarage Error: 98.28%, Adjusted Avarage Error: 95.94%\n",
      "Mean Square: 0.4004\n",
      "\n",
      "[100.01, 98.83, 97.6, 96.81, 95.68, 97.09, 103.52, 112.97, 128.23, 78.29, 93.88, 96.64, 98.11, 99.36, 100.64, 102.22, 104.32, 105.26, 103.15, 99.94]\n",
      "Avarage Error: 100.63%, Adjusted Avarage Error: 98.41%\n",
      "Mean Square: 0.3628\n",
      "\n",
      "[100.21, 98.82, 97.31, 96.12, 94.55, 96.4, 103.3, 112.76, 167.71, 88.54, 94.62, 96.7, 98.08, 99.39, 100.8, 102.53, 104.81, 105.71, 103.15, 99.26]\n",
      "Avarage Error: 103.04%, Adjusted Avarage Error: 98.91%\n",
      "Mean Square: 0.3276\n",
      "\n",
      "[100.49, 98.79, 97.02, 95.2, 93.52, 95.86, 103.32, 112.89, 53.7, 91.6, 95.07, 96.67, 98.03, 99.42, 100.99, 102.83, 104.88, 106.02, 103.08, 98.52]\n",
      "Avarage Error: 97.39%, Adjusted Avarage Error: 96.05%\n",
      "Mean Square: 0.2979\n",
      "\n",
      "[100.9, 98.79, 96.67, 94.19, 92.46, 95.14, 102.86, 117.35, 80.53, 92.06, 95.06, 96.55, 97.94, 99.45, 101.22, 103.22, 105.25, 106.27, 103.07, 97.63]\n",
      "Avarage Error: 98.83%, Adjusted Avarage Error: 97.39%\n",
      "Mean Square: 0.2674\n",
      "\n",
      "[101.52, 98.8, 96.29, 93.29, 91.69, 94.5, 102.3, 125.01, 85.86, 92.57, 94.84, 96.35, 97.81, 99.47, 101.55, 103.83, 105.94, 106.67, 102.88, 96.64]\n",
      "Avarage Error: 99.39%, Adjusted Avarage Error: 97.56%\n",
      "Mean Square: 0.2362\n",
      "\n",
      "[102.52, 98.81, 95.92, 92.45, 90.96, 93.99, 101.82, 145.75, 86.6, 92.8, 94.64, 96.11, 97.63, 99.49, 101.95, 104.57, 106.73, 107.28, 102.76, 95.42]\n",
      "Avarage Error: 100.41%, Adjusted Avarage Error: 97.51%\n",
      "Mean Square: 0.2074\n",
      "\n",
      "[104.21, 98.84, 95.55, 91.53, 90.14, 93.58, 101.5, 997.75, 87.26, 92.99, 94.42, 95.87, 97.45, 99.52, 102.45, 105.46, 107.95, 108.06, 102.73, 93.9]\n",
      "Avarage Error: 143.06%, Adjusted Avarage Error: 97.52%\n",
      "Mean Square: 0.1837\n",
      "\n",
      "[107.32, 98.92, 95.17, 90.63, 89.33, 93.67, 101.11, 2.85, 88.72, 92.85, 94.22, 95.62, 97.25, 99.58, 103.04, 106.5, 109.14, 108.98, 102.54, 92.02]\n",
      "Avarage Error: 93.47%, Adjusted Avarage Error: 91.74%\n",
      "Mean Square: 0.1674\n",
      "\n",
      "[114.1, 99.01, 94.81, 89.86, 88.65, 93.72, 100.8, -13.53, 89.96, 92.58, 93.96, 95.32, 97.02, 99.64, 103.67, 107.8, 110.36, 109.99, 102.24, 89.85]\n",
      "Avarage Error: 94.34%, Adjusted Avarage Error: 92.36%\n",
      "Mean Square: 0.1544\n",
      "\n",
      "[138.59, 99.12, 94.41, 89.18, 88.27, 93.79, 100.56, -403.02, 90.5, 92.32, 93.67, 95.03, 96.73, 99.73, 104.67, 110.33, 112.18, 110.28, 101.7, 88.56]\n",
      "Avarage Error: 115.13%, Adjusted Avarage Error: 97.84%\n",
      "Mean Square: 0.1437\n",
      "\n",
      "[-193.98, 99.27, 93.98, 88.67, 88.4, 93.78, 100.33, 29.87, 90.93, 92.16, 93.38, 94.72, 96.48, 99.83, 106.19, 114.4, 115.31, 110.38, 101.12, 87.28]\n",
      "Avarage Error: 99.52%, Adjusted Avarage Error: 93.4%\n",
      "Mean Square: 0.1364\n",
      "\n",
      "[50.12, 99.48, 93.57, 88.33, 88.58, 93.65, 100.06, 68.93, 91.04, 92.03, 93.14, 94.47, 96.2, 99.96, 108.57, 124.62, 121.68, 110.55, 100.63, 87.07]\n",
      "Avarage Error: 95.13%, Adjusted Avarage Error: 92.02%\n",
      "Mean Square: 0.1323\n",
      "\n",
      "[65.35, 99.78, 93.25, 88.12, 88.6, 93.58, 99.8, 79.36, 91.04, 91.92, 92.93, 94.18, 95.9, 100.14, 112.35, 172.53, 134.37, 111.48, 100.23, 85.88]\n",
      "Avarage Error: 99.54%, Adjusted Avarage Error: 93.55%\n",
      "Mean Square: 0.1293\n",
      "\n",
      "[72.02, 100.17, 92.96, 88.13, 88.47, 93.45, 99.53, 82.9, 91.0, 91.84, 92.71, 93.87, 95.56, 100.39, 120.22, -99.43, 164.9, 113.02, 99.84, 83.71]\n",
      "Avarage Error: 98.21%, Adjusted Avarage Error: 93.28%\n",
      "Mean Square: 0.1277\n",
      "\n",
      "[75.33, 100.68, 92.59, 88.1, 88.37, 93.25, 99.27, 84.93, 90.83, 91.66, 92.47, 93.51, 95.12, 100.74, 140.83, 54.75, 685.19, 117.48, 99.4, 80.82]\n",
      "Avarage Error: 123.27%, Adjusted Avarage Error: 91.07%\n",
      "Mean Square: 0.1256\n",
      "\n",
      "[78.75, 101.33, 92.26, 88.11, 88.62, 93.02, 99.04, 85.65, 90.66, 91.47, 92.22, 93.11, 94.56, 101.21, 260.0, 69.92, -25.17, 125.84, 98.99, 80.13]\n",
      "Avarage Error: 97.5%, Adjusted Avarage Error: 86.9%\n",
      "Mean Square: 0.1276\n",
      "\n",
      "[81.45, 102.2, 91.99, 88.16, 88.78, 92.72, 98.79, 86.62, 90.5, 91.28, 91.96, 92.68, 93.98, 101.96, -8.47, 76.31, 35.2, 142.81, 98.55, 81.36]\n",
      "Avarage Error: 86.79%, Adjusted Avarage Error: 82.82%\n",
      "Mean Square: 0.1329\n",
      "\n",
      "[83.3, 103.35, 91.77, 88.16, 88.98, 92.29, 98.49, 87.25, 90.34, 91.09, 91.7, 92.28, 93.47, 102.98, 54.29, 80.73, 58.82, 179.9, 98.05, 84.27]\n",
      "Avarage Error: 92.58%, Adjusted Avarage Error: 87.12%\n",
      "Mean Square: 0.1431\n",
      "\n",
      "[84.77, 105.16, 91.59, 88.22, 89.08, 91.94, 98.12, 87.82, 90.19, 90.91, 91.49, 91.94, 93.06, 104.45, 70.19, 83.53, 68.23, 257.76, 97.57, 84.4]\n",
      "Avarage Error: 98.02%, Adjusted Avarage Error: 88.75%\n",
      "Mean Square: 0.155\n",
      "\n",
      "[85.97, 108.8, 91.43, 88.43, 89.11, 91.56, 97.67, 88.64, 90.05, 90.73, 91.31, 91.61, 92.6, 107.34, 78.32, 85.37, 73.46, 257.52, 97.01, 83.38]\n",
      "Avarage Error: 99.01%, Adjusted Avarage Error: 89.67%\n",
      "Mean Square: 0.1679\n",
      "\n",
      "[86.94, 118.54, 91.26, 88.69, 89.17, 91.13, 97.12, 89.4, 89.92, 90.57, 91.14, 91.28, 92.18, 114.82, 82.31, 86.5, 76.94, 196.47, 96.49, 83.96]\n",
      "Avarage Error: 97.24%, Adjusted Avarage Error: 90.54%\n",
      "Mean Square: 0.1871\n",
      "\n",
      "[87.66, 164.73, 91.12, 89.0, 89.18, 90.62, 96.44, 89.95, 89.83, 90.44, 90.98, 90.9, 91.62, 189.54, 84.59, 87.01, 79.16, 200.3, 95.98, 84.76]\n",
      "Avarage Error: 103.69%, Adjusted Avarage Error: 93.55%\n",
      "Mean Square: 0.2092\n",
      "\n",
      "[88.08, -10.24, 91.01, 89.22, 89.22, 90.03, 95.58, 90.56, 89.76, 90.33, 90.84, 90.51, 90.96, 65.09, 86.31, 87.39, 80.86, -411.37, 95.67, 85.17]\n",
      "Avarage Error: 100.41%, Adjusted Avarage Error: 83.4%\n",
      "Mean Square: 0.2331\n",
      "\n",
      "[88.4, 57.11, 90.95, 89.36, 89.29, 89.45, 94.47, 90.64, 89.74, 90.25, 90.7, 90.22, 90.57, 81.65, 87.36, 87.53, 82.43, 37.96, 95.37, 85.61]\n",
      "Avarage Error: 84.95%, Adjusted Avarage Error: 83.85%\n",
      "Mean Square: 0.2609\n",
      "\n",
      "[88.7, 71.11, 90.89, 89.48, 89.35, 88.88, 92.95, 90.72, 89.75, 90.21, 90.6, 90.0, 90.42, 85.18, 88.25, 87.68, 83.88, 63.96, 95.21, 85.98]\n",
      "Avarage Error: 87.16%, Adjusted Avarage Error: 86.39%\n",
      "Mean Square: 0.2941\n",
      "\n",
      "[88.94, 77.75, 90.84, 89.57, 89.37, 88.79, 91.64, 90.74, 89.79, 90.17, 90.5, 89.86, 90.33, 86.23, 88.94, 87.86, 85.28, 74.4, 94.89, 86.07]\n",
      "Avarage Error: 88.1%, Adjusted Avarage Error: 87.52%\n",
      "Mean Square: 0.3311\n",
      "\n",
      "[89.17, 81.81, 90.75, 89.64, 89.43, 88.86, 90.96, 90.74, 89.83, 90.15, 90.41, 89.76, 90.2, 87.36, 89.47, 87.99, 86.3, 79.59, 94.47, 85.96]\n",
      "Avarage Error: 88.64%, Adjusted Avarage Error: 88.19%\n",
      "Mean Square: 0.3709\n",
      "\n",
      "[89.31, 84.15, 90.63, 89.7, 89.5, 88.93, 90.45, 90.74, 89.86, 90.11, 90.33, 89.75, 90.04, 88.29, 89.64, 88.08, 87.1, 82.82, 93.71, 85.95]\n",
      "Avarage Error: 88.95%, Adjusted Avarage Error: 88.59%\n",
      "Mean Square: 0.4126\n",
      "\n",
      "[89.41, 85.69, 90.51, 89.74, 89.57, 89.08, 89.83, 90.75, 89.87, 90.08, 90.26, 89.75, 89.69, 88.42, 89.63, 88.09, 87.56, 85.61, 92.64, 86.69]\n",
      "Avarage Error: 89.14%, Adjusted Avarage Error: 88.86%\n",
      "Mean Square: 0.4632\n",
      "\n",
      "[89.5, 86.87, 90.41, 89.76, 89.62, 89.19, 89.18, 90.73, 89.91, 90.07, 90.19, 89.78, 89.27, 88.09, 89.64, 88.23, 88.08, 87.57, 92.16, 87.53]\n",
      "Avarage Error: 89.29%, Adjusted Avarage Error: 89.05%\n",
      "Mean Square: 0.5246\n",
      "\n",
      "[89.59, 87.71, 90.32, 89.78, 89.66, 89.22, 88.77, 90.66, 89.92, 90.04, 90.1, 89.81, 89.38, 88.01, 89.65, 88.79, 88.83, 88.83, 91.57, 88.56]\n",
      "Avarage Error: 89.46%, Adjusted Avarage Error: 89.28%\n",
      "Mean Square: 0.6037\n",
      "\n",
      "[89.69, 88.24, 90.23, 89.81, 89.7, 89.27, 88.77, 90.59, 89.93, 90.0, 90.03, 89.87, 89.64, 88.16, 89.6, 89.09, 89.22, 89.45, 90.95, 88.99]\n",
      "Avarage Error: 89.56%, Adjusted Avarage Error: 89.43%\n",
      "Mean Square: 0.6783\n",
      "\n",
      "[89.76, 88.62, 90.18, 89.84, 89.74, 89.35, 88.93, 90.54, 89.92, 89.96, 89.98, 89.89, 89.71, 88.66, 89.56, 89.25, 89.42, 89.61, 90.8, 89.19]\n",
      "Avarage Error: 89.65%, Adjusted Avarage Error: 89.53%\n",
      "Mean Square: 0.7516\n",
      "\n",
      "[89.79, 88.93, 90.14, 89.9, 89.78, 89.46, 89.15, 90.51, 89.92, 89.95, 89.98, 89.91, 89.85, 88.91, 89.57, 89.44, 89.6, 89.73, 90.62, 89.4]\n",
      "Avarage Error: 89.73%, Adjusted Avarage Error: 89.63%\n",
      "Mean Square: 0.8321\n",
      "\n",
      "[89.83, 89.18, 90.13, 89.96, 89.82, 89.58, 89.29, 90.45, 89.92, 89.95, 90.0, 89.95, 89.93, 89.09, 89.64, 89.63, 89.8, 89.85, 90.58, 89.61]\n",
      "Avarage Error: 89.81%, Adjusted Avarage Error: 89.73%\n",
      "Mean Square: 0.922\n",
      "\n",
      "[89.86, 89.36, 90.1, 89.99, 89.85, 89.67, 89.33, 90.31, 89.9, 89.93, 90.02, 89.98, 89.97, 89.24, 89.72, 89.75, 89.97, 89.97, 90.52, 89.78]\n",
      "Avarage Error: 89.86%, Adjusted Avarage Error: 89.8%\n",
      "Mean Square: 1.0157\n",
      "\n",
      "[89.87, 89.46, 90.09, 90.01, 89.89, 89.77, 89.34, 90.13, 89.91, 89.92, 90.06, 90.0, 90.02, 89.43, 89.82, 89.81, 90.07, 90.02, 90.34, 89.9]\n",
      "Avarage Error: 89.89%, Adjusted Avarage Error: 89.85%\n",
      "Mean Square: 1.1119\n",
      "\n",
      "[89.9, 89.53, 90.09, 90.01, 89.91, 89.85, 89.38, 90.02, 89.94, 89.93, 90.07, 90.0, 90.04, 89.79, 89.91, 89.84, 90.07, 89.99, 90.21, 89.95]\n",
      "Avarage Error: 89.92%, Adjusted Avarage Error: 89.9%\n",
      "Mean Square: 1.2101\n",
      "\n",
      "[89.94, 89.61, 90.1, 90.01, 89.92, 89.93, 89.44, 89.96, 89.95, 89.96, 90.07, 89.99, 90.07, 90.21, 89.98, 89.87, 90.06, 89.99, 90.19, 90.0]\n",
      "Avarage Error: 89.96%, Adjusted Avarage Error: 89.94%\n",
      "Mean Square: 1.3154\n",
      "\n",
      "[89.99, 89.68, 90.11, 90.01, 89.93, 89.97, 89.45, 89.9, 89.94, 89.97, 90.06, 89.98, 90.04, 90.22, 90.02, 89.91, 90.05, 90.03, 90.1, 90.03]\n",
      "Avarage Error: 89.97%, Adjusted Avarage Error: 89.95%\n",
      "Mean Square: 1.4249\n",
      "\n",
      "[90.02, 89.74, 90.11, 90.0, 89.94, 89.95, 89.48, 89.81, 89.91, 89.98, 90.08, 89.99, 90.1, 89.97, 90.02, 89.94, 90.06, 90.08, 90.06, 90.06]\n",
      "Avarage Error: 89.97%, Adjusted Avarage Error: 89.95%\n",
      "Mean Square: 1.5402\n",
      "\n",
      "[90.02, 89.77, 90.11, 89.99, 89.96, 89.9, 89.52, 89.82, 89.93, 89.98, 90.1, 90.01, 90.1, 89.91, 90.0, 89.97, 90.07, 90.12, 90.05, 90.08]\n",
      "Avarage Error: 89.97%, Adjusted Avarage Error: 89.95%\n",
      "Mean Square: 1.6601\n",
      "\n",
      "[90.01, 89.8, 90.1, 89.97, 89.96, 89.88, 89.6, 89.85, 89.93, 89.99, 90.09, 90.0, 90.07, 89.87, 89.98, 89.97, 90.06, 90.14, 90.01, 90.08]\n",
      "Avarage Error: 89.97%, Adjusted Avarage Error: 89.95%\n",
      "Mean Square: 1.782\n",
      "\n",
      "[90.0, 89.81, 90.09, 89.96, 89.97, 89.9, 89.65, 89.87, 89.94, 89.97, 90.09, 90.01, 90.04, 89.94, 89.95, 89.97, 90.06, 90.15, 89.99, 90.07]\n",
      "Avarage Error: 89.97%, Adjusted Avarage Error: 89.96%\n",
      "Mean Square: 1.91\n",
      "\n",
      "[90.0, 89.81, 90.09, 89.95, 89.98, 89.91, 89.73, 89.89, 89.96, 89.97, 90.08, 90.01, 90.0, 90.09, 89.95, 89.96, 90.06, 90.15, 89.96, 90.06]\n",
      "Avarage Error: 89.98%, Adjusted Avarage Error: 89.96%\n",
      "Mean Square: 2.0451\n",
      "\n",
      "[90.0, 89.79, 90.08, 89.95, 89.98, 89.94, 89.84, 89.92, 89.98, 89.98, 90.06, 90.0, 89.98, 90.1, 89.96, 89.96, 90.05, 90.14, 89.92, 90.05]\n",
      "Avarage Error: 89.98%, Adjusted Avarage Error: 89.97%\n",
      "Mean Square: 2.1883\n",
      "\n",
      "[90.0, 89.76, 90.07, 89.94, 89.97, 89.97, 89.94, 89.95, 90.02, 89.98, 90.04, 89.99, 89.96, 90.02, 89.96, 89.96, 90.04, 90.14, 89.88, 90.04]\n",
      "Avarage Error: 89.98%, Adjusted Avarage Error: 89.97%\n",
      "Mean Square: 2.3377\n",
      "\n",
      "[90.0, 89.74, 90.07, 89.94, 89.97, 90.01, 90.0, 89.98, 90.04, 89.97, 90.02, 89.98, 89.94, 89.93, 89.96, 89.97, 90.02, 90.12, 89.86, 90.02]\n",
      "Avarage Error: 89.98%, Adjusted Avarage Error: 89.96%\n",
      "Mean Square: 2.4946\n",
      "\n",
      "[90.0, 89.72, 90.06, 89.94, 89.97, 90.02, 90.0, 90.05, 90.07, 89.96, 90.01, 89.98, 89.95, 89.7, 89.97, 89.97, 90.01, 90.09, 89.84, 90.01]\n",
      "Avarage Error: 89.97%, Adjusted Avarage Error: 89.95%\n",
      "Mean Square: 2.6592\n",
      "\n",
      "[90.0, 89.71, 90.07, 89.95, 89.98, 90.02, 89.99, 90.08, 90.1, 89.94, 90.0, 89.97, 89.98, 89.59, 89.94, 89.97, 89.99, 90.07, 89.83, 89.99]\n",
      "Avarage Error: 89.96%, Adjusted Avarage Error: 89.94%\n",
      "Mean Square: 2.83\n",
      "\n",
      "[90.01, 89.71, 90.08, 89.96, 90.0, 90.02, 90.01, 90.1, 90.12, 89.94, 89.99, 89.96, 89.99, 89.43, 89.9, 89.95, 89.97, 90.05, 89.83, 89.98]\n",
      "Avarage Error: 89.95%, Adjusted Avarage Error: 89.93%\n",
      "Mean Square: 3.0106\n",
      "\n",
      "[90.02, 89.71, 90.09, 89.96, 90.02, 90.0, 90.02, 90.1, 90.12, 89.96, 90.0, 89.96, 90.05, 89.43, 89.88, 89.95, 89.97, 90.04, 89.82, 89.96]\n",
      "Avarage Error: 89.95%, Adjusted Avarage Error: 89.94%\n",
      "Mean Square: 3.2026\n",
      "\n",
      "[90.02, 89.72, 90.08, 89.97, 90.04, 89.99, 90.05, 90.1, 90.12, 89.99, 90.03, 89.99, 90.12, 89.5, 89.89, 89.96, 89.96, 90.03, 89.82, 89.96]\n",
      "Avarage Error: 89.97%, Adjusted Avarage Error: 89.95%\n",
      "Mean Square: 3.4083\n",
      "\n",
      "[90.02, 89.74, 90.06, 89.99, 90.05, 90.0, 90.09, 90.1, 90.11, 90.0, 90.04, 90.01, 90.14, 89.61, 89.89, 89.96, 89.95, 90.0, 89.84, 89.95]\n",
      "Avarage Error: 89.98%, Adjusted Avarage Error: 89.96%\n",
      "Mean Square: 3.62\n",
      "\n",
      "[90.03, 89.78, 90.04, 90.01, 90.06, 90.01, 90.16, 90.11, 90.1, 90.02, 90.05, 90.05, 90.16, 89.66, 89.88, 89.94, 89.95, 89.98, 89.86, 89.94]\n",
      "Avarage Error: 89.99%, Adjusted Avarage Error: 89.97%\n",
      "Mean Square: 3.8425\n",
      "\n",
      "[90.04, 89.8, 90.03, 90.01, 90.06, 90.03, 90.26, 90.12, 90.1, 90.05, 90.05, 90.08, 90.15, 89.75, 89.87, 89.93, 89.94, 89.97, 89.86, 89.94]\n",
      "Avarage Error: 90.0%, Adjusted Avarage Error: 89.98%\n",
      "Mean Square: 4.0754\n",
      "\n",
      "[90.05, 89.82, 90.02, 90.0, 90.05, 90.04, 90.27, 90.1, 90.09, 90.06, 90.04, 90.07, 90.14, 89.78, 89.87, 89.91, 89.92, 89.96, 89.87, 89.94]\n",
      "Avarage Error: 90.0%, Adjusted Avarage Error: 89.98%\n",
      "Mean Square: 4.3146\n",
      "\n",
      "[90.05, 89.85, 90.03, 90.0, 90.03, 90.05, 90.26, 90.09, 90.08, 90.05, 90.05, 90.05, 90.12, 89.82, 89.87, 89.9, 89.9, 89.95, 89.86, 89.93]\n",
      "Avarage Error: 90.0%, Adjusted Avarage Error: 89.98%\n",
      "Mean Square: 4.5609\n",
      "\n",
      "[90.06, 89.87, 90.04, 89.99, 90.01, 90.06, 90.26, 90.07, 90.05, 90.05, 90.05, 90.05, 90.1, 89.84, 89.88, 89.89, 89.89, 89.93, 89.84, 89.92]\n",
      "Avarage Error: 89.99%, Adjusted Avarage Error: 89.97%\n",
      "Mean Square: 4.8173\n",
      "\n",
      "[90.05, 89.87, 90.05, 89.98, 90.0, 90.07, 90.27, 90.07, 90.03, 90.05, 90.05, 90.04, 90.09, 89.87, 89.91, 89.89, 89.86, 89.91, 89.81, 89.91]\n",
      "Avarage Error: 89.99%, Adjusted Avarage Error: 89.97%\n",
      "Mean Square: 5.081\n",
      "\n",
      "[90.05, 89.87, 90.05, 89.98, 90.0, 90.08, 90.28, 90.05, 90.02, 90.05, 90.05, 90.04, 90.06, 89.9, 89.92, 89.89, 89.83, 89.88, 89.77, 89.89]\n",
      "Avarage Error: 89.98%, Adjusted Avarage Error: 89.96%\n",
      "Mean Square: 5.355\n",
      "\n",
      "[90.03, 89.86, 90.06, 89.97, 90.0, 90.08, 90.27, 90.03, 90.01, 90.03, 90.06, 90.02, 90.02, 89.95, 89.93, 89.89, 89.8, 89.85, 89.72, 89.87]\n",
      "Avarage Error: 89.97%, Adjusted Avarage Error: 89.95%\n",
      "Mean Square: 5.6321\n",
      "\n",
      "[89.99, 89.82, 90.07, 89.97, 90.0, 90.07, 90.22, 90.0, 90.0, 90.02, 90.06, 90.01, 89.99, 90.01, 89.93, 89.89, 89.78, 89.82, 89.69, 89.84]\n",
      "Avarage Error: 89.96%, Adjusted Avarage Error: 89.94%\n",
      "Mean Square: 5.9104\n",
      "\n",
      "[89.95, 89.79, 90.07, 89.97, 90.01, 90.06, 90.18, 89.98, 90.0, 89.99, 90.06, 90.0, 89.96, 90.07, 89.94, 89.89, 89.75, 89.8, 89.65, 89.81]\n",
      "Avarage Error: 89.95%, Adjusted Avarage Error: 89.93%\n",
      "Mean Square: 6.1978\n",
      "\n",
      "[89.9, 89.75, 90.07, 89.96, 90.01, 90.03, 90.12, 89.97, 90.02, 89.97, 90.07, 89.99, 89.96, 90.09, 89.95, 89.9, 89.72, 89.78, 89.61, 89.78]\n",
      "Avarage Error: 89.93%, Adjusted Avarage Error: 89.92%\n",
      "Mean Square: 6.4968\n",
      "\n",
      "[89.85, 89.71, 90.08, 89.96, 90.02, 90.0, 90.07, 89.98, 90.05, 89.95, 90.09, 90.0, 89.98, 90.1, 89.95, 89.91, 89.71, 89.79, 89.59, 89.77]\n",
      "Avarage Error: 89.93%, Adjusted Avarage Error: 89.91%\n",
      "Mean Square: 6.8129\n",
      "\n",
      "[89.8, 89.66, 90.09, 89.98, 90.03, 89.99, 90.04, 90.02, 90.06, 89.95, 90.09, 90.02, 90.01, 90.09, 89.95, 89.91, 89.72, 89.82, 89.56, 89.77]\n",
      "Avarage Error: 89.93%, Adjusted Avarage Error: 89.91%\n",
      "Mean Square: 7.1496\n",
      "\n",
      "[89.76, 89.61, 90.08, 89.99, 90.03, 89.98, 90.04, 90.05, 90.06, 89.95, 90.09, 90.02, 90.03, 90.07, 89.92, 89.91, 89.73, 89.87, 89.53, 89.76]\n",
      "Avarage Error: 89.92%, Adjusted Avarage Error: 89.91%\n",
      "Mean Square: 7.4949\n",
      "\n",
      "[89.71, 89.57, 90.08, 89.99, 90.03, 89.97, 90.02, 90.04, 90.05, 89.91, 90.11, 90.02, 90.05, 90.06, 89.91, 89.9, 89.75, 89.92, 89.49, 89.77]\n",
      "Avarage Error: 89.92%, Adjusted Avarage Error: 89.9%\n",
      "Mean Square: 7.8493\n",
      "\n",
      "[89.67, 89.53, 90.06, 89.99, 90.01, 89.96, 90.0, 90.03, 90.04, 89.87, 90.1, 90.03, 90.09, 90.05, 89.9, 89.88, 89.78, 89.94, 89.5, 89.77]\n",
      "Avarage Error: 89.91%, Adjusted Avarage Error: 89.89%\n",
      "Mean Square: 8.2179\n",
      "\n",
      "[89.63, 89.5, 90.03, 89.99, 89.99, 89.95, 89.98, 90.02, 90.04, 89.81, 90.08, 90.05, 90.08, 90.06, 89.89, 89.84, 89.71, 89.86, 89.44, 89.7]\n",
      "Avarage Error: 89.88%, Adjusted Avarage Error: 89.86%\n",
      "Mean Square: 8.5382\n",
      "\n",
      "[89.58, 89.46, 89.99, 89.99, 89.98, 89.93, 89.97, 90.01, 90.04, 89.76, 90.03, 90.06, 90.07, 90.08, 89.86, 89.79, 89.6, 89.74, 89.35, 89.59]\n",
      "Avarage Error: 89.84%, Adjusted Avarage Error: 89.82%\n",
      "Mean Square: 8.841\n",
      "\n",
      "[89.52, 89.43, 89.96, 89.98, 89.96, 89.92, 89.97, 90.0, 90.04, 89.73, 89.98, 90.06, 90.06, 90.08, 89.83, 89.72, 89.49, 89.63, 89.25, 89.48]\n",
      "Avarage Error: 89.8%, Adjusted Avarage Error: 89.77%\n",
      "Mean Square: 9.1485\n",
      "\n",
      "[89.47, 89.39, 89.92, 89.97, 89.94, 89.91, 89.91, 89.99, 90.02, 89.74, 89.93, 90.06, 90.03, 90.08, 89.79, 89.66, 89.37, 89.51, 89.15, 89.37]\n",
      "Avarage Error: 89.76%, Adjusted Avarage Error: 89.73%\n",
      "Mean Square: 9.4619\n",
      "\n",
      "[89.42, 89.35, 89.88, 89.96, 89.92, 89.9, 89.84, 89.98, 90.01, 89.73, 89.87, 90.05, 90.0, 90.07, 89.74, 89.59, 89.25, 89.38, 89.05, 89.26]\n",
      "Avarage Error: 89.71%, Adjusted Avarage Error: 89.67%\n",
      "Mean Square: 9.7746\n",
      "\n",
      "[89.37, 89.31, 89.84, 89.95, 89.9, 89.89, 89.79, 89.97, 90.0, 89.7, 89.8, 90.02, 89.96, 90.06, 89.69, 89.53, 89.13, 89.25, 88.95, 89.14]\n",
      "Avarage Error: 89.66%, Adjusted Avarage Error: 89.62%\n",
      "Mean Square: 10.0893\n",
      "\n",
      "[89.31, 89.28, 89.8, 89.93, 89.87, 89.87, 89.75, 90.03, 90.01, 89.61, 89.72, 89.99, 89.91, 90.04, 89.63, 89.46, 89.0, 89.12, 88.85, 89.02]\n",
      "Avarage Error: 89.61%, Adjusted Avarage Error: 89.56%\n",
      "Mean Square: 10.3956\n",
      "\n",
      "[89.26, 89.24, 89.75, 89.9, 89.84, 89.85, 89.71, 90.08, 90.03, 89.5, 89.58, 89.93, 89.87, 90.02, 89.54, 89.39, 88.87, 88.99, 88.74, 88.9]\n",
      "Avarage Error: 89.55%, Adjusted Avarage Error: 89.49%\n",
      "Mean Square: 10.6942\n",
      "\n",
      "[89.21, 89.19, 89.7, 89.88, 89.81, 89.84, 89.67, 90.15, 90.06, 89.31, 89.39, 89.78, 89.82, 89.99, 89.44, 89.32, 88.75, 88.85, 88.64, 88.78]\n",
      "Avarage Error: 89.48%, Adjusted Avarage Error: 89.41%\n",
      "Mean Square: 10.9741\n",
      "\n",
      "[89.16, 89.15, 89.65, 89.86, 89.78, 89.81, 89.62, 90.22, 90.09, 88.99, 89.01, 89.64, 89.77, 89.96, 89.34, 89.24, 88.62, 88.71, 88.53, 88.65]\n",
      "Avarage Error: 89.39%, Adjusted Avarage Error: 89.31%\n",
      "Mean Square: 11.2188\n",
      "\n",
      "[89.11, 89.11, 89.6, 89.84, 89.75, 89.78, 89.59, 90.27, 90.14, 88.43, 88.13, 89.64, 89.71, 89.94, 89.24, 89.15, 88.48, 88.57, 88.4, 88.51]\n",
      "Avarage Error: 89.27%, Adjusted Avarage Error: 89.17%\n",
      "Mean Square: 11.3973\n",
      "\n",
      "[89.05, 89.05, 89.55, 89.81, 89.71, 89.75, 89.55, 90.32, 90.22, 88.08, 86.63, 89.79, 89.64, 89.92, 89.14, 89.06, 88.33, 88.42, 88.28, 88.37]\n",
      "Avarage Error: 89.13%, Adjusted Avarage Error: 89.01%\n",
      "Mean Square: 11.5792\n",
      "\n",
      "[88.98, 88.99, 89.5, 89.79, 89.67, 89.72, 89.46, 90.37, 90.43, 87.91, 84.94, 90.21, 89.57, 89.89, 89.01, 88.97, 88.19, 88.27, 88.16, 88.23]\n",
      "Avarage Error: 89.01%, Adjusted Avarage Error: 88.86%\n",
      "Mean Square: 11.8447\n",
      "\n",
      "[88.91, 88.93, 89.44, 89.77, 89.63, 89.68, 89.35, 90.43, 90.66, 87.34, 84.84, 90.49, 89.49, 89.86, 88.88, 88.87, 88.04, 88.12, 88.05, 88.09]\n",
      "Avarage Error: 88.94%, Adjusted Avarage Error: 88.76%\n",
      "Mean Square: 12.1639\n",
      "\n",
      "[88.84, 88.87, 89.38, 89.74, 89.59, 89.65, 89.22, 90.48, 90.92, 86.62, 85.6, 90.64, 89.4, 89.84, 88.75, 88.77, 87.89, 87.96, 87.94, 87.95]\n",
      "Avarage Error: 88.9%, Adjusted Avarage Error: 88.69%\n",
      "Mean Square: 12.5213\n",
      "\n",
      "[88.77, 88.81, 89.32, 89.72, 89.56, 89.61, 89.09, 90.52, 91.11, 85.58, 86.3, 90.69, 89.32, 89.82, 88.63, 88.67, 87.74, 87.8, 87.84, 87.81]\n",
      "Avarage Error: 88.83%, Adjusted Avarage Error: 88.61%\n",
      "Mean Square: 12.8544\n",
      "\n",
      "[88.69, 88.74, 89.27, 89.69, 89.51, 89.58, 88.95, 90.56, 91.06, 84.66, 87.26, 90.75, 89.23, 89.8, 88.53, 88.56, 87.58, 87.62, 87.73, 87.66]\n",
      "Avarage Error: 88.77%, Adjusted Avarage Error: 88.53%\n",
      "Mean Square: 13.2087\n",
      "\n",
      "[88.61, 88.67, 89.21, 89.67, 89.47, 89.54, 88.8, 90.58, 91.06, 83.98, 88.26, 90.86, 89.15, 89.78, 88.43, 88.46, 87.42, 87.43, 87.63, 87.52]\n",
      "Avarage Error: 88.73%, Adjusted Avarage Error: 88.48%\n",
      "Mean Square: 13.6386\n",
      "\n",
      "[88.53, 88.59, 89.16, 89.65, 89.42, 89.49, 88.65, 90.61, 91.06, 83.82, 89.19, 90.97, 89.06, 89.77, 88.33, 88.35, 87.25, 87.22, 87.53, 87.38]\n",
      "Avarage Error: 88.7%, Adjusted Avarage Error: 88.44%\n",
      "Mean Square: 14.132\n",
      "\n",
      "[88.45, 88.51, 89.1, 89.62, 89.38, 89.45, 88.52, 90.63, 91.07, 84.51, 89.68, 91.07, 88.95, 89.74, 88.22, 88.25, 87.08, 87.0, 87.43, 87.23]\n",
      "Avarage Error: 88.7%, Adjusted Avarage Error: 88.43%\n",
      "Mean Square: 14.6333\n",
      "\n",
      "[88.37, 88.43, 89.05, 89.59, 89.33, 89.41, 88.37, 90.64, 91.08, 85.34, 90.34, 91.15, 88.84, 89.71, 88.11, 88.15, 86.9, 86.77, 87.34, 87.08]\n",
      "Avarage Error: 88.7%, Adjusted Avarage Error: 88.43%\n",
      "Mean Square: 15.2194\n",
      "\n",
      "[88.29, 88.34, 88.99, 89.56, 89.28, 89.37, 88.17, 90.66, 91.09, 86.07, 91.01, 91.22, 88.72, 89.68, 88.0, 88.04, 86.72, 86.53, 87.25, 86.94]\n",
      "Avarage Error: 88.7%, Adjusted Avarage Error: 88.42%\n",
      "Mean Square: 15.8671\n",
      "\n",
      "[88.21, 88.26, 88.92, 89.53, 89.23, 89.32, 87.93, 90.68, 91.1, 86.71, 91.33, 91.27, 88.57, 89.64, 87.91, 87.93, 86.52, 86.27, 87.16, 86.79]\n",
      "Avarage Error: 88.66%, Adjusted Avarage Error: 88.37%\n",
      "Mean Square: 16.4474\n",
      "\n",
      "[88.13, 88.17, 88.87, 89.49, 89.18, 89.27, 87.6, 90.71, 91.09, 87.23, 91.05, 91.3, 88.42, 89.6, 87.83, 87.83, 86.33, 86.01, 87.09, 86.64]\n",
      "Avarage Error: 88.59%, Adjusted Avarage Error: 88.3%\n",
      "Mean Square: 16.7988\n",
      "\n",
      "[88.05, 88.08, 88.82, 89.45, 89.13, 89.22, 87.09, 90.73, 91.07, 87.58, 90.16, 91.3, 88.25, 89.55, 87.73, 87.72, 86.13, 85.75, 87.03, 86.49]\n",
      "Avarage Error: 88.47%, Adjusted Avarage Error: 88.16%\n",
      "Mean Square: 16.931\n",
      "\n",
      "[87.97, 87.98, 88.77, 89.42, 89.07, 89.17, 86.5, 90.74, 91.05, 86.77, 89.18, 91.19, 88.06, 89.5, 87.64, 87.62, 85.94, 85.48, 86.96, 86.34]\n",
      "Avarage Error: 88.27%, Adjusted Avarage Error: 87.95%\n",
      "Mean Square: 16.9271\n",
      "\n",
      "[87.88, 87.89, 88.72, 89.38, 89.01, 89.12, 85.8, 90.76, 91.03, 80.53, 88.46, 90.98, 87.87, 89.45, 87.58, 87.51, 85.73, 85.21, 86.89, 86.19]\n",
      "Avarage Error: 87.8%, Adjusted Avarage Error: 87.44%\n",
      "Mean Square: 16.6681\n",
      "\n",
      "[87.81, 87.8, 88.67, 89.34, 88.95, 89.06, 84.92, 90.78, 91.0, 32.32, 87.95, 90.66, 87.65, 89.4, 87.53, 87.4, 85.49, 84.92, 86.83, 86.04]\n",
      "Avarage Error: 85.23%, Adjusted Avarage Error: 84.6%\n",
      "Mean Square: 16.564\n",
      "\n",
      "[87.73, 87.71, 88.64, 89.3, 88.89, 89.0, 83.83, 90.81, 90.96, 144.68, 87.49, 90.18, 87.48, 89.35, 87.48, 87.29, 85.24, 84.63, 86.76, 85.89]\n",
      "Avarage Error: 90.67%, Adjusted Avarage Error: 87.65%\n",
      "Mean Square: 16.9142\n",
      "\n",
      "[87.65, 87.62, 88.61, 89.27, 88.83, 88.95, 82.24, 90.88, 90.92, 125.35, 87.01, 88.81, 85.82, 89.29, 87.44, 87.17, 84.99, 84.32, 86.7, 85.74]\n",
      "Avarage Error: 89.38%, Adjusted Avarage Error: 87.3%\n",
      "Mean Square: 17.2091\n",
      "\n",
      "[87.58, 87.52, 88.58, 89.23, 88.77, 88.89, 80.09, 90.98, 90.87, 121.38, 86.55, 84.78, 84.93, 89.23, 87.4, 87.06, 84.74, 84.01, 86.64, 85.58]\n",
      "Avarage Error: 88.74%, Adjusted Avarage Error: 86.8%\n",
      "Mean Square: 17.2914\n",
      "\n",
      "[87.5, 87.43, 88.55, 89.19, 88.7, 88.83, 77.74, 91.1, 90.82, 119.91, 86.05, 72.8, 86.57, 89.17, 87.4, 86.94, 84.47, 83.68, 86.59, 85.42]\n",
      "Avarage Error: 87.94%, Adjusted Avarage Error: 85.99%\n",
      "Mean Square: 17.409\n",
      "\n",
      "[87.42, 87.33, 88.53, 89.16, 88.64, 88.78, 75.43, 91.3, 90.77, 120.16, 85.47, 47.89, 88.16, 89.1, 87.41, 86.82, 84.18, 83.31, 86.55, 85.26]\n",
      "Avarage Error: 86.58%, Adjusted Avarage Error: 84.46%\n",
      "Mean Square: 17.6293\n",
      "\n",
      "[87.34, 87.23, 88.5, 89.12, 88.58, 88.72, 72.78, 91.54, 90.72, 120.73, 84.9, -21.27, 90.6, 89.02, 87.39, 86.7, 83.89, 82.94, 86.51, 85.1]\n",
      "Avarage Error: 85.18%, Adjusted Avarage Error: 82.85%\n",
      "Mean Square: 17.9467\n",
      "\n",
      "[87.27, 87.13, 88.48, 89.08, 88.51, 88.66, 69.84, 91.83, 90.67, 121.93, 84.36, -382.29, 92.9, 88.93, 87.37, 86.59, 83.61, 82.55, 86.47, 84.94]\n",
      "Avarage Error: 103.17%, Adjusted Avarage Error: 86.62%\n",
      "Mean Square: 18.3489\n",
      "\n",
      "[87.19, 87.03, 88.45, 89.04, 88.45, 88.6, 66.61, 92.16, 90.61, 123.73, 83.84, 1114.37, 94.28, 88.82, 87.33, 86.47, 83.33, 82.16, 86.42, 84.78]\n",
      "Avarage Error: 139.68%, Adjusted Avarage Error: 86.42%\n",
      "Mean Square: 18.7974\n",
      "\n",
      "[87.11, 86.93, 88.42, 89.0, 88.38, 88.54, 63.02, 92.52, 90.55, 126.42, 83.56, 459.86, 95.14, 88.7, 87.26, 86.36, 83.05, 81.77, 86.36, 84.62]\n",
      "Avarage Error: 106.88%, Adjusted Avarage Error: 86.18%\n",
      "Mean Square: 19.2927\n",
      "\n",
      "[87.03, 86.82, 88.39, 88.95, 88.32, 88.48, 58.72, 92.9, 90.49, 130.18, 83.34, 338.91, 95.6, 88.56, 87.17, 86.25, 82.77, 81.38, 86.32, 84.45]\n",
      "Avarage Error: 100.75%, Adjusted Avarage Error: 85.89%\n",
      "Mean Square: 19.7562\n",
      "\n",
      "[86.95, 86.71, 88.37, 88.91, 88.25, 88.42, 53.42, 93.22, 90.43, 136.01, 83.14, 290.44, 95.86, 88.43, 87.08, 86.14, 82.5, 80.96, 86.28, 84.28]\n",
      "Avarage Error: 98.29%, Adjusted Avarage Error: 85.52%\n",
      "Mean Square: 20.1784\n",
      "\n",
      "[86.87, 86.6, 88.34, 88.86, 88.18, 88.36, 47.32, 93.48, 90.37, 144.62, 82.95, 263.01, 96.0, 88.28, 86.98, 86.03, 82.24, 80.54, 86.25, 84.11]\n",
      "Avarage Error: 96.97%, Adjusted Avarage Error: 85.1%\n",
      "Mean Square: 20.5595\n",
      "\n",
      "[86.8, 86.49, 88.33, 88.83, 88.12, 88.29, 38.18, 93.74, 90.3, 155.25, 82.77, 244.33, 96.07, 88.12, 86.89, 85.92, 81.99, 80.12, 86.26, 83.94]\n",
      "Avarage Error: 96.04%, Adjusted Avarage Error: 84.51%\n",
      "Mean Square: 20.9522\n",
      "\n",
      "[86.72, 86.38, 88.33, 88.8, 88.05, 88.22, 26.67, 93.92, 90.23, 168.13, 82.6, 231.28, 96.11, 87.94, 86.8, 85.81, 81.74, 79.67, 86.27, 83.77]\n",
      "Avarage Error: 95.37%, Adjusted Avarage Error: 83.78%\n",
      "Mean Square: 21.3335\n",
      "\n",
      "[86.64, 86.26, 88.33, 88.77, 87.98, 88.16, 12.82, 94.09, 90.16, 181.8, 82.44, 222.9, 96.11, 87.75, 86.7, 85.71, 81.49, 79.19, 86.29, 83.6]\n",
      "Avarage Error: 94.86%, Adjusted Avarage Error: 82.92%\n",
      "Mean Square: 21.7142\n",
      "\n",
      "[86.55, 86.14, 88.32, 88.73, 87.91, 88.09, -7.48, 94.24, 90.09, 192.14, 82.29, 214.72, 96.11, 87.55, 86.61, 85.62, 81.26, 78.68, 86.3, 83.42]\n",
      "Avarage Error: 94.61%, Adjusted Avarage Error: 82.52%\n",
      "Mean Square: 22.1084\n",
      "\n",
      "[86.47, 86.02, 88.32, 88.7, 87.85, 88.02, -41.29, 94.39, 90.02, 199.12, 82.13, 207.03, 96.09, 87.39, 86.52, 85.54, 81.05, 78.23, 86.3, 83.24]\n",
      "Avarage Error: 96.19%, Adjusted Avarage Error: 84.31%\n",
      "Mean Square: 22.5109\n",
      "\n",
      "[86.39, 85.9, 88.32, 88.66, 87.78, 87.95, -110.22, 94.56, 89.95, 203.14, 81.96, 198.73, 96.08, 87.21, 86.42, 85.46, 80.83, 77.76, 86.33, 83.06]\n",
      "Avarage Error: 99.34%, Adjusted Avarage Error: 88.05%\n",
      "Mean Square: 22.9313\n",
      "\n",
      "[86.31, 85.77, 88.31, 88.62, 87.71, 87.88, -261.23, 94.67, 89.87, 201.77, 81.78, 189.88, 96.06, 86.87, 86.29, 85.39, 80.62, 77.26, 86.38, 82.88]\n",
      "Avarage Error: 106.28%, Adjusted Avarage Error: 92.36%\n",
      "Mean Square: 23.3449\n",
      "\n",
      "[86.23, 85.65, 88.3, 88.59, 87.64, 87.81, -1056.09, 94.75, 89.8, 198.04, 81.61, 181.57, 96.03, 86.5, 86.1, 85.32, 80.4, 76.74, 86.45, 82.69]\n",
      "Avarage Error: 145.32%, Adjusted Avarage Error: 91.79%\n",
      "Mean Square: 23.7499\n",
      "\n",
      "[86.15, 85.52, 88.29, 88.55, 87.57, 87.74, 1157.16, 94.8, 89.72, 193.43, 81.44, 174.04, 96.01, 86.12, 85.83, 85.25, 80.18, 76.23, 86.54, 82.51]\n",
      "Avarage Error: 149.65%, Adjusted Avarage Error: 91.25%\n",
      "Mean Square: 24.1463\n",
      "\n",
      "[86.07, 85.39, 88.28, 88.51, 87.5, 87.66, 491.04, 94.84, 89.64, 188.75, 81.28, 166.77, 95.98, 85.63, 85.38, 85.19, 79.97, 75.73, 86.64, 82.33]\n",
      "Avarage Error: 115.63%, Adjusted Avarage Error: 90.71%\n",
      "Mean Square: 24.5241\n",
      "\n",
      "[85.99, 85.27, 88.27, 88.47, 87.43, 87.59, 366.35, 94.85, 89.57, 184.86, 81.11, 160.86, 95.96, 85.18, 84.94, 85.11, 79.78, 75.15, 86.61, 82.17]\n",
      "Avarage Error: 108.78%, Adjusted Avarage Error: 90.24%\n",
      "Mean Square: 24.9031\n",
      "\n",
      "[85.91, 85.14, 88.25, 88.43, 87.36, 87.52, 307.36, 94.85, 89.49, 181.32, 80.95, 155.6, 95.93, 84.66, 84.33, 85.02, 79.61, 74.54, 86.54, 82.01]\n",
      "Avarage Error: 105.24%, Adjusted Avarage Error: 89.78%\n",
      "Mean Square: 25.2658\n",
      "\n",
      "[85.83, 85.01, 88.24, 88.38, 87.28, 87.44, 271.31, 94.85, 89.41, 178.18, 80.79, 151.39, 95.9, 84.02, 83.96, 84.9, 79.44, 73.91, 86.48, 81.86]\n",
      "Avarage Error: 102.93%, Adjusted Avarage Error: 89.39%\n",
      "Mean Square: 25.6451\n",
      "\n",
      "[85.74, 84.88, 88.22, 88.34, 87.21, 87.36, 248.65, 94.84, 89.33, 175.43, 80.64, 148.14, 95.88, 82.84, 83.84, 84.73, 79.26, 73.25, 86.43, 81.71]\n",
      "Avarage Error: 101.34%, Adjusted Avarage Error: 89.04%\n",
      "Mean Square: 26.0267\n",
      "\n",
      "[85.65, 84.75, 88.21, 88.29, 87.14, 87.29, 231.94, 94.82, 89.25, 173.58, 80.49, 143.84, 95.85, 81.31, 83.78, 84.47, 79.07, 72.55, 86.4, 81.56]\n",
      "Avarage Error: 100.01%, Adjusted Avarage Error: 88.6%\n",
      "Mean Square: 26.4063\n",
      "\n",
      "[85.56, 84.62, 88.19, 88.25, 87.06, 87.21, 219.87, 94.8, 89.17, 172.18, 80.34, 139.23, 95.83, 79.17, 83.49, 83.97, 78.88, 71.81, 86.39, 81.41]\n",
      "Avarage Error: 98.87%, Adjusted Avarage Error: 88.08%\n",
      "Mean Square: 26.7353\n",
      "\n",
      "[85.48, 84.49, 88.17, 88.2, 86.99, 87.13, 211.17, 94.78, 89.09, 171.07, 80.19, 135.25, 95.81, 76.13, 82.47, 83.18, 78.68, 71.0, 86.39, 81.26]\n",
      "Avarage Error: 97.85%, Adjusted Avarage Error: 87.48%\n",
      "Mean Square: 26.9713\n",
      "\n",
      "[85.39, 84.35, 88.14, 88.16, 86.92, 87.06, 204.03, 94.76, 89.01, 170.34, 80.04, 131.65, 95.78, 71.95, 81.7, 82.11, 78.48, 70.11, 86.39, 81.11]\n",
      "Avarage Error: 96.87%, Adjusted Avarage Error: 86.84%\n",
      "Mean Square: 27.2172\n",
      "\n",
      "[85.3, 84.21, 88.12, 88.11, 86.85, 86.98, 197.28, 94.74, 88.93, 169.86, 79.89, 128.14, 95.75, 64.61, 81.13, 80.88, 78.29, 69.1, 86.39, 80.96]\n",
      "Avarage Error: 95.78%, Adjusted Avarage Error: 86.02%\n",
      "Mean Square: 27.5067\n",
      "\n",
      "[85.2, 84.07, 88.1, 88.07, 86.79, 86.9, 193.43, 94.71, 88.85, 169.6, 79.74, 124.73, 95.73, 51.23, 80.79, 79.9, 78.09, 67.94, 86.39, 80.81]\n",
      "Avarage Error: 94.55%, Adjusted Avarage Error: 84.89%\n",
      "Mean Square: 27.8988\n",
      "\n",
      "[85.11, 83.92, 88.08, 88.02, 86.72, 86.82, 190.38, 94.69, 88.77, 169.45, 79.59, 121.79, 95.7, 17.14, 80.46, 78.99, 77.9, 66.64, 86.41, 80.66]\n",
      "Avarage Error: 92.36%, Adjusted Avarage Error: 82.63%\n",
      "Mean Square: 28.3527\n",
      "\n",
      "[85.02, 83.78, 88.05, 87.98, 86.65, 86.75, 188.23, 94.66, 88.69, 169.45, 79.45, 119.07, 95.67, -230.35, 80.62, 78.17, 77.7, 65.2, 86.44, 80.51]\n",
      "Avarage Error: 102.62%, Adjusted Avarage Error: 90.77%\n",
      "Mean Square: 28.9187\n",
      "\n",
      "[84.93, 83.63, 88.03, 87.93, 86.59, 86.67, 186.28, 94.64, 88.61, 169.57, 79.3, 116.56, 95.65, 295.43, 80.95, 77.51, 77.49, 63.82, 86.46, 80.36]\n",
      "Avarage Error: 105.52%, Adjusted Avarage Error: 90.48%\n",
      "Mean Square: 29.6091\n",
      "\n",
      "[84.83, 83.47, 88.0, 87.88, 86.52, 86.59, 184.41, 94.61, 88.53, 169.79, 79.15, 114.57, 95.62, 177.97, 81.45, 76.81, 77.27, 62.39, 86.47, 80.21]\n",
      "Avarage Error: 99.33%, Adjusted Avarage Error: 90.23%\n",
      "Mean Square: 30.3782\n",
      "\n",
      "[84.74, 83.3, 87.98, 87.83, 86.46, 86.51, 182.64, 94.59, 88.44, 170.16, 79.0, 113.4, 95.58, 155.84, 82.18, 76.13, 77.04, 60.88, 86.47, 80.06]\n",
      "Avarage Error: 97.96%, Adjusted Avarage Error: 89.25%\n",
      "Mean Square: 31.1187\n",
      "\n",
      "[84.64, 83.12, 87.96, 87.78, 86.39, 86.42, 181.14, 94.56, 88.36, 170.69, 78.86, 112.49, 95.55, 148.1, 82.48, 75.59, 76.81, 59.42, 86.48, 79.9]\n",
      "Avarage Error: 97.34%, Adjusted Avarage Error: 88.6%\n",
      "Mean Square: 31.836\n",
      "\n",
      "Avarage Tot. Error: 97.05%\n",
      "\n",
      "After removing the two worst errors. Avarage Adjusted Tot. Error: 90.02%\n",
      "Total error for nodes 8, 9, 10, 11. Avarage Adjusted Tot. Error: 108.89%\n"
     ]
    }
   ],
   "source": [
    "testFinal(tester_model2, datasetNew4, device_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "real      out       %error    Sign      \n",
      "4.303     4.305     0.06      True      \n",
      "-0.844    -0.849    0.54      True      \n",
      "1.293     1.291     -0.16     True      \n",
      "2.235     2.237     0.09      True      \n",
      "2.203     2.202     -0.05     True      \n",
      "1.485     1.481     -0.24     True      \n",
      "0.398     0.399     0.43      True      \n",
      "-0.747    -0.754    1.03      True      \n",
      "-1.677    -1.681    0.21      True      \n",
      "-2.195    -2.203    0.36      True      \n",
      "-2.195    -2.206    0.49      True      \n",
      "-1.677    -1.692    0.87      True      \n",
      "-0.747    -0.766    2.58      True      \n",
      "0.398     0.397     -0.11     True      \n",
      "1.485     1.493     0.56      True      \n",
      "2.203     2.216     0.56      True      \n",
      "2.235     2.248     0.55      True      \n",
      "1.293     1.294     0.06      True      \n",
      "-0.844    -0.857    1.48      True      \n",
      "-4.303    -4.320    0.39      True      \n"
     ]
    }
   ],
   "source": [
    "testData(tester_model, datasetNew,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in the model: 95545\n"
     ]
    }
   ],
   "source": [
    "# printing parameters\n",
    "\n",
    "def count_parameters(model):\n",
    "    \"\"\"Count the number of trainable parameters in a PyTorch model.\"\"\"\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "# Print the number of parameters\n",
    "print(\"Number of parameters in the model: {}\".format(count_parameters(model4)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f155e3810749e3e5c86d0714d9f4acbbc66b2c24e666ad929ed58436b422284a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
