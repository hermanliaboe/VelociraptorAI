{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Run this before starting\n",
    "!python -c \"import torch; print(torch.__version__)\"\n",
    "!python -c \"import torch; print(torch.version.cuda)\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch.nn import Parameter\n",
    "from torch_geometric.nn.conv import MessagePassing\n",
    "from torch_geometric.utils import remove_self_loops, add_self_loops, softmax\n",
    "import torch.optim as optim\n",
    "from torch_geometric.nn.conv import feast_conv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normal(tensor, mean, std):\n",
    "    if tensor is not None:\n",
    "        tensor.data.normal_(mean, std)\n",
    "\n",
    "class FeaStConv(MessagePassing):\n",
    "    def __init__(self,\n",
    "                 in_channels,\n",
    "                 out_channels,\n",
    "                 heads=8,\n",
    "                 bias=True,\n",
    "                 t_inv=True,\n",
    "                 **kwargs):\n",
    "        super(FeaStConv, self).__init__(aggr='mean', **kwargs)\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.heads = heads\n",
    "        self.t_inv = t_inv\n",
    "\n",
    "        self.weight = Parameter(torch.Tensor(in_channels,\n",
    "                                             heads * out_channels))\n",
    "        self.u = Parameter(torch.Tensor(in_channels, heads))\n",
    "        self.c = Parameter(torch.Tensor(heads))\n",
    "        if not self.t_inv:\n",
    "            self.v = Parameter(torch.Tensor(in_channels, heads))\n",
    "\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.Tensor(out_channels))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        normal(self.weight, mean=0, std=0.1)\n",
    "        normal(self.u, mean=0, std=0.1)\n",
    "        normal(self.c, mean=0, std=0.1)\n",
    "        normal(self.bias, mean=0, std=0.1)\n",
    "        if not self.t_inv:\n",
    "            normal(self.v, mean=0, std=0.1)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        edge_index, _ = remove_self_loops(edge_index)\n",
    "        edge_index, _ = add_self_loops(edge_index, num_nodes=x.size(0))\n",
    "\n",
    "        return self.propagate(edge_index, x=x, num_nodes=x.size(0))\n",
    "\n",
    "    def message(self, x_i, x_j):\n",
    "        # dim: x_i, [E, F_in];\n",
    "        if self.t_inv:\n",
    "            # with translation invariance\n",
    "            q = torch.mm((x_i - x_j), self.u) + self.c  #[E, heads]\n",
    "        else:\n",
    "            q = torch.mm(x_i, self.u) + torch.mm(x_j, self.v) + self.c\n",
    "        q = F.softmax(q, dim=1)  #[E, heads]\n",
    "\n",
    "        x_j = torch.mm(x_j, self.weight).view(-1, self.heads,\n",
    "                                              self.out_channels)\n",
    "        return (x_j * q.view(-1, self.heads, 1)).sum(dim=1)\n",
    "\n",
    "    def update(self, aggr_out):\n",
    "        if self.bias is not None:\n",
    "            aggr_out = aggr_out + self.bias\n",
    "        return aggr_out\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '{}({}, {}, heads={})'.format(self.__class__.__name__,\n",
    "                                             self.in_channels,\n",
    "                                             self.out_channels, self.heads)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeaStNet(torch.nn.Module):\n",
    "    def __init__(self, in_channels, num_classes, heads, t_inv=True):\n",
    "        super(FeaStNet, self).__init__()\n",
    "\n",
    "        self.fc0 = nn.Linear(in_channels, 16)\n",
    "        self.conv1 = feast_conv(16, 32, heads=heads, t_inv=t_inv)\n",
    "        self.conv2 = feast_conv(32, 64, heads=heads, t_inv=t_inv)\n",
    "        self.conv3 = feast_conv(64, 128, heads=heads, t_inv=t_inv)\n",
    "        self.fc1 = nn.Linear(128, 256)\n",
    "        self.fc2 = nn.Linear(256, num_classes)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.conv1.reset_parameters()\n",
    "        self.conv2.reset_parameters()\n",
    "        self.conv3.reset_parameters()\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = F.elu(self.fc0(x))\n",
    "        x = F.elu(self.conv1(x, edge_index))\n",
    "        x = F.elu(self.conv2(x, edge_index))\n",
    "        x = F.elu(self.conv3(x, edge_index))\n",
    "        x = F.elu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n",
      "torch.float32\n",
      "torch.Size([20, 3])\n",
      "torch.Size([2, 38])\n",
      "torch.Size([20])\n"
     ]
    }
   ],
   "source": [
    "#data\n",
    "#TxtFile to lists\n",
    "import numpy as np \n",
    "\n",
    "#data = np.loadtxt('C:\\\\Users\\\\eliasak\\\\OneDrive - NTNU\\\\Master thesis\\\\07_ML\\\\ArchGNN\\\\data.txt')\n",
    "\n",
    "values_list = []  # create an empty list to store the values for each row\n",
    "x = []\n",
    "z = []\n",
    "m = []\n",
    "l = []\n",
    "\n",
    "with open(\"C:\\\\Users\\kt\\\\OneDrive - NTNU\\\\Master thesis\\\\03_Herman\\\\mega-data.txt\", \"r\") as f:\n",
    "    for line in f:  # loop over each line in the file\n",
    "        values = line.strip().split(\"\\t\")  # split the line into a list of strings\n",
    "\n",
    "        xL = []\n",
    "        zL = []\n",
    "        mL = []\n",
    "        lL = []\n",
    "\n",
    "        for i in range(20):\n",
    "            xL.append(round(float(values[i]),3))\n",
    "            zL.append(round(float(values[i+20]),3))\n",
    "            mL.append(round(float(values[i+20*2]),3))\n",
    "            try:\n",
    "                lL.append(round(float(values[i+20*3]),3))\n",
    "            except:\n",
    "                None\n",
    "        x.append(xL)\n",
    "        z.append(zL)\n",
    "        m.append(mL)\n",
    "        l.append(lL)\n",
    "\n",
    "\n",
    "dataY = []\n",
    "\n",
    "dataX = []\n",
    "for i in range(len(x)):\n",
    "    dataXs = []\n",
    "    for j in range(0,len(x[i])):\n",
    "        dataX0 = []\n",
    "        dataX0.append(j)\n",
    "        dataX0.append(x[i][j])\n",
    "        dataX0.append(z[i][j])\n",
    "        dataXs.append(dataX0)\n",
    "    dataX.append(dataXs)\n",
    "\n",
    "dataX = np.array(dataX)\n",
    "dataY = np.array(m)\n",
    "\n",
    "dataEgdeIndex = []\n",
    "for i in range(len(x[0])-1):\n",
    "    dataEgdeIndex.append([i,i+1])\n",
    "    dataEgdeIndex.append([i+1,i])  \n",
    "\n",
    "dataEgdeIndex = np.array(dataEgdeIndex) \n",
    "dataEgdeIndex =np.transpose(dataEgdeIndex)\n",
    "\n",
    "\n",
    "input_data = dataX\n",
    "target_data = dataY\n",
    "edge_index = dataEgdeIndex\n",
    "dataX = torch.from_numpy(dataX)\n",
    "dataY = torch.from_numpy(dataY)\n",
    "edge_index = torch.from_numpy(edge_index)\n",
    "edge_index = edge_index.to(torch.long)\n",
    "dataX = dataX.to(torch.float)\n",
    "dataY = dataY.to(torch.float)\n",
    "\n",
    "\n",
    "dataset = []\n",
    "for i in range(dataX.shape[0]):\n",
    "    dataset.append(Data(x=dataX[i], edge_index=edge_index, y=dataY[i]))\n",
    "\n",
    "train_loader = dataset[:200]\n",
    "test_loader = dataset[200:]\n",
    "print(dataX.dtype)\n",
    "print(dataY.dtype)\n",
    "\n",
    "\n",
    "# one data object from train_loader:\n",
    "graph = train_loader[0]\n",
    "\n",
    "print(graph.x.shape)\n",
    "print(graph.edge_index.shape)\n",
    "print(graph.y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArchNN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, num_classes, heads, t_inv = True):\n",
    "        super(ArchNN, self).__init__()\n",
    "        self.fc0 = nn.Linear(in_channels, 16)\n",
    "        self.conv1 = FeaStConv(16, 32, heads=heads, t_inv=t_inv)\n",
    "        self.conv2 = FeaStConv(32, 64, heads=heads, t_inv=t_inv)\n",
    "        self.conv3 = FeaStConv(64, 128, heads=heads, t_inv=t_inv)\n",
    "        self.fc1 = nn.Linear(128, 256)\n",
    "        self.fc2 = nn.Linear(256, 1)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.conv1.reset_parameters()\n",
    "        self.conv2.reset_parameters()\n",
    "        self.conv3.reset_parameters()\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = F.elu(self.fc0(x))\n",
    "        x = F.elu(self.conv1(x, edge_index))\n",
    "        x = F.elu(self.conv2(x, edge_index))\n",
    "        x = F.elu(self.conv3(x, edge_index))\n",
    "        x = F.elu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        #F.log_softmax(x, dijm=1)\n",
    "        x = torch.squeeze(x, dim=1)\n",
    "        return x\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def print_info(info):\n",
    "    message = ('Epoch: {}/{}, Duration: {:.3f}s,'\n",
    "               'Train Loss: {:.4f}, Test Loss:{:.4f}').format(\n",
    "                   info['current_epoch'], info['epochs'], info['t_duration'],\n",
    "                   info['train_loss'], info['test_loss'])\n",
    "    print(message)\n",
    "\n",
    "\n",
    "def run(model, train_loader, test_loader, num_nodes, epochs, optimizer):\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        t = time.time()\n",
    "        train_loss = train(model, train_loader, optimizer)\n",
    "        t_duration = time.time() - t\n",
    "        test_loss = test(model, test_loader, num_nodes)\n",
    "        eval_info = {\n",
    "            'train_loss': train_loss,\n",
    "            'test_loss': test_loss,\n",
    "            'current_epoch': epoch,\n",
    "            'epochs': epochs,\n",
    "            't_duration': t_duration\n",
    "        }\n",
    "\n",
    "        print_info(eval_info)\n",
    "\n",
    "\n",
    "def train(model, train_loader, optimizer):\n",
    "    model.train()\n",
    "\n",
    "    total_loss = 0\n",
    "    for idx, data in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        #log_probs = F.log_softmax(output, dim=1).double()\n",
    "        #print(log_probs.dtype)\n",
    "        #print(data.y.dtype)\n",
    "        loss = F.mse_loss(output, data.y)\n",
    "        #loss = F.nll_loss(log_probs, data.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "\n",
    "def test(model, test_loader, num_nodes):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total_loss = 0\n",
    "    n_graphs = 0\n",
    "    with torch.no_grad():\n",
    "        for idx, data in enumerate(test_loader):\n",
    "            out = model(data)\n",
    "            total_loss += F.mse_loss(out, data.y).item()\n",
    "            #pred = out.max(1)[1]\n",
    "            #correct += pred.eq(data.y).sum().item()\n",
    "            #n_graphs += data.num_graphs\n",
    "    return total_loss / len(test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10, Duration: 1.599s,Train Loss: 31.1131, Test Loss:455.2588\n",
      "Epoch: 2/10, Duration: 1.546s,Train Loss: 3.3077, Test Loss:464.3560\n",
      "Epoch: 3/10, Duration: 1.584s,Train Loss: 4.7777, Test Loss:384.6578\n",
      "Epoch: 4/10, Duration: 1.753s,Train Loss: 4.9031, Test Loss:308.6826\n",
      "Epoch: 5/10, Duration: 1.698s,Train Loss: 4.9050, Test Loss:358.8964\n",
      "Epoch: 6/10, Duration: 1.771s,Train Loss: 3.6502, Test Loss:347.6817\n",
      "Epoch: 7/10, Duration: 1.984s,Train Loss: 3.9746, Test Loss:370.7009\n",
      "Epoch: 8/10, Duration: 1.649s,Train Loss: 2.8581, Test Loss:361.9896\n",
      "Epoch: 9/10, Duration: 1.912s,Train Loss: 3.2240, Test Loss:353.9258\n",
      "Epoch: 10/10, Duration: 1.737s,Train Loss: 2.7431, Test Loss:359.0310\n"
     ]
    }
   ],
   "source": [
    "#runner\n",
    "num_nodes = train_loader[0].x.shape[0]\n",
    "num_features = train_loader[0].x.shape[1]\n",
    "\n",
    "model = ArchNN(num_features, num_nodes, heads=10)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(),\n",
    "                       lr=0.001)\n",
    "\n",
    "\n",
    "run(model, train_loader, test_loader, num_nodes, 10, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: loss=27.194686889648438\n",
      "Epoch 1: loss=23.968759536743164\n",
      "Epoch 2: loss=21.4787540435791\n",
      "Epoch 3: loss=19.75748062133789\n",
      "Epoch 4: loss=18.83110237121582\n",
      "Epoch 5: loss=18.46010971069336\n",
      "Epoch 6: loss=18.401479721069336\n",
      "Epoch 7: loss=18.3200740814209\n",
      "Epoch 8: loss=18.0425968170166\n",
      "Epoch 9: loss=17.594493865966797\n",
      "Epoch 10: loss=17.073143005371094\n",
      "Epoch 11: loss=16.612995147705078\n",
      "Epoch 12: loss=16.298181533813477\n",
      "Epoch 13: loss=16.103126525878906\n",
      "Epoch 14: loss=16.005006790161133\n",
      "Epoch 15: loss=15.95953369140625\n",
      "Epoch 16: loss=15.92503547668457\n",
      "Epoch 17: loss=15.877996444702148\n",
      "Epoch 18: loss=15.810765266418457\n",
      "Epoch 19: loss=15.72706127166748\n",
      "Epoch 20: loss=15.643503189086914\n",
      "Epoch 21: loss=15.57574462890625\n",
      "Epoch 22: loss=15.531913757324219\n",
      "Epoch 23: loss=15.508995056152344\n",
      "Epoch 24: loss=15.494709968566895\n",
      "Epoch 25: loss=15.474119186401367\n",
      "Epoch 26: loss=15.437162399291992\n",
      "Epoch 27: loss=15.396413803100586\n",
      "Epoch 28: loss=15.341238021850586\n",
      "Epoch 29: loss=15.272732734680176\n",
      "Epoch 30: loss=15.215484619140625\n",
      "Epoch 31: loss=15.18469524383545\n",
      "Epoch 32: loss=15.162510871887207\n",
      "Epoch 33: loss=15.140666007995605\n",
      "Epoch 34: loss=15.1116361618042\n",
      "Epoch 35: loss=15.071574211120605\n",
      "Epoch 36: loss=15.033129692077637\n",
      "Epoch 37: loss=14.992965698242188\n",
      "Epoch 38: loss=14.947591781616211\n",
      "Epoch 39: loss=14.900650024414062\n",
      "Epoch 40: loss=14.877293586730957\n",
      "Epoch 41: loss=14.851730346679688\n",
      "Epoch 42: loss=14.817270278930664\n",
      "Epoch 43: loss=14.772893905639648\n",
      "Epoch 44: loss=14.725802421569824\n",
      "Epoch 45: loss=14.690373420715332\n",
      "Epoch 46: loss=14.644685745239258\n",
      "Epoch 47: loss=14.619364738464355\n",
      "Epoch 48: loss=14.58965015411377\n",
      "Epoch 49: loss=14.552943229675293\n",
      "Epoch 50: loss=14.507730484008789\n",
      "Epoch 51: loss=14.461618423461914\n",
      "Epoch 52: loss=14.419944763183594\n",
      "Epoch 53: loss=14.36890697479248\n",
      "Epoch 54: loss=14.32396125793457\n",
      "Epoch 55: loss=14.280107498168945\n",
      "Epoch 56: loss=14.231744766235352\n",
      "Epoch 57: loss=14.186758041381836\n",
      "Epoch 58: loss=14.161712646484375\n",
      "Epoch 59: loss=14.12987232208252\n",
      "Epoch 60: loss=14.085469245910645\n",
      "Epoch 61: loss=14.046224594116211\n",
      "Epoch 62: loss=13.989375114440918\n",
      "Epoch 63: loss=13.950770378112793\n",
      "Epoch 64: loss=13.906529426574707\n",
      "Epoch 65: loss=13.858988761901855\n",
      "Epoch 66: loss=13.80235481262207\n",
      "Epoch 67: loss=13.751245498657227\n",
      "Epoch 68: loss=13.706174850463867\n",
      "Epoch 69: loss=13.634160995483398\n",
      "Epoch 70: loss=13.585428237915039\n",
      "Epoch 71: loss=13.54632568359375\n",
      "Epoch 72: loss=13.47564697265625\n",
      "Epoch 73: loss=13.437359809875488\n",
      "Epoch 74: loss=13.39153003692627\n",
      "Epoch 75: loss=13.322637557983398\n",
      "Epoch 76: loss=13.23725414276123\n",
      "Epoch 77: loss=13.235748291015625\n",
      "Epoch 78: loss=13.165998458862305\n",
      "Epoch 79: loss=13.037338256835938\n",
      "Epoch 80: loss=12.9708890914917\n",
      "Epoch 81: loss=12.879816055297852\n",
      "Epoch 82: loss=12.77706241607666\n",
      "Epoch 83: loss=12.696346282958984\n",
      "Epoch 84: loss=12.588312149047852\n",
      "Epoch 85: loss=12.515447616577148\n",
      "Epoch 86: loss=12.426271438598633\n",
      "Epoch 87: loss=12.372831344604492\n",
      "Epoch 88: loss=12.30498218536377\n",
      "Epoch 89: loss=12.214488983154297\n",
      "Epoch 90: loss=12.112728118896484\n",
      "Epoch 91: loss=12.005380630493164\n",
      "Epoch 92: loss=11.942048072814941\n",
      "Epoch 93: loss=11.788455963134766\n",
      "Epoch 94: loss=11.69173812866211\n",
      "Epoch 95: loss=11.584510803222656\n",
      "Epoch 96: loss=11.47330379486084\n",
      "Epoch 97: loss=11.385019302368164\n",
      "Epoch 98: loss=11.250876426696777\n",
      "Epoch 99: loss=11.135069847106934\n",
      "tensor([[0, 1, 1, 2, 2, 3, 3, 4],\n",
      "        [1, 0, 2, 1, 3, 2, 4, 3]])\n"
     ]
    }
   ],
   "source": [
    "#OLD\n",
    "# Define a simple GNN model\n",
    "class MyGNN(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(MyGNN, self).__init__()\n",
    "        \n",
    "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, hidden_dim)\n",
    "        self.conv3 = GCNConv(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.relu(self.conv2(x, edge_index))\n",
    "        x = self.conv3(x, edge_index)\n",
    "        return x\n",
    "\n",
    "# Define a simple training function for the GNN model\n",
    "def train(model, data, optimizer):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(data.x, data.edge_index)\n",
    "    loss = F.mse_loss(output, data.y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "# Define a simple testing function for the GNN model\n",
    "def test(model, data):\n",
    "    model.eval()\n",
    "    output = model(data.x, data.edge_index)\n",
    "    return output\n",
    "\n",
    "# Set up the input data\n",
    "x = torch.tensor([\n",
    "    [0,0,1,1,1,0,0,0],\n",
    "    [10,5,0,0,0,0,5,0],\n",
    "    [20,10,0,0,0,0,5,0],\n",
    "    [30,5,0,0,0,0,5,0],\n",
    "    [40,0,1,1,1,0,0,0]\n",
    "], dtype=torch.float)\n",
    "\n",
    "edge_index = torch.tensor([\n",
    "    [0,1], [1,0], [1,2], [2,1], [2,3], [3,2], [3,4], [4,3]\n",
    "], dtype=torch.long).t()\n",
    "\n",
    "# Set up the output data (nodal forces and moments)\n",
    "y = torch.tensor([\n",
    "    [0,0,0,0,0,10,20,0],\n",
    "    [0,0,0,0,0,5,0,0],\n",
    "    [0,0,0,0,0,5,0,0],\n",
    "    [0,0,0,0,0,5,0,0],\n",
    "    [0,0,0,0,0,10,20,0]\n",
    "], dtype=torch.float)\n",
    "\n",
    "# Create a Data object that encapsulates the input and output data\n",
    "data = Data(x=x, edge_index=edge_index, y=y)\n",
    "\n",
    "# Initialize the GNN model\n",
    "input_dim = x.shape[1]\n",
    "output_dim = y.shape[1]\n",
    "hidden_dim = 16\n",
    "model = MyGNN(input_dim, hidden_dim, output_dim)\n",
    "\n",
    "# Define the optimizer and the number of epochs for training\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "num_epochs = 100\n",
    "\n",
    "# Train the GNN model\n",
    "for epoch in range(num_epochs):\n",
    "    loss = train(model, data, optimizer)\n",
    "    print('Epoch {}: loss={}'.format(epoch, loss))\n",
    "\n",
    "# Test the GNN model on a new input example\n",
    "x_new = torch.tensor([\n",
    "    [0,0,1,1,1,0,0,0],\n",
    "    [10,5,0,0,0,0,5,0],\n",
    "    [20,10,0,0,0,0,5,0],\n",
    "    [30,5,0,0,0,0,5,0]\n",
    "])\n",
    "print(edge_index)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f155e3810749e3e5c86d0714d9f4acbbc66b2c24e666ad929ed58436b422284a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
