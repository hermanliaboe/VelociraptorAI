{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Run this before starting\n",
    "!python -c \"import torch; print(torch.__version__)\"\n",
    "!python -c \"import torch; print(torch.version.cuda)\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch.nn import Parameter\n",
    "from torch_geometric.nn.conv import MessagePassing\n",
    "from torch_geometric.utils import remove_self_loops, add_self_loops, softmax\n",
    "import torch.optim as optim\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normal(tensor, mean, std):\n",
    "    if tensor is not None:\n",
    "        tensor.data.normal_(mean, std)\n",
    "\n",
    "class FeaStConv(MessagePassing):\n",
    "    def __init__(self,\n",
    "                 in_channels,\n",
    "                 out_channels,\n",
    "                 heads=8,\n",
    "                 bias=True,\n",
    "                 t_inv=True,\n",
    "                 **kwargs):\n",
    "        super(FeaStConv, self).__init__(aggr='mean', **kwargs)\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.heads = heads\n",
    "        self.t_inv = t_inv\n",
    "\n",
    "        self.weight = Parameter(torch.Tensor(in_channels,\n",
    "                                             heads * out_channels))\n",
    "        self.u = Parameter(torch.Tensor(in_channels, heads))\n",
    "        self.c = Parameter(torch.Tensor(heads))\n",
    "        if not self.t_inv:\n",
    "            self.v = Parameter(torch.Tensor(in_channels, heads))\n",
    "\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.Tensor(out_channels))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        normal(self.weight, mean=0, std=0.1)\n",
    "        normal(self.u, mean=0, std=0.1)\n",
    "        normal(self.c, mean=0, std=0.1)\n",
    "        normal(self.bias, mean=0, std=0.1)\n",
    "        if not self.t_inv:\n",
    "            normal(self.v, mean=0, std=0.1)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        edge_index, _ = remove_self_loops(edge_index)\n",
    "        edge_index, _ = add_self_loops(edge_index, num_nodes=x.size(0))\n",
    "\n",
    "        return self.propagate(edge_index, x=x, num_nodes=x.size(0))\n",
    "\n",
    "    def message(self, x_i, x_j):\n",
    "        # dim: x_i, [E, F_in];\n",
    "        if self.t_inv:\n",
    "            # with translation invariance\n",
    "            q = torch.mm((x_i - x_j), self.u) + self.c  #[E, heads]\n",
    "        else:\n",
    "            q = torch.mm(x_i, self.u) + torch.mm(x_j, self.v) + self.c\n",
    "        q = F.softmax(q, dim=1)  #[E, heads]\n",
    "\n",
    "        x_j = torch.mm(x_j, self.weight).view(-1, self.heads,\n",
    "                                              self.out_channels)\n",
    "        return (x_j * q.view(-1, self.heads, 1)).sum(dim=1)\n",
    "\n",
    "    def update(self, aggr_out):\n",
    "        if self.bias is not None:\n",
    "            aggr_out = aggr_out + self.bias\n",
    "        return aggr_out\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '{}({}, {}, heads={})'.format(self.__class__.__name__,\n",
    "                                             self.in_channels,\n",
    "                                             self.out_channels, self.heads)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeaStNet(torch.nn.Module):\n",
    "    def __init__(self, in_channels, num_classes, heads, t_inv=True):\n",
    "        super(FeaStNet, self).__init__()\n",
    "\n",
    "        self.fc0 = nn.Linear(in_channels, 16)\n",
    "        self.conv1 = FeaStConv(16, 32, heads=heads, t_inv=t_inv)\n",
    "        self.conv2 = FeaStConv(32, 64, heads=heads, t_inv=t_inv)\n",
    "        self.conv3 = FeaStConv(64, 128, heads=heads, t_inv=t_inv)\n",
    "        self.fc1 = nn.Linear(128, 256)\n",
    "        self.fc2 = nn.Linear(256, num_classes)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.conv1.reset_parameters()\n",
    "        self.conv2.reset_parameters()\n",
    "        self.conv3.reset_parameters()\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = F.elu(self.fc0(x))\n",
    "        x = F.elu(self.conv1(x, edge_index))\n",
    "        x = F.elu(self.conv2(x, edge_index))\n",
    "        x = F.elu(self.conv3(x, edge_index))\n",
    "        x = F.elu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArchNN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, num_classes, heads, t_inv = True):\n",
    "        super(ArchNN, self).__init__()\n",
    "        self.fc0 = nn.Linear(in_channels, 16)\n",
    "        self.conv1 = FeaStConv(16, 32, heads=heads, t_inv=t_inv)\n",
    "        self.conv2 = FeaStConv(32, 64, heads=heads, t_inv=t_inv)\n",
    "        self.conv3 = FeaStConv(64, 128, heads=heads, t_inv=t_inv)\n",
    "        self.fc1 = nn.Linear(128, 256)\n",
    "        self.fc2 = nn.Linear(256, num_classes)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.conv1.reset_parameters()\n",
    "        self.conv2.reset_parameters()\n",
    "        self.conv3.reset_parameters()\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = F.elu(self.fc0(x))\n",
    "        x = F.elu(self.conv1(x, edge_index))\n",
    "        x = F.elu(self.conv2(x, edge_index))\n",
    "        x = F.elu(self.conv3(x, edge_index))\n",
    "        x = F.elu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        print(x)\n",
    "        print(F.log_softmax(x, dijm=1))\n",
    "        return F.log_softmax(x, dijm=1)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 5.5400, -1.2220,  1.5160,  2.7780,  2.8050,  1.9420,  0.5850, -0.8640,\n",
      "        -2.0530, -2.7170, -2.7170, -2.0530, -0.8640,  0.5850,  1.9420,  2.8050,\n",
      "         2.7780,  1.5160, -1.2220, -5.5400], dtype=torch.float64)\n",
      "tensor([[  0.0000,   0.0000,   0.0000],\n",
      "        [  1.0000,   1.0230,  12.3450],\n",
      "        [  2.0000,   4.0640,  24.3520],\n",
      "        [  3.0000,   9.0390,  35.6960],\n",
      "        [  4.0000,  15.8140,  46.0660],\n",
      "        [  5.0000,  24.2040,  55.1790],\n",
      "        [  6.0000,  33.9790,  62.7870],\n",
      "        [  7.0000,  44.8730,  68.6830],\n",
      "        [  8.0000,  56.5890,  72.7050],\n",
      "        [  9.0000,  68.8070,  74.7440],\n",
      "        [ 10.0000,  81.1930,  74.7440],\n",
      "        [ 11.0000,  93.4110,  72.7050],\n",
      "        [ 12.0000, 105.1270,  68.6830],\n",
      "        [ 13.0000, 116.0210,  62.7870],\n",
      "        [ 14.0000, 125.7960,  55.1790],\n",
      "        [ 15.0000, 134.1860,  46.0660],\n",
      "        [ 16.0000, 140.9610,  35.6960],\n",
      "        [ 17.0000, 145.9360,  24.3520],\n",
      "        [ 18.0000, 148.9770,  12.3450],\n",
      "        [ 19.0000, 150.0000,   0.0000]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "#data\n",
    "#TxtFile to lists\n",
    "import numpy as np \n",
    "\n",
    "#data = np.loadtxt('C:\\\\Users\\\\eliasak\\\\OneDrive - NTNU\\\\Master thesis\\\\07_ML\\\\ArchGNN\\\\data.txt')\n",
    "\n",
    "values_list = []  # create an empty list to store the values for each row\n",
    "x = []\n",
    "z = []\n",
    "m = []\n",
    "l = []\n",
    "\n",
    "with open(\"data.txt\", \"r\") as f:\n",
    "    for line in f:  # loop over each line in the file\n",
    "        values = line.strip().split(\"\\t\")  # split the line into a list of strings\n",
    "\n",
    "        xL = []\n",
    "        zL = []\n",
    "        mL = []\n",
    "        lL = []\n",
    "\n",
    "        for i in range(20):\n",
    "            xL.append(round(float(values[i]),3))\n",
    "            zL.append(round(float(values[i+20]),3))\n",
    "            mL.append(round(float(values[i+20*2]),3))\n",
    "            try:\n",
    "                lL.append(round(float(values[i+20*3]),3))\n",
    "            except:\n",
    "                None\n",
    "        x.append(xL)\n",
    "        z.append(zL)\n",
    "        m.append(mL)\n",
    "        l.append(lL)\n",
    "\n",
    "\n",
    "dataY = []\n",
    "\n",
    "dataX = []\n",
    "for i in range(len(x)):\n",
    "    dataXs = []\n",
    "    for j in range(0,len(x[i])):\n",
    "        dataX0 = []\n",
    "        dataX0.append(j)\n",
    "        dataX0.append(x[i][j])\n",
    "        dataX0.append(z[i][j])\n",
    "        dataXs.append(dataX0)\n",
    "    dataX.append(dataXs)\n",
    "\n",
    "dataX = np.array(dataX)\n",
    "dataY = np.array(m)\n",
    "\n",
    "dataEgdeIndex = []\n",
    "for i in range(len(x[0])-1):\n",
    "    dataEgdeIndex.append([i,i+1])\n",
    "    dataEgdeIndex.append([i+1,i])  \n",
    "\n",
    "dataEgdeIndex = np.array(dataEgdeIndex) \n",
    "dataEgdeIndex =np.transpose(dataEgdeIndex)\n",
    "\n",
    "\n",
    "input_data = dataX\n",
    "target_data = dataY\n",
    "edge_index = dataEgdeIndex\n",
    "dataX = torch.from_numpy(dataX)\n",
    "dataY = torch.from_numpy(dataY)\n",
    "edge_index = torch.from_numpy(edge_index)\n",
    "\n",
    "dataset = []\n",
    "for i in range(dataX.shape[0]):\n",
    "    dataset.append(Data(x=dataX[i], edge_index=edge_index, y=dataY[i]))\n",
    "\n",
    "train_loader = dataset[:200]\n",
    "test_loader = dataset[200:]\n",
    "print(dataset[0].y)\n",
    "print(dataset[0].x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def print_info(info):\n",
    "    message = ('Epoch: {}/{}, Duration: {:.3f}s, ACC: {:.4f}, '\n",
    "               'Train Loss: {:.4f}, Test Loss:{:.4f}').format(\n",
    "                   info['current_epoch'], info['epochs'], info['t_duration'],\n",
    "                   info['acc'], info['train_loss'], info['test_loss'])\n",
    "    print(message)\n",
    "\n",
    "\n",
    "def run(model, train_loader, test_loader, num_nodes, epochs, optimizer):\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        t = time.time()\n",
    "        train_loss = train(model, train_loader, optimizer)\n",
    "        t_duration = time.time() - t\n",
    "        acc, test_loss = test(model, test_loader, num_nodes)\n",
    "        eval_info = {\n",
    "            'train_loss': train_loss,\n",
    "            'test_loss': test_loss,\n",
    "            'acc': acc,\n",
    "            'current_epoch': epoch,\n",
    "            'epochs': epochs,\n",
    "            't_duration': t_duration\n",
    "        }\n",
    "\n",
    "        print_info(eval_info)\n",
    "\n",
    "\n",
    "def train(model, train_loader, optimizer):\n",
    "    model.train()\n",
    "\n",
    "    total_loss = 0\n",
    "    for idx, data in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        print(model(data))\n",
    "        loss = F.nll_loss(model(data), data.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "\n",
    "def test(model, test_loader, num_nodes):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total_loss = 0\n",
    "    n_graphs = 0\n",
    "    with torch.no_grad():\n",
    "        for idx, data in enumerate(test_loader):\n",
    "            out = model(data)\n",
    "            total_loss += F.nll_loss(out, data.y).item()\n",
    "            pred = out.max(1)[1]\n",
    "            correct += pred.eq(data.y).sum().item()\n",
    "            n_graphs += data.num_graphs\n",
    "    return correct / (n_graphs * num_nodes), total_loss / len(test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 must have the same dtype",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[55], line 11\u001b[0m\n\u001b[0;32m      5\u001b[0m model \u001b[39m=\u001b[39m ArchNN(num_features, num_nodes, heads\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m)\n\u001b[0;32m      7\u001b[0m optimizer \u001b[39m=\u001b[39m optim\u001b[39m.\u001b[39mAdam(model\u001b[39m.\u001b[39mparameters(),\n\u001b[0;32m      8\u001b[0m                        lr\u001b[39m=\u001b[39m\u001b[39m0.01\u001b[39m)\n\u001b[1;32m---> 11\u001b[0m run(model, train_loader, test_loader, num_nodes, \u001b[39m10\u001b[39;49m, optimizer)\n",
      "Cell \u001b[1;32mIn[52], line 18\u001b[0m, in \u001b[0;36mrun\u001b[1;34m(model, train_loader, test_loader, num_nodes, epochs, optimizer)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, epochs \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m):\n\u001b[0;32m     17\u001b[0m     t \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m---> 18\u001b[0m     train_loss \u001b[39m=\u001b[39m train(model, train_loader, optimizer)\n\u001b[0;32m     19\u001b[0m     t_duration \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m t\n\u001b[0;32m     20\u001b[0m     acc, test_loss \u001b[39m=\u001b[39m test(model, test_loader, num_nodes)\n",
      "Cell \u001b[1;32mIn[52], line 39\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, train_loader, optimizer)\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[39mfor\u001b[39;00m idx, data \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(train_loader):\n\u001b[0;32m     38\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m---> 39\u001b[0m     \u001b[39mprint\u001b[39m(model(data))\n\u001b[0;32m     40\u001b[0m     loss \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mnll_loss(model(data), data\u001b[39m.\u001b[39my)\n\u001b[0;32m     41\u001b[0m     loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\kt\\OneDrive - NTNU\\Master thesis\\03_Herman\\masters2023\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[54], line 20\u001b[0m, in \u001b[0;36mArchNN.forward\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, data):\n\u001b[0;32m     19\u001b[0m     x, edge_index \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39mx, data\u001b[39m.\u001b[39medge_index\n\u001b[1;32m---> 20\u001b[0m     x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39melu(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfc0(x))\n\u001b[0;32m     21\u001b[0m     x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39melu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv1(x, edge_index))\n\u001b[0;32m     22\u001b[0m     x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39melu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv2(x, edge_index))\n",
      "File \u001b[1;32mc:\\Users\\kt\\OneDrive - NTNU\\Master thesis\\03_Herman\\masters2023\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\kt\\OneDrive - NTNU\\Master thesis\\03_Herman\\masters2023\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 must have the same dtype"
     ]
    }
   ],
   "source": [
    "#runner\n",
    "num_nodes = train_loader[0].x.shape[0]\n",
    "num_features = train_loader[0].x.shape[1]\n",
    "\n",
    "model = ArchNN(num_features, num_nodes, heads=10)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(),\n",
    "                       lr=0.01)\n",
    "\n",
    "\n",
    "run(model, train_loader, test_loader, num_nodes, 10, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OLD\n",
    "# Define a simple GNN model\n",
    "class MyGNN(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(MyGNN, self).__init__()\n",
    "        \n",
    "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, hidden_dim)\n",
    "        self.conv3 = GCNConv(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.relu(self.conv2(x, edge_index))\n",
    "        x = self.conv3(x, edge_index)\n",
    "        return x\n",
    "\n",
    "# Define a simple training function for the GNN model\n",
    "def train(model, data, optimizer):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(data.x, data.edge_index)\n",
    "    loss = F.mse_loss(output, data.y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "# Define a simple testing function for the GNN model\n",
    "def test(model, data):\n",
    "    model.eval()\n",
    "    output = model(data.x, data.edge_index)\n",
    "    return output\n",
    "\n",
    "# Set up the input data\n",
    "x = torch.tensor([\n",
    "    [0,0,1,1,1,0,0,0],\n",
    "    [10,5,0,0,0,0,5,0],\n",
    "    [20,10,0,0,0,0,5,0],\n",
    "    [30,5,0,0,0,0,5,0],\n",
    "    [40,0,1,1,1,0,0,0]\n",
    "], dtype=torch.float)\n",
    "\n",
    "edge_index = torch.tensor([\n",
    "    [0,1], [1,0], [1,2], [2,1], [2,3], [3,2], [3,4], [4,3]\n",
    "], dtype=torch.long).t()\n",
    "\n",
    "# Set up the output data (nodal forces and moments)\n",
    "y = torch.tensor([\n",
    "    [0,0,0,0,0,10,20,0],\n",
    "    [0,0,0,0,0,5,0,0],\n",
    "    [0,0,0,0,0,5,0,0],\n",
    "    [0,0,0,0,0,5,0,0],\n",
    "    [0,0,0,0,0,10,20,0]\n",
    "], dtype=torch.float)\n",
    "\n",
    "# Create a Data object that encapsulates the input and output data\n",
    "data = Data(x=x, edge_index=edge_index, y=y)\n",
    "\n",
    "# Initialize the GNN model\n",
    "input_dim = x.shape[1]\n",
    "output_dim = y.shape[1]\n",
    "hidden_dim = 16\n",
    "model = MyGNN(input_dim, hidden_dim, output_dim)\n",
    "\n",
    "# Define the optimizer and the number of epochs for training\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "num_epochs = 100\n",
    "\n",
    "# Train the GNN model\n",
    "for epoch in range(num_epochs):\n",
    "    loss = train(model, data, optimizer)\n",
    "    print('Epoch {}: loss={}'.format(epoch, loss))\n",
    "\n",
    "# Test the GNN model on a new input example\n",
    "x_new = torch.tensor([\n",
    "    [0,0,1,1,1,0,0,0],\n",
    "    [10,5,0,0,0,0,5,0],\n",
    "    [20,10,0,0,0,0,5,0],\n",
    "    [30,5,0,0,0,0,5,0]\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cu121Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: torch in c:\\users\\kt\\onedrive - ntnu\\master thesis\\03_herman\\masters2023\\.venv\\lib\\site-packages (2.0.0)\n",
      "Requirement already satisfied: torchvision in c:\\users\\kt\\onedrive - ntnu\\master thesis\\03_herman\\masters2023\\.venv\\lib\\site-packages (0.15.1+cu117)\n",
      "Requirement already satisfied: torchaudio in c:\\users\\kt\\onedrive - ntnu\\master thesis\\03_herman\\masters2023\\.venv\\lib\\site-packages (2.0.1+cu117)\n",
      "Requirement already satisfied: filelock in c:\\users\\kt\\onedrive - ntnu\\master thesis\\03_herman\\masters2023\\.venv\\lib\\site-packages (from torch) (3.10.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\kt\\onedrive - ntnu\\master thesis\\03_herman\\masters2023\\.venv\\lib\\site-packages (from torch) (4.5.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\kt\\onedrive - ntnu\\master thesis\\03_herman\\masters2023\\.venv\\lib\\site-packages (from torch) (1.11.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\kt\\onedrive - ntnu\\master thesis\\03_herman\\masters2023\\.venv\\lib\\site-packages (from torch) (3.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\kt\\onedrive - ntnu\\master thesis\\03_herman\\masters2023\\.venv\\lib\\site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\kt\\onedrive - ntnu\\master thesis\\03_herman\\masters2023\\.venv\\lib\\site-packages (from torchvision) (1.24.2)\n",
      "Requirement already satisfied: requests in c:\\users\\kt\\onedrive - ntnu\\master thesis\\03_herman\\masters2023\\.venv\\lib\\site-packages (from torchvision) (2.28.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\kt\\onedrive - ntnu\\master thesis\\03_herman\\masters2023\\.venv\\lib\\site-packages (from torchvision) (9.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\kt\\onedrive - ntnu\\master thesis\\03_herman\\masters2023\\.venv\\lib\\site-packages (from jinja2->torch) (2.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\kt\\onedrive - ntnu\\master thesis\\03_herman\\masters2023\\.venv\\lib\\site-packages (from requests->torchvision) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\kt\\onedrive - ntnu\\master thesis\\03_herman\\masters2023\\.venv\\lib\\site-packages (from requests->torchvision) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\kt\\onedrive - ntnu\\master thesis\\03_herman\\masters2023\\.venv\\lib\\site-packages (from requests->torchvision) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\kt\\onedrive - ntnu\\master thesis\\03_herman\\masters2023\\.venv\\lib\\site-packages (from requests->torchvision) (2022.12.7)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\kt\\onedrive - ntnu\\master thesis\\03_herman\\masters2023\\.venv\\lib\\site-packages (from sympy->torch) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numba\n",
      "  Using cached numba-0.56.4.tar.gz (2.4 MB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'error'\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × python setup.py egg_info did not run successfully.\n",
      "  │ exit code: 1\n",
      "  ╰─> [8 lines of output]\n",
      "      Traceback (most recent call last):\n",
      "        File \"<string>\", line 2, in <module>\n",
      "        File \"<pip-setuptools-caller>\", line 34, in <module>\n",
      "        File \"C:\\Users\\kt\\AppData\\Local\\Temp\\pip-install-zrcxyxi6\\numba_7683b5d5502042c187dcc44c323381b0\\setup.py\", line 51, in <module>\n",
      "          _guard_py_ver()\n",
      "        File \"C:\\Users\\kt\\AppData\\Local\\Temp\\pip-install-zrcxyxi6\\numba_7683b5d5502042c187dcc44c323381b0\\setup.py\", line 48, in _guard_py_ver\n",
      "          raise RuntimeError(msg.format(cur_py, min_py, max_py))\n",
      "      RuntimeError: Cannot install on Python version 3.11.0; only versions >=3.7,<3.11 are supported.\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: metadata-generation-failed\n",
      "\n",
      "× Encountered error while generating package metadata.\n",
      "╰─> See above for output.\n",
      "\n",
      "note: This is an issue with the package mentioned above, not pip.\n",
      "hint: See above for details.\n"
     ]
    }
   ],
   "source": [
    "pip install numba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numba'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnumba\u001b[39;00m \u001b[39mimport\u001b[39;00m jit, cuda\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'numba'"
     ]
    }
   ],
   "source": [
    "from numba import jit, cuda"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f155e3810749e3e5c86d0714d9f4acbbc66b2c24e666ad929ed58436b422284a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
