{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Run this before starting\n",
    "!python -c \"import torch; print(torch.__version__)\"\n",
    "!python -c \"import torch; print(torch.version.cuda)\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch.nn import Parameter\n",
    "from torch_geometric.nn.conv import MessagePassing\n",
    "from torch_geometric.utils import remove_self_loops, add_self_loops, softmax\n",
    "import torch.optim as optim\n",
    "from torch_geometric.nn.conv import feast_conv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu.\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device_name = torch.device(\"cuda\")\n",
    "else:\n",
    "    device_name = torch.device('cpu')\n",
    "\n",
    "print(\"Using {}.\".format(device_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7678\n",
      "train_loader size:  5182\n",
      "test_loader size:  1728\n",
      "torch.Size([20, 3])\n",
      "torch.Size([2, 38])\n",
      "torch.Size([20])\n"
     ]
    }
   ],
   "source": [
    "#data\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "\n",
    "#Import data function\n",
    "def get_data(filepath):\n",
    "    data = np.loadtxt(filepath)\n",
    "    x = data[:, :20]\n",
    "    z = data[:, 20:40]\n",
    "    m = data[:, 40:60]\n",
    "\n",
    "    dataX = []\n",
    "    for i in range(x.shape[0]):\n",
    "        dataXs = np.column_stack((np.arange(20), x[i], z[i]))\n",
    "        dataX.append(dataXs)\n",
    "\n",
    "    dataY = m\n",
    "    zipped = list(zip(dataX, dataY))\n",
    "    np.random.shuffle(zipped)\n",
    "    dataX, dataY = zip(*zipped)\n",
    "\n",
    "    dataX = np.array(dataX)\n",
    "    dataY = np.array(dataY)\n",
    "\n",
    "    dataEdgeIndex = np.column_stack((np.arange(20)[:-1], np.arange(20)[1:]))\n",
    "    dataEdgeIndex = np.vstack((dataEdgeIndex, dataEdgeIndex[:, ::-1])).T\n",
    "\n",
    "    dataset = [Data(x=torch.from_numpy(x).float(), edge_index=torch.from_numpy(dataEdgeIndex).long(), y=torch.from_numpy(y).float()) for x, y in zip(dataX, dataY)]\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "#Getting data\n",
    "dataset = get_data(\"data1.txt\")\n",
    "print(len(dataset))\n",
    "#Splitting dataset\n",
    "datasetRun = dataset[:int(len(dataset)*0.90)]\n",
    "datasetTest = dataset[int(len(dataset)*0.90):]\n",
    "\n",
    "train_loader = datasetRun[:int(len(datasetRun)*0.75)]\n",
    "test_loader = datasetRun[int(len(datasetRun)*0.75):]\n",
    "\"\"\"\n",
    "train_loader = train_loader[:100]\n",
    "test_loader = test_loader[:25]\n",
    "\"\"\"\n",
    "\n",
    "#Checks\n",
    "\n",
    "print('train_loader size: ', len(train_loader))\n",
    "print('test_loader size: ', len(test_loader))\n",
    "\n",
    "\n",
    "# one data object from train_loader:\n",
    "graph = train_loader[0]\n",
    "print(graph.x.shape)\n",
    "print(graph.edge_index.shape)\n",
    "print(graph.y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArchNN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, num_classes, heads, t_inv = True):\n",
    "        super(ArchNN, self).__init__()\n",
    "        self.fc0 = nn.Linear(in_channels, 16)\n",
    "        self.conv1 = feast_conv.FeaStConv(16, 32, heads=heads, t_inv=t_inv)\n",
    "        self.conv2 = feast_conv.FeaStConv(32, 64, heads=heads, t_inv=t_inv)\n",
    "        self.conv3 = feast_conv.FeaStConv(64, 128, heads=heads, t_inv=t_inv)\n",
    "        #self.conv4 = feast_conv.FeaStConv(128, 256, heads=heads, t_inv=t_inv)\n",
    "        #self.conv5 = feast_conv.FeaStConv(256, 512, heads=heads, t_inv=t_inv)\n",
    "        #self.conv6 = feast_conv.FeaStConv(512, 256, heads=heads, t_inv=t_inv)\n",
    "        #self.conv7 = feast_conv.FeaStConv(256, 128, heads=heads, t_inv=t_inv)\n",
    "        self.fc1 = nn.Linear(128, 64)\n",
    "        self.fc2 = nn.Linear(64, 1)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.conv1.reset_parameters()\n",
    "        self.conv2.reset_parameters()\n",
    "        self.conv3.reset_parameters()\n",
    "        #self.conv4.reset_parameters()\n",
    "        #self.conv5.reset_parameters()\n",
    "        #self.conv6.reset_parameters()\n",
    "        #self.conv7.reset_parameters()\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = F.elu(self.fc0(x))\n",
    "        x = F.elu(self.conv1(x, edge_index))\n",
    "        x = F.elu(self.conv2(x, edge_index))\n",
    "        x = F.elu(self.conv3(x, edge_index))\n",
    "        #x = F.elu(self.conv4(x, edge_index))\n",
    "        #x = F.elu(self.conv5(x, edge_index))\n",
    "        #x = F.elu(self.conv6(x, edge_index))\n",
    "        #x = F.elu(self.conv7(x, edge_index))\n",
    "        x = F.elu(self.fc1(x))\n",
    "        #x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        #F.log_softmax(x, dijm=1)\n",
    "        x = torch.squeeze(x, dim=1)\n",
    "        return x\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def print_info(info):\n",
    "    message = ('Epoch: {}/{}, Duration: {:.3f}s,'\n",
    "               'Train Loss: {:.4f}, Test Loss:{:.4f}').format(\n",
    "                   info['current_epoch'], info['epochs'], info['t_duration'],\n",
    "                   info['train_loss'], info['test_loss'])\n",
    "    print(message)\n",
    "\n",
    "\n",
    "def run(model, train_loader, test_loader, num_nodes, epochs, optimizer, device):\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        t = time.time()\n",
    "        train_loss = train(model, train_loader, optimizer, device)\n",
    "        t_duration = time.time() - t\n",
    "        test_loss = test(model, test_loader, num_nodes, device)\n",
    "        eval_info = {\n",
    "            'train_loss': train_loss,\n",
    "            'test_loss': test_loss,\n",
    "            'current_epoch': epoch,\n",
    "            'epochs': epochs,\n",
    "            't_duration': t_duration\n",
    "        }\n",
    "\n",
    "        print_info(eval_info)\n",
    "\n",
    "\n",
    "def train(model, train_loader, optimizer, device):\n",
    "    model.train()\n",
    "\n",
    "    total_loss = 0\n",
    "    for idx, data in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data.to(device))\n",
    "        losses = F.mse_loss(output, data.y.to(device), reduction='none')  # Calculate MSE loss for each item in output and labels\n",
    "        loss = losses.mean()  # Compute mean loss for backpropagation\n",
    "        loss.backward(retain_graph=True)\n",
    "        #loss = model.compute_loss(output, data.y)  # compute loss with L1 regularization\n",
    "        #loss = F.nll_loss(log_probs, data.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "\n",
    "def test(model, test_loader, num_nodes, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total_loss = 0\n",
    "    n_graphs = 0\n",
    "    with torch.no_grad():\n",
    "        for idx, data in enumerate(test_loader):\n",
    "            out = model(data.to(device))\n",
    "            total_loss += F.mse_loss(out, data.y.to(device)).item()\n",
    "            #pred = out.max(1)[1]\n",
    "            #correct += pred.eq(data.y).sum().item()\n",
    "            #n_graphs += data.num_graphs\n",
    "    return total_loss / len(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/100, Duration: 42.933s,Train Loss: 5.5934, Test Loss:4.4755\n",
      "Epoch: 2/100, Duration: 46.307s,Train Loss: 3.3071, Test Loss:2.8194\n",
      "Epoch: 3/100, Duration: 56.143s,Train Loss: 2.1562, Test Loss:1.9985\n",
      "Epoch: 4/100, Duration: 45.732s,Train Loss: 1.5677, Test Loss:1.3895\n",
      "Epoch: 5/100, Duration: 49.264s,Train Loss: 1.1568, Test Loss:0.9881\n",
      "Epoch: 6/100, Duration: 46.661s,Train Loss: 0.9285, Test Loss:0.7710\n",
      "Epoch: 7/100, Duration: 50.323s,Train Loss: 0.7896, Test Loss:0.6327\n",
      "Epoch: 8/100, Duration: 54.681s,Train Loss: 0.6858, Test Loss:0.5348\n",
      "Epoch: 9/100, Duration: 48.862s,Train Loss: 0.5986, Test Loss:0.4639\n",
      "Epoch: 10/100, Duration: 46.966s,Train Loss: 0.5260, Test Loss:0.4053\n",
      "Epoch: 11/100, Duration: 46.569s,Train Loss: 0.4642, Test Loss:0.3563\n",
      "Epoch: 12/100, Duration: 47.994s,Train Loss: 0.4117, Test Loss:0.3168\n",
      "Epoch: 13/100, Duration: 47.407s,Train Loss: 0.3636, Test Loss:0.2801\n",
      "Epoch: 14/100, Duration: 46.450s,Train Loss: 0.3195, Test Loss:0.2405\n",
      "Epoch: 15/100, Duration: 46.379s,Train Loss: 0.2894, Test Loss:0.2092\n",
      "Epoch: 16/100, Duration: 46.166s,Train Loss: 0.2676, Test Loss:0.1864\n",
      "Epoch: 17/100, Duration: 46.993s,Train Loss: 0.2476, Test Loss:0.1719\n",
      "Epoch: 18/100, Duration: 46.779s,Train Loss: 0.2284, Test Loss:0.1656\n",
      "Epoch: 19/100, Duration: 47.313s,Train Loss: 0.2102, Test Loss:0.1658\n",
      "Epoch: 20/100, Duration: 46.392s,Train Loss: 0.1947, Test Loss:0.1647\n",
      "Epoch: 21/100, Duration: 46.905s,Train Loss: 0.1835, Test Loss:0.1566\n",
      "Epoch: 22/100, Duration: 46.596s,Train Loss: 0.1749, Test Loss:0.1439\n",
      "Epoch: 23/100, Duration: 46.721s,Train Loss: 0.1675, Test Loss:0.1313\n",
      "Epoch: 24/100, Duration: 46.234s,Train Loss: 0.1593, Test Loss:0.1188\n",
      "Epoch: 25/100, Duration: 46.574s,Train Loss: 0.1503, Test Loss:0.1076\n",
      "Epoch: 26/100, Duration: 45.890s,Train Loss: 0.1422, Test Loss:0.0941\n",
      "Epoch: 27/100, Duration: 45.934s,Train Loss: 0.1347, Test Loss:0.0789\n",
      "Epoch: 28/100, Duration: 46.004s,Train Loss: 0.1261, Test Loss:0.0739\n",
      "Epoch: 29/100, Duration: 46.175s,Train Loss: 0.1205, Test Loss:0.0721\n",
      "Epoch: 30/100, Duration: 45.781s,Train Loss: 0.1166, Test Loss:0.0712\n",
      "Epoch: 31/100, Duration: 45.190s,Train Loss: 0.1135, Test Loss:0.0706\n",
      "Epoch: 32/100, Duration: 44.689s,Train Loss: 0.1123, Test Loss:0.0701\n",
      "Epoch: 33/100, Duration: 44.516s,Train Loss: 0.1096, Test Loss:0.0725\n",
      "Epoch: 34/100, Duration: 45.048s,Train Loss: 0.1064, Test Loss:0.0766\n",
      "Epoch: 35/100, Duration: 45.003s,Train Loss: 0.1032, Test Loss:0.0804\n",
      "Epoch: 36/100, Duration: 45.139s,Train Loss: 0.1002, Test Loss:0.0809\n",
      "Epoch: 37/100, Duration: 44.842s,Train Loss: 0.0971, Test Loss:0.0805\n",
      "Epoch: 38/100, Duration: 44.805s,Train Loss: 0.0944, Test Loss:0.0808\n",
      "Epoch: 39/100, Duration: 44.566s,Train Loss: 0.0918, Test Loss:0.0836\n",
      "Epoch: 40/100, Duration: 44.806s,Train Loss: 0.0895, Test Loss:0.0920\n",
      "Epoch: 41/100, Duration: 45.234s,Train Loss: 0.0858, Test Loss:0.0914\n",
      "Epoch: 42/100, Duration: 46.069s,Train Loss: 0.0835, Test Loss:0.0953\n",
      "Epoch: 43/100, Duration: 46.102s,Train Loss: 0.0824, Test Loss:0.0900\n",
      "Epoch: 44/100, Duration: 44.930s,Train Loss: 0.0815, Test Loss:0.0950\n",
      "Epoch: 45/100, Duration: 45.113s,Train Loss: 0.0801, Test Loss:0.1074\n",
      "Epoch: 46/100, Duration: 44.918s,Train Loss: 0.0784, Test Loss:0.1119\n",
      "Epoch: 47/100, Duration: 45.621s,Train Loss: 0.0768, Test Loss:0.1286\n",
      "Epoch: 48/100, Duration: 45.177s,Train Loss: 0.0795, Test Loss:0.1041\n",
      "Epoch: 49/100, Duration: 45.244s,Train Loss: 0.0722, Test Loss:0.1165\n",
      "Epoch: 50/100, Duration: 45.051s,Train Loss: 0.0829, Test Loss:0.0587\n",
      "Epoch: 51/100, Duration: 45.465s,Train Loss: 0.0821, Test Loss:0.0632\n",
      "Epoch: 52/100, Duration: 44.835s,Train Loss: 0.0795, Test Loss:0.0747\n",
      "Epoch: 53/100, Duration: 44.929s,Train Loss: 0.0847, Test Loss:0.0593\n",
      "Epoch: 54/100, Duration: 45.234s,Train Loss: 0.0821, Test Loss:0.0565\n",
      "Epoch: 55/100, Duration: 45.435s,Train Loss: 0.0823, Test Loss:0.0542\n",
      "Epoch: 56/100, Duration: 45.218s,Train Loss: 0.0823, Test Loss:0.0526\n",
      "Epoch: 57/100, Duration: 45.985s,Train Loss: 0.0828, Test Loss:0.0517\n",
      "Epoch: 58/100, Duration: 45.050s,Train Loss: 0.0828, Test Loss:0.0495\n",
      "Epoch: 59/100, Duration: 44.797s,Train Loss: 0.0808, Test Loss:0.0467\n",
      "Epoch: 60/100, Duration: 45.528s,Train Loss: 0.0801, Test Loss:0.0455\n",
      "Epoch: 61/100, Duration: 45.438s,Train Loss: 0.0783, Test Loss:0.0436\n",
      "Epoch: 62/100, Duration: 45.469s,Train Loss: 0.0772, Test Loss:0.0436\n",
      "Epoch: 63/100, Duration: 45.011s,Train Loss: 0.0761, Test Loss:0.0436\n",
      "Epoch: 64/100, Duration: 45.624s,Train Loss: 0.0752, Test Loss:0.0444\n",
      "Epoch: 65/100, Duration: 45.123s,Train Loss: 0.0745, Test Loss:0.0446\n",
      "Epoch: 66/100, Duration: 45.631s,Train Loss: 0.0736, Test Loss:0.0440\n",
      "Epoch: 67/100, Duration: 45.740s,Train Loss: 0.0729, Test Loss:0.0439\n",
      "Epoch: 68/100, Duration: 44.973s,Train Loss: 0.0737, Test Loss:0.0443\n",
      "Epoch: 69/100, Duration: 45.119s,Train Loss: 0.0710, Test Loss:0.0441\n",
      "Epoch: 70/100, Duration: 45.379s,Train Loss: 0.0636, Test Loss:0.0423\n",
      "Epoch: 71/100, Duration: 44.973s,Train Loss: 0.0604, Test Loss:0.0439\n",
      "Epoch: 72/100, Duration: 46.032s,Train Loss: 0.0596, Test Loss:0.0433\n",
      "Epoch: 73/100, Duration: 45.410s,Train Loss: 0.0628, Test Loss:0.0386\n",
      "Epoch: 74/100, Duration: 45.112s,Train Loss: 0.0596, Test Loss:0.0426\n",
      "Epoch: 75/100, Duration: 45.179s,Train Loss: 0.0586, Test Loss:0.0411\n",
      "Epoch: 76/100, Duration: 45.147s,Train Loss: 0.0672, Test Loss:0.0411\n",
      "Epoch: 77/100, Duration: 45.278s,Train Loss: 0.0584, Test Loss:0.0394\n",
      "Epoch: 78/100, Duration: 45.799s,Train Loss: 0.0677, Test Loss:0.0406\n",
      "Epoch: 79/100, Duration: 45.251s,Train Loss: 0.0650, Test Loss:0.0398\n",
      "Epoch: 80/100, Duration: 45.233s,Train Loss: 0.0639, Test Loss:0.0370\n",
      "Epoch: 81/100, Duration: 45.330s,Train Loss: 0.0623, Test Loss:0.0380\n",
      "Epoch: 82/100, Duration: 45.011s,Train Loss: 0.0550, Test Loss:0.0389\n",
      "Epoch: 83/100, Duration: 44.951s,Train Loss: 0.0538, Test Loss:0.0396\n",
      "Epoch: 84/100, Duration: 44.971s,Train Loss: 0.0497, Test Loss:0.0514\n",
      "Epoch: 85/100, Duration: 45.313s,Train Loss: 0.0636, Test Loss:0.0376\n",
      "Epoch: 86/100, Duration: 45.355s,Train Loss: 0.0484, Test Loss:0.0475\n",
      "Epoch: 87/100, Duration: 44.845s,Train Loss: 0.0583, Test Loss:0.0386\n",
      "Epoch: 88/100, Duration: 44.863s,Train Loss: 0.0546, Test Loss:0.0371\n",
      "Epoch: 89/100, Duration: 45.385s,Train Loss: 0.0495, Test Loss:0.0389\n",
      "Epoch: 90/100, Duration: 44.977s,Train Loss: 0.0500, Test Loss:0.0390\n",
      "Epoch: 91/100, Duration: 45.475s,Train Loss: 0.0493, Test Loss:0.0426\n",
      "Epoch: 92/100, Duration: 45.022s,Train Loss: 0.0590, Test Loss:0.0350\n",
      "Epoch: 93/100, Duration: 45.045s,Train Loss: 0.0492, Test Loss:0.0379\n",
      "Epoch: 94/100, Duration: 45.174s,Train Loss: 0.0580, Test Loss:0.0343\n",
      "Epoch: 95/100, Duration: 44.780s,Train Loss: 0.0479, Test Loss:0.0372\n",
      "Epoch: 96/100, Duration: 45.235s,Train Loss: 0.0474, Test Loss:0.0370\n",
      "Epoch: 97/100, Duration: 45.176s,Train Loss: 0.0479, Test Loss:0.0399\n",
      "Epoch: 98/100, Duration: 45.355s,Train Loss: 0.0490, Test Loss:0.0348\n",
      "Epoch: 99/100, Duration: 44.974s,Train Loss: 0.0534, Test Loss:0.0311\n",
      "Epoch: 100/100, Duration: 45.193s,Train Loss: 0.0506, Test Loss:0.0313\n"
     ]
    }
   ],
   "source": [
    "#runner\n",
    "num_nodes = train_loader[0].x.shape[0]\n",
    "num_features = train_loader[0].x.shape[1]\n",
    "\n",
    "model = ArchNN(num_features, num_nodes, heads=1).to(device_name)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(),\n",
    "                       lr=0.0001,\n",
    "                       weight_decay=0.001)\n",
    "\n",
    "\n",
    "run(model, train_loader, test_loader, num_nodes, 100, optimizer, device_name)\n",
    "\n",
    "torch.save(model, \"apr18.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "\n",
    "torch.save(model, \"apr18.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1.5929, -0.0468,  0.8205,  1.1202,  0.9881,  0.5728,  0.0228, -0.5255,\n",
      "        -0.9589, -1.1967, -1.1967, -0.9589, -0.5255,  0.0228,  0.5728,  0.9881,\n",
      "         1.1202,  0.8205, -0.0468, -1.5929])\n",
      "tensor([ 1.6214, -0.0345,  0.8675,  1.1567,  1.0123,  0.5758,  0.0064, -0.5194,\n",
      "        -0.9393, -1.1778, -1.1911, -0.9689, -0.5195,  0.0392,  0.5983,  1.0137,\n",
      "         1.0970,  0.9285,  0.1239, -1.1868], grad_fn=<SqueezeBackward1>)\n",
      "tensor([-0.3996,  0.9465,  1.0823,  0.9144,  0.5465,  0.0754, -0.4131, -0.8463,\n",
      "        -1.1675, -1.3381, -1.3381, -1.1675, -0.8463, -0.4131,  0.0754,  0.5465,\n",
      "         0.9144,  1.0823,  0.9465,  0.3996])\n",
      "tensor([-0.5025,  0.8308,  1.0607,  0.9177,  0.5771,  0.0946, -0.3914, -0.8011,\n",
      "        -1.1205, -1.3207, -1.3267, -1.1601, -0.8647, -0.4389,  0.0315,  0.5263,\n",
      "         1.0105,  1.3051,  1.2712,  0.9909], grad_fn=<SqueezeBackward1>)\n",
      "tensor([-0.9057,  0.9870,  0.8736,  0.6225,  0.2890, -0.0760, -0.4270, -0.7255,\n",
      "        -0.9416, -1.0549, -1.0549, -0.9416, -0.7255, -0.4270, -0.0760,  0.2890,\n",
      "         0.6225,  0.8736,  0.9870,  0.9057])\n",
      "tensor([-0.9877,  0.9604,  0.8767,  0.6246,  0.3013, -0.0541, -0.4033, -0.7072,\n",
      "        -0.9531, -1.0766, -1.0551, -0.9181, -0.7101, -0.4481, -0.0939,  0.2664,\n",
      "         0.6853,  1.1070,  1.1520,  1.1563], grad_fn=<SqueezeBackward1>)\n"
     ]
    }
   ],
   "source": [
    "#load model\n",
    "\n",
    "model = torch.load(\"apr18.pt\")\n",
    "model.eval()\n",
    "tester = model(datasetTest[0].to(device_name))\n",
    "print(datasetTest[0].y)\n",
    "print(tester)\n",
    "tester = model(datasetTest[1].to(device_name))\n",
    "print(datasetTest[1].y)\n",
    "print(tester)\n",
    "tester = model(datasetTest[2].to(device_name))\n",
    "print(datasetTest[2].y)\n",
    "print(tester)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1.3550,  0.1630,  0.9600,  1.1790,  0.9760,  0.5080, -0.0740, -0.6390,\n",
      "        -1.0780, -1.3180, -1.3180, -1.0780, -0.6390, -0.0740,  0.5080,  0.9760,\n",
      "         1.1790,  0.9600,  0.1630, -1.3550])\n",
      "tensor([ 1.4588,  0.0168,  0.9337,  1.0978,  0.8159,  0.3859, -0.1911, -0.7819,\n",
      "        -1.2410, -1.4660, -1.4306, -1.0969, -0.5489,  0.1082,  0.5618,  0.9636,\n",
      "         1.3206,  0.9643,  0.0294, -1.8978], grad_fn=<SqueezeBackward1>)\n",
      "tensor([ 1.6200,  0.1130,  1.0340,  1.3030,  1.0950,  0.5870, -0.0530, -0.6770,\n",
      "        -1.1640, -1.4290, -1.4290, -1.1640, -0.6770, -0.0530,  0.5870,  1.0950,\n",
      "         1.3030,  1.0340,  0.1130, -1.6200])\n",
      "tensor([ 1.7416, -0.0129,  1.0249,  1.2137,  0.9230,  0.4418, -0.1897, -0.8252,\n",
      "        -1.3337, -1.5737, -1.4708, -1.0195, -0.3363,  0.4624,  0.9845,  1.3977,\n",
      "         1.5277,  1.0012, -0.6278, -2.5273], grad_fn=<SqueezeBackward1>)\n"
     ]
    }
   ],
   "source": [
    "model_tester_data = \"testdata.txt\"\n",
    "model_tester_dataset = create_dataset(model_tester_data)\n",
    "\n",
    "print(model_tester_dataset[0].y)\n",
    "tester = model(model_tester_dataset[0].to(device_name))\n",
    "\n",
    "print(tester)\n",
    "\n",
    "print(model_tester_dataset[1].y)\n",
    "tester2 = model(model_tester_dataset[1].to(device_name))\n",
    "print(tester2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: loss=27.194686889648438\n",
      "Epoch 1: loss=23.968759536743164\n",
      "Epoch 2: loss=21.4787540435791\n",
      "Epoch 3: loss=19.75748062133789\n",
      "Epoch 4: loss=18.83110237121582\n",
      "Epoch 5: loss=18.46010971069336\n",
      "Epoch 6: loss=18.401479721069336\n",
      "Epoch 7: loss=18.3200740814209\n",
      "Epoch 8: loss=18.0425968170166\n",
      "Epoch 9: loss=17.594493865966797\n",
      "Epoch 10: loss=17.073143005371094\n",
      "Epoch 11: loss=16.612995147705078\n",
      "Epoch 12: loss=16.298181533813477\n",
      "Epoch 13: loss=16.103126525878906\n",
      "Epoch 14: loss=16.005006790161133\n",
      "Epoch 15: loss=15.95953369140625\n",
      "Epoch 16: loss=15.92503547668457\n",
      "Epoch 17: loss=15.877996444702148\n",
      "Epoch 18: loss=15.810765266418457\n",
      "Epoch 19: loss=15.72706127166748\n",
      "Epoch 20: loss=15.643503189086914\n",
      "Epoch 21: loss=15.57574462890625\n",
      "Epoch 22: loss=15.531913757324219\n",
      "Epoch 23: loss=15.508995056152344\n",
      "Epoch 24: loss=15.494709968566895\n",
      "Epoch 25: loss=15.474119186401367\n",
      "Epoch 26: loss=15.437162399291992\n",
      "Epoch 27: loss=15.396413803100586\n",
      "Epoch 28: loss=15.341238021850586\n",
      "Epoch 29: loss=15.272732734680176\n",
      "Epoch 30: loss=15.215484619140625\n",
      "Epoch 31: loss=15.18469524383545\n",
      "Epoch 32: loss=15.162510871887207\n",
      "Epoch 33: loss=15.140666007995605\n",
      "Epoch 34: loss=15.1116361618042\n",
      "Epoch 35: loss=15.071574211120605\n",
      "Epoch 36: loss=15.033129692077637\n",
      "Epoch 37: loss=14.992965698242188\n",
      "Epoch 38: loss=14.947591781616211\n",
      "Epoch 39: loss=14.900650024414062\n",
      "Epoch 40: loss=14.877293586730957\n",
      "Epoch 41: loss=14.851730346679688\n",
      "Epoch 42: loss=14.817270278930664\n",
      "Epoch 43: loss=14.772893905639648\n",
      "Epoch 44: loss=14.725802421569824\n",
      "Epoch 45: loss=14.690373420715332\n",
      "Epoch 46: loss=14.644685745239258\n",
      "Epoch 47: loss=14.619364738464355\n",
      "Epoch 48: loss=14.58965015411377\n",
      "Epoch 49: loss=14.552943229675293\n",
      "Epoch 50: loss=14.507730484008789\n",
      "Epoch 51: loss=14.461618423461914\n",
      "Epoch 52: loss=14.419944763183594\n",
      "Epoch 53: loss=14.36890697479248\n",
      "Epoch 54: loss=14.32396125793457\n",
      "Epoch 55: loss=14.280107498168945\n",
      "Epoch 56: loss=14.231744766235352\n",
      "Epoch 57: loss=14.186758041381836\n",
      "Epoch 58: loss=14.161712646484375\n",
      "Epoch 59: loss=14.12987232208252\n",
      "Epoch 60: loss=14.085469245910645\n",
      "Epoch 61: loss=14.046224594116211\n",
      "Epoch 62: loss=13.989375114440918\n",
      "Epoch 63: loss=13.950770378112793\n",
      "Epoch 64: loss=13.906529426574707\n",
      "Epoch 65: loss=13.858988761901855\n",
      "Epoch 66: loss=13.80235481262207\n",
      "Epoch 67: loss=13.751245498657227\n",
      "Epoch 68: loss=13.706174850463867\n",
      "Epoch 69: loss=13.634160995483398\n",
      "Epoch 70: loss=13.585428237915039\n",
      "Epoch 71: loss=13.54632568359375\n",
      "Epoch 72: loss=13.47564697265625\n",
      "Epoch 73: loss=13.437359809875488\n",
      "Epoch 74: loss=13.39153003692627\n",
      "Epoch 75: loss=13.322637557983398\n",
      "Epoch 76: loss=13.23725414276123\n",
      "Epoch 77: loss=13.235748291015625\n",
      "Epoch 78: loss=13.165998458862305\n",
      "Epoch 79: loss=13.037338256835938\n",
      "Epoch 80: loss=12.9708890914917\n",
      "Epoch 81: loss=12.879816055297852\n",
      "Epoch 82: loss=12.77706241607666\n",
      "Epoch 83: loss=12.696346282958984\n",
      "Epoch 84: loss=12.588312149047852\n",
      "Epoch 85: loss=12.515447616577148\n",
      "Epoch 86: loss=12.426271438598633\n",
      "Epoch 87: loss=12.372831344604492\n",
      "Epoch 88: loss=12.30498218536377\n",
      "Epoch 89: loss=12.214488983154297\n",
      "Epoch 90: loss=12.112728118896484\n",
      "Epoch 91: loss=12.005380630493164\n",
      "Epoch 92: loss=11.942048072814941\n",
      "Epoch 93: loss=11.788455963134766\n",
      "Epoch 94: loss=11.69173812866211\n",
      "Epoch 95: loss=11.584510803222656\n",
      "Epoch 96: loss=11.47330379486084\n",
      "Epoch 97: loss=11.385019302368164\n",
      "Epoch 98: loss=11.250876426696777\n",
      "Epoch 99: loss=11.135069847106934\n",
      "tensor([[0, 1, 1, 2, 2, 3, 3, 4],\n",
      "        [1, 0, 2, 1, 3, 2, 4, 3]])\n"
     ]
    }
   ],
   "source": [
    "#OLD\n",
    "# Define a simple GNN model\n",
    "class MyGNN(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(MyGNN, self).__init__()\n",
    "        \n",
    "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, hidden_dim)\n",
    "        self.conv3 = GCNConv(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.relu(self.conv2(x, edge_index))\n",
    "        x = self.conv3(x, edge_index)\n",
    "        return x\n",
    "\n",
    "# Define a simple training function for the GNN model\n",
    "def train(model, data, optimizer):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(data.x, data.edge_index)\n",
    "    loss = F.mse_loss(output, data.y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "# Define a simple testing function for the GNN model\n",
    "def test(model, data):\n",
    "    model.eval()\n",
    "    output = model(data.x, data.edge_index)\n",
    "    return output\n",
    "\n",
    "# Set up the input data\n",
    "x = torch.tensor([\n",
    "    [0,0,1,1,1,0,0,0],\n",
    "    [10,5,0,0,0,0,5,0],\n",
    "    [20,10,0,0,0,0,5,0],\n",
    "    [30,5,0,0,0,0,5,0],\n",
    "    [40,0,1,1,1,0,0,0]\n",
    "], dtype=torch.float)\n",
    "\n",
    "edge_index = torch.tensor([\n",
    "    [0,1], [1,0], [1,2], [2,1], [2,3], [3,2], [3,4], [4,3]\n",
    "], dtype=torch.long).t()\n",
    "\n",
    "# Set up the output data (nodal forces and moments)\n",
    "y = torch.tensor([\n",
    "    [0,0,0,0,0,10,20,0],\n",
    "    [0,0,0,0,0,5,0,0],\n",
    "    [0,0,0,0,0,5,0,0],\n",
    "    [0,0,0,0,0,5,0,0],\n",
    "    [0,0,0,0,0,10,20,0]\n",
    "], dtype=torch.float)\n",
    "\n",
    "# Create a Data object that encapsulates the input and output data\n",
    "data = Data(x=x, edge_index=edge_index, y=y)\n",
    "\n",
    "# Initialize the GNN model\n",
    "input_dim = x.shape[1]\n",
    "output_dim = y.shape[1]\n",
    "hidden_dim = 16\n",
    "model = MyGNN(input_dim, hidden_dim, output_dim)\n",
    "\n",
    "# Define the optimizer and the number of epochs for training\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "num_epochs = 100\n",
    "\n",
    "# Train the GNN model\n",
    "for epoch in range(num_epochs):\n",
    "    loss = train(model, data, optimizer)\n",
    "    print('Epoch {}: loss={}'.format(epoch, loss))\n",
    "\n",
    "# Test the GNN model on a new input example\n",
    "x_new = torch.tensor([\n",
    "    [0,0,1,1,1,0,0,0],\n",
    "    [10,5,0,0,0,0,5,0],\n",
    "    [20,10,0,0,0,0,5,0],\n",
    "    [30,5,0,0,0,0,5,0]\n",
    "])\n",
    "print(edge_index)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f155e3810749e3e5c86d0714d9f4acbbc66b2c24e666ad929ed58436b422284a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
