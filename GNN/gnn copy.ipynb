{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Run this before starting\n",
    "!python -c \"import torch; print(torch.__version__)\"\n",
    "!python -c \"import torch; print(torch.version.cuda)\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0+cpu\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "!python -c \"import torch; print(torch.__version__)\"\n",
    "!python -c \"import torch; print(torch.version.cuda)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([[3, 4], [2, 2], [4, 6]], [[1, 2], [2, 3], [1, 1]])\n",
      "([2, 3, 4], [1, 2, 3])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "\n",
    "values_list = []  # create an empty list to store the values for each row\n",
    "x = []\n",
    "z = []\n",
    "m = []\n",
    "l = []\n",
    "\n",
    "with open('data.txt', \"r\") as f:\n",
    "    for line in f:  # loop over each line in the file\n",
    "        values = line.strip().split(\"\\t\")  # split the line into a list of strings\n",
    "\n",
    "        xL = []\n",
    "        zL = []\n",
    "        mL = []\n",
    "        lL = []\n",
    "\n",
    "        for i in range(20):\n",
    "            xL.append(round(float(values[i]),4))\n",
    "            zL.append(round(float(values[i+20]),4))\n",
    "            mL.append(round(float(values[i+20*2]),4))\n",
    "            try:\n",
    "                lL.append(round(float(values[i+20*3]),4))\n",
    "            except:\n",
    "                None\n",
    "        x.append(xL)\n",
    "        z.append(zL)\n",
    "        m.append(mL)\n",
    "        l.append(lL)\n",
    "\n",
    "\n",
    "dataY = []\n",
    "\n",
    "dataX = []\n",
    "for i in range(len(x)):\n",
    "    dataXs = []\n",
    "    for j in range(0,len(x[i])):\n",
    "        dataX0 = []\n",
    "        dataX0.append(j)\n",
    "        dataX0.append(x[i][j])\n",
    "        dataX0.append(z[i][j])\n",
    "        dataXs.append(dataX0)\n",
    "    dataX.append(dataXs)\n",
    "\n",
    "dataX = np.array(dataX)\n",
    "dataY = np.array(m)\n",
    "\n",
    "dataEgdeIndex = []\n",
    "for i in range(len(x[0])-1):\n",
    "    dataEgdeIndex.append([i,i+1])\n",
    "    dataEgdeIndex.append([i+1,i])  \n",
    "\n",
    "\n",
    "dataEgdeIndex = np.array(dataEgdeIndex) \n",
    "#dataEgdeIndex =np.transpose(dataEgdeIndex)\n",
    "\n",
    "\n",
    "input_data = dataX\n",
    "target_data = dataY\n",
    "edge_index = dataEgdeIndex\n",
    "\n",
    "#zipped = list(zip(dataX, dataY))\n",
    "#random.shuffle(zipped)\n",
    "#dataX, dataY = zip(*zipped)\n",
    "\n",
    "l1 = [[[1,2],[2,3],[1,1]],[[3,4],[2,2],[4,6]]]\n",
    "l2 = [[1,2,3],[2,3,4]]\n",
    "\n",
    "zipped = list(zip(l1, l2))\n",
    "random.shuffle(zipped)\n",
    "l1, l2 = zip(*zipped)\n",
    "\n",
    "print(l1)\n",
    "print(l2)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0.       0.       0.    ]\n",
      " [  1.       6.2105   0.    ]\n",
      " [  2.      12.4211   0.    ]\n",
      " [  3.      18.6316   0.    ]\n",
      " [  4.      24.8421   0.    ]\n",
      " [  5.      31.0526   0.    ]\n",
      " [  6.      37.2632   0.    ]\n",
      " [  7.      43.4737   0.    ]\n",
      " [  8.      49.6842   0.    ]\n",
      " [  9.      55.8947   0.    ]\n",
      " [ 10.      62.1053   0.    ]\n",
      " [ 11.      68.3158   0.    ]\n",
      " [ 12.      74.5263   0.    ]\n",
      " [ 13.      80.7368   0.    ]\n",
      " [ 14.      86.9474   0.    ]\n",
      " [ 15.      93.1579   0.    ]\n",
      " [ 16.      99.3684   0.    ]\n",
      " [ 17.     105.5789   0.    ]\n",
      " [ 18.     111.7895   0.    ]\n",
      " [ 19.     118.       0.    ]]\n"
     ]
    }
   ],
   "source": [
    "print(dataX[140])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn.conv import feast_conv\n",
    "from torch_geometric.loader import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Net.forward() takes 2 positional arguments but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[46], line 78\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[39m# Train the GNN model\u001b[39;00m\n\u001b[0;32m     77\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(dataX)):\n\u001b[1;32m---> 78\u001b[0m     loss \u001b[39m=\u001b[39m train(model, data[i], optimizer)\n\u001b[0;32m     79\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m: loss=\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(i, loss))\n",
      "Cell \u001b[1;32mIn[46], line 18\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, data, optimizer)\u001b[0m\n\u001b[0;32m     16\u001b[0m model\u001b[39m.\u001b[39mtrain()\n\u001b[0;32m     17\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m---> 18\u001b[0m output \u001b[39m=\u001b[39m model(data\u001b[39m.\u001b[39;49mx, data\u001b[39m.\u001b[39;49medge_index)\n\u001b[0;32m     19\u001b[0m loss \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mmse_loss(output, data\u001b[39m.\u001b[39my)\n\u001b[0;32m     20\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\eliasak\\OneDrive - NTNU\\Master thesis\\04_Elias\\ML1\\masters2023\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;31mTypeError\u001b[0m: Net.forward() takes 2 positional arguments but 3 were given"
     ]
    }
   ],
   "source": [
    "#Original Herman Code\n",
    "# Define a simple GNN model\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = GCNConv(3, 16)\n",
    "        self.conv2 = GCNConv(16, 1)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x.squeeze(dim=1)\n",
    "\n",
    "# Define a simple training function for the GNN model\n",
    "def train(model, data, optimizer):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(data.x, data.edge_index)\n",
    "    loss = F.mse_loss(output, data.y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "# Define a simple testing function for the GNN model\n",
    "def test(model, data):\n",
    "    model.eval()\n",
    "    output = model(data.x, data.edge_index)\n",
    "    return output\n",
    "\n",
    "\"\"\"\"\n",
    "# Set up the input data\n",
    "x = torch.tensor([\n",
    "    [0,0,1,1,1,0,0,0],\n",
    "    [10,5,0,0,0,0,5,0],\n",
    "    [20,10,0,0,0,0,5,0],\n",
    "    [30,5,0,0,0,0,5,0],\n",
    "    [40,0,1,1,1,0,0,0]\n",
    "], dtype=torch.float)\n",
    "\n",
    "edge_index = torch.tensor([\n",
    "    [0,1], [1,0], [1,2], [2,1], [2,3], [3,2], [3,4], [4,3]\n",
    "], dtype=torch.long).t()\n",
    "\n",
    "# Set up the output data (nodal forces and moments)\n",
    "y = torch.tensor([\n",
    "    [0,0,0,0,0,10,20,0],\n",
    "    [0,0,0,0,0,5,0,0],\n",
    "    [0,0,0,0,0,5,0,0],\n",
    "    [0,0,0,0,0,5,0,0],\n",
    "    [0,0,0,0,0,10,20,0]\n",
    "], dtype=torch.float)\n",
    "data = Data(x=x, edge_index=edge_index, y=y)\n",
    "\"\"\"\n",
    "\n",
    "# Create a Data object that encapsulates the input and output data\n",
    "\n",
    "data = []\n",
    "for i in range(len(dataX)):\n",
    "    x = torch.tensor(dataX[i])\n",
    "    y = torch.tensor(dataY[i])\n",
    "    edge_index = torch.tensor((dataEgdeIndex), dtype=torch.long).t()\n",
    "    data.append(Data(x=x, edge_index=edge_index, y=y))\n",
    "\n",
    "\n",
    "\n",
    "# Initialize the GNN model\n",
    "#input_dim = x.shape[1]\n",
    "#output_dim = y.shape[1]\n",
    "hidden_dim = 16\n",
    "model = Net()\n",
    "\n",
    "# Define the optimizer and the number of epochs for training\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "num_epochs = 100\n",
    "\n",
    "# Train the GNN model\n",
    "for i in range(len(dataX)):\n",
    "    loss = train(model, data[i], optimizer)\n",
    "    print('Epoch {}: loss={}'.format(i, loss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eliasak\\AppData\\Local\\Temp\\ipykernel_5252\\1883230339.py:58: UserWarning: Using a target size (torch.Size([20])) that is different to the input size (torch.Size([20, 20])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss = F.mse_loss(out, y)\n",
      "C:\\Users\\eliasak\\AppData\\Local\\Temp\\ipykernel_5252\\1883230339.py:67: UserWarning: Using a target size (torch.Size([20])) that is different to the input size (torch.Size([20, 20])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  val_loss = F.mse_loss(out, y)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Train Loss: 1.4504, Val Loss: 9.0506\n",
      "Epoch 1, Train Loss: 1.1552, Val Loss: 1.4317\n",
      "Epoch 2, Train Loss: 0.9875, Val Loss: 1.4353\n",
      "Epoch 3, Train Loss: 6.1541, Val Loss: 6.6199\n",
      "Epoch 4, Train Loss: 2.5247, Val Loss: 2.9006\n",
      "Epoch 5, Train Loss: 1.8007, Val Loss: 3.1473\n",
      "Epoch 6, Train Loss: 1.1705, Val Loss: 0.9762\n",
      "Epoch 7, Train Loss: 5.5007, Val Loss: 4.9300\n",
      "Epoch 8, Train Loss: 7.8584, Val Loss: 7.7447\n",
      "Epoch 9, Train Loss: 4.7236, Val Loss: 5.5922\n",
      "Epoch 10, Train Loss: 1.5297, Val Loss: 1.0068\n",
      "Epoch 11, Train Loss: 1.1551, Val Loss: 0.9874\n",
      "Epoch 12, Train Loss: 2.1427, Val Loss: 2.0653\n",
      "Epoch 13, Train Loss: 55.2495, Val Loss: 54.5084\n",
      "Epoch 14, Train Loss: 2.4372, Val Loss: 2.1260\n",
      "Epoch 15, Train Loss: 1.9539, Val Loss: 1.8453\n",
      "Epoch 16, Train Loss: 35.7614, Val Loss: 34.8595\n",
      "Epoch 17, Train Loss: 2.5678, Val Loss: 2.4047\n",
      "Epoch 18, Train Loss: 21.7806, Val Loss: 21.2561\n",
      "Epoch 19, Train Loss: 1.2228, Val Loss: 1.0389\n",
      "Epoch 20, Train Loss: 1.1808, Val Loss: 0.9470\n",
      "Epoch 21, Train Loss: 6.4435, Val Loss: 6.3704\n",
      "Epoch 22, Train Loss: 5.0556, Val Loss: 4.9264\n",
      "Epoch 23, Train Loss: 5.7998, Val Loss: 5.3369\n",
      "Epoch 24, Train Loss: 0.9415, Val Loss: 0.7783\n",
      "Epoch 25, Train Loss: 1.1712, Val Loss: 0.9349\n",
      "Epoch 26, Train Loss: 0.6016, Val Loss: 0.5957\n",
      "Epoch 27, Train Loss: 3.8208, Val Loss: 3.6321\n",
      "Epoch 28, Train Loss: 1.0153, Val Loss: 0.9020\n",
      "Epoch 29, Train Loss: 1.1133, Val Loss: 1.0808\n",
      "Epoch 30, Train Loss: 2.4174, Val Loss: 2.3576\n",
      "Epoch 31, Train Loss: 1.0040, Val Loss: 1.0233\n",
      "Epoch 32, Train Loss: 1.1396, Val Loss: 1.0809\n",
      "Epoch 33, Train Loss: 1.4901, Val Loss: 1.0200\n",
      "Epoch 34, Train Loss: 1.0866, Val Loss: 1.0210\n",
      "Epoch 35, Train Loss: 1.2274, Val Loss: 1.0061\n",
      "Epoch 36, Train Loss: 1.0068, Val Loss: 0.9707\n",
      "Epoch 37, Train Loss: 5.7733, Val Loss: 5.6212\n",
      "Epoch 38, Train Loss: 1.0861, Val Loss: 1.0314\n",
      "Epoch 39, Train Loss: 73.1181, Val Loss: 72.1681\n",
      "Epoch 40, Train Loss: 0.8611, Val Loss: 0.8479\n",
      "Epoch 41, Train Loss: 1.0707, Val Loss: 0.9934\n",
      "Epoch 42, Train Loss: 5.0798, Val Loss: 5.0158\n",
      "Epoch 43, Train Loss: 1.7926, Val Loss: 1.7445\n",
      "Epoch 44, Train Loss: 1.8008, Val Loss: 1.7751\n",
      "Epoch 45, Train Loss: 4.1575, Val Loss: 4.0477\n",
      "Epoch 46, Train Loss: 2.2972, Val Loss: 2.1662\n",
      "Epoch 47, Train Loss: 6.2803, Val Loss: 6.2731\n",
      "Epoch 48, Train Loss: 2.3549, Val Loss: 2.2624\n",
      "Epoch 49, Train Loss: 0.8202, Val Loss: 0.6421\n",
      "Epoch 50, Train Loss: 1.0462, Val Loss: 0.9617\n",
      "Epoch 51, Train Loss: 1.1218, Val Loss: 0.9379\n",
      "Epoch 52, Train Loss: 3.1334, Val Loss: 2.6202\n",
      "Epoch 53, Train Loss: 0.9131, Val Loss: 0.8585\n",
      "Epoch 54, Train Loss: 7.0856, Val Loss: 6.9865\n",
      "Epoch 55, Train Loss: 5.9662, Val Loss: 5.8370\n",
      "Epoch 56, Train Loss: 3.0163, Val Loss: 2.7708\n",
      "Epoch 57, Train Loss: 5.3996, Val Loss: 5.2583\n",
      "Epoch 58, Train Loss: 3.0032, Val Loss: 2.9247\n",
      "Epoch 59, Train Loss: 2.8695, Val Loss: 2.7497\n",
      "Epoch 60, Train Loss: 1.0295, Val Loss: 0.9419\n",
      "Epoch 61, Train Loss: 1.1978, Val Loss: 0.9646\n",
      "Epoch 62, Train Loss: 6.6442, Val Loss: 6.4889\n",
      "Epoch 63, Train Loss: 1.0444, Val Loss: 0.9088\n",
      "Epoch 64, Train Loss: 0.7741, Val Loss: 0.5998\n",
      "Epoch 65, Train Loss: 1.1477, Val Loss: 0.9675\n",
      "Epoch 66, Train Loss: 39.9320, Val Loss: 39.6872\n",
      "Epoch 67, Train Loss: 1.1884, Val Loss: 0.7422\n",
      "Epoch 68, Train Loss: 0.8886, Val Loss: 0.7549\n",
      "Epoch 69, Train Loss: 6.4849, Val Loss: 6.3416\n",
      "Epoch 70, Train Loss: 1.0763, Val Loss: 0.9926\n",
      "Epoch 71, Train Loss: 5.0780, Val Loss: 5.0008\n",
      "Epoch 72, Train Loss: 6.1982, Val Loss: 5.9580\n",
      "Epoch 73, Train Loss: 0.9749, Val Loss: 0.9672\n",
      "Epoch 74, Train Loss: 0.6626, Val Loss: 0.5885\n",
      "Epoch 75, Train Loss: 1.0732, Val Loss: 0.8887\n",
      "Epoch 76, Train Loss: 1.0723, Val Loss: 0.9332\n",
      "Epoch 77, Train Loss: 0.8108, Val Loss: 0.7424\n",
      "Epoch 78, Train Loss: 0.8361, Val Loss: 0.6473\n",
      "Epoch 79, Train Loss: 6.8699, Val Loss: 6.7358\n",
      "Epoch 80, Train Loss: 0.7215, Val Loss: 0.5517\n",
      "Epoch 81, Train Loss: 2.7399, Val Loss: 2.6325\n",
      "Epoch 82, Train Loss: 5.0272, Val Loss: 4.9889\n",
      "Epoch 83, Train Loss: 6.0090, Val Loss: 5.9543\n",
      "Epoch 84, Train Loss: 5.6133, Val Loss: 5.5156\n",
      "Epoch 85, Train Loss: 7.0768, Val Loss: 6.8370\n",
      "Epoch 86, Train Loss: 0.6596, Val Loss: 0.5307\n",
      "Epoch 87, Train Loss: 0.9989, Val Loss: 0.8884\n",
      "Epoch 88, Train Loss: 1.3019, Val Loss: 1.2301\n",
      "Epoch 89, Train Loss: 2.4214, Val Loss: 2.2586\n",
      "Epoch 90, Train Loss: 1.7277, Val Loss: 1.4891\n",
      "Epoch 91, Train Loss: 1.6200, Val Loss: 1.6543\n",
      "Epoch 92, Train Loss: 16.9251, Val Loss: 16.7262\n",
      "Epoch 93, Train Loss: 6.5634, Val Loss: 6.5110\n",
      "Epoch 94, Train Loss: 2.7930, Val Loss: 2.6358\n",
      "Epoch 95, Train Loss: 1.1653, Val Loss: 1.1252\n",
      "Epoch 96, Train Loss: 0.6493, Val Loss: 0.6302\n",
      "Epoch 97, Train Loss: 5.0063, Val Loss: 4.7857\n",
      "Epoch 98, Train Loss: 6.6350, Val Loss: 6.2893\n",
      "Epoch 99, Train Loss: 0.6396, Val Loss: 0.6121\n",
      "Epoch 100, Train Loss: 0.7430, Val Loss: 0.6206\n",
      "Epoch 101, Train Loss: 5.4594, Val Loss: 5.3612\n",
      "Epoch 102, Train Loss: 0.7133, Val Loss: 0.6736\n",
      "Epoch 103, Train Loss: 1.2007, Val Loss: 0.8832\n",
      "Epoch 104, Train Loss: 2.1753, Val Loss: 2.1099\n",
      "Epoch 105, Train Loss: 1.2807, Val Loss: 0.9864\n",
      "Epoch 106, Train Loss: 1.6797, Val Loss: 1.6201\n",
      "Epoch 107, Train Loss: 1.1020, Val Loss: 0.9360\n",
      "Epoch 108, Train Loss: 3.1763, Val Loss: 3.1326\n",
      "Epoch 109, Train Loss: 32.4434, Val Loss: 32.3422\n",
      "Epoch 110, Train Loss: 0.9892, Val Loss: 0.8452\n",
      "Epoch 111, Train Loss: 1.1357, Val Loss: 0.9825\n",
      "Epoch 112, Train Loss: 0.7279, Val Loss: 0.6617\n",
      "Epoch 113, Train Loss: 4.7600, Val Loss: 4.6463\n",
      "Epoch 114, Train Loss: 18.0491, Val Loss: 18.0171\n",
      "Epoch 115, Train Loss: 28.1008, Val Loss: 28.0910\n",
      "Epoch 116, Train Loss: 1.0436, Val Loss: 0.9330\n",
      "Epoch 117, Train Loss: 26.1996, Val Loss: 26.1691\n",
      "Epoch 118, Train Loss: 4.3977, Val Loss: 4.3164\n",
      "Epoch 119, Train Loss: 1.3735, Val Loss: 1.2860\n",
      "Epoch 120, Train Loss: 4.3789, Val Loss: 4.2992\n",
      "Epoch 121, Train Loss: 67.9057, Val Loss: 67.8208\n",
      "Epoch 122, Train Loss: 0.5792, Val Loss: 0.5338\n",
      "Epoch 123, Train Loss: 5.5584, Val Loss: 5.6517\n",
      "Epoch 124, Train Loss: 1.0323, Val Loss: 0.9102\n",
      "Epoch 125, Train Loss: 0.9930, Val Loss: 0.9060\n",
      "Epoch 126, Train Loss: 2.4771, Val Loss: 2.3714\n",
      "Epoch 127, Train Loss: 0.5708, Val Loss: 0.5578\n",
      "Epoch 128, Train Loss: 0.8886, Val Loss: 0.8812\n",
      "Epoch 129, Train Loss: 0.8275, Val Loss: 0.7684\n",
      "Epoch 130, Train Loss: 0.9277, Val Loss: 0.9101\n",
      "Epoch 131, Train Loss: 0.9593, Val Loss: 0.7990\n",
      "Epoch 132, Train Loss: 0.9472, Val Loss: 0.9329\n",
      "Epoch 133, Train Loss: 5.6173, Val Loss: 5.5443\n",
      "Epoch 134, Train Loss: 3.3156, Val Loss: 3.2383\n",
      "Epoch 135, Train Loss: 75.9192, Val Loss: 75.7699\n",
      "Epoch 136, Train Loss: 5.1555, Val Loss: 4.9789\n",
      "Epoch 137, Train Loss: 4.1888, Val Loss: 4.1114\n",
      "Epoch 138, Train Loss: 6.1803, Val Loss: 6.0456\n",
      "Epoch 139, Train Loss: 1.9812, Val Loss: 1.9286\n",
      "Epoch 140, Train Loss: 0.5478, Val Loss: 0.5419\n",
      "Epoch 141, Train Loss: 3.2577, Val Loss: 3.0454\n",
      "Epoch 142, Train Loss: 0.8604, Val Loss: 0.9348\n",
      "Epoch 143, Train Loss: 1.6396, Val Loss: 1.5044\n",
      "Epoch 144, Train Loss: 80.1083, Val Loss: 79.8682\n",
      "Epoch 145, Train Loss: 7.2746, Val Loss: 7.2053\n",
      "Epoch 146, Train Loss: 0.8497, Val Loss: 0.6211\n",
      "Epoch 147, Train Loss: 2.9405, Val Loss: 2.8487\n",
      "Epoch 148, Train Loss: 4.1549, Val Loss: 4.0933\n",
      "Epoch 149, Train Loss: 0.9632, Val Loss: 0.8744\n",
      "Epoch 150, Train Loss: 1.5744, Val Loss: 1.3280\n",
      "Epoch 151, Train Loss: 0.7876, Val Loss: 0.8335\n",
      "Epoch 152, Train Loss: 0.7292, Val Loss: 0.6522\n",
      "Epoch 153, Train Loss: 63.7218, Val Loss: 63.6735\n",
      "Epoch 154, Train Loss: 0.6979, Val Loss: 0.6504\n",
      "Epoch 155, Train Loss: 1.4382, Val Loss: 1.4287\n",
      "Epoch 156, Train Loss: 4.5099, Val Loss: 4.4306\n",
      "Epoch 157, Train Loss: 0.8512, Val Loss: 0.7484\n",
      "Epoch 158, Train Loss: 0.8754, Val Loss: 0.8246\n",
      "Epoch 159, Train Loss: 0.8553, Val Loss: 0.8096\n",
      "Epoch 160, Train Loss: 6.0103, Val Loss: 5.8677\n",
      "Epoch 161, Train Loss: 44.4795, Val Loss: 44.3498\n",
      "Epoch 162, Train Loss: 0.9193, Val Loss: 0.8211\n",
      "Epoch 163, Train Loss: 1.5526, Val Loss: 1.5369\n",
      "Epoch 164, Train Loss: 5.6885, Val Loss: 5.6061\n",
      "Epoch 165, Train Loss: 3.7473, Val Loss: 3.7199\n",
      "Epoch 166, Train Loss: 0.9130, Val Loss: 0.7952\n",
      "Epoch 167, Train Loss: 0.6208, Val Loss: 0.5326\n",
      "Epoch 168, Train Loss: 60.1426, Val Loss: 60.0495\n",
      "Epoch 169, Train Loss: 1.0741, Val Loss: 0.9067\n",
      "Epoch 170, Train Loss: 4.1376, Val Loss: 4.0878\n",
      "Epoch 171, Train Loss: 0.9752, Val Loss: 0.8629\n",
      "Epoch 172, Train Loss: 1.8744, Val Loss: 1.8669\n",
      "Epoch 173, Train Loss: 4.3860, Val Loss: 4.3388\n",
      "Epoch 174, Train Loss: 0.9599, Val Loss: 0.8216\n",
      "Epoch 175, Train Loss: 6.7280, Val Loss: 6.7086\n",
      "Epoch 176, Train Loss: 24.0603, Val Loss: 24.0413\n",
      "Epoch 177, Train Loss: 0.5576, Val Loss: 0.4738\n",
      "Epoch 178, Train Loss: 0.8729, Val Loss: 0.7787\n",
      "Epoch 179, Train Loss: 3.6727, Val Loss: 3.6097\n",
      "Epoch 180, Train Loss: 0.6615, Val Loss: 0.6304\n",
      "Epoch 181, Train Loss: 6.0058, Val Loss: 5.9565\n",
      "Epoch 182, Train Loss: 1.0337, Val Loss: 0.9895\n",
      "Epoch 183, Train Loss: 0.9642, Val Loss: 0.8425\n",
      "Epoch 184, Train Loss: 5.5884, Val Loss: 5.4190\n",
      "Epoch 185, Train Loss: 1.7648, Val Loss: 1.6925\n",
      "Epoch 186, Train Loss: 1.0360, Val Loss: 0.8191\n",
      "Epoch 187, Train Loss: 0.8402, Val Loss: 0.8319\n",
      "Epoch 188, Train Loss: 0.8235, Val Loss: 0.7783\n",
      "Epoch 189, Train Loss: 0.9685, Val Loss: 0.8897\n",
      "Epoch 190, Train Loss: 0.9564, Val Loss: 0.9263\n",
      "Epoch 191, Train Loss: 4.5289, Val Loss: 4.4514\n",
      "Epoch 192, Train Loss: 0.8732, Val Loss: 0.7945\n",
      "Epoch 193, Train Loss: 0.9767, Val Loss: 0.8488\n",
      "Epoch 194, Train Loss: 2.1517, Val Loss: 2.0957\n",
      "Epoch 195, Train Loss: 0.9505, Val Loss: 0.7846\n",
      "Epoch 196, Train Loss: 1.8428, Val Loss: 1.7817\n",
      "Epoch 197, Train Loss: 4.1824, Val Loss: 4.0522\n",
      "Epoch 198, Train Loss: 0.8506, Val Loss: 0.8147\n",
      "Epoch 199, Train Loss: 2.6199, Val Loss: 2.5818\n",
      "Epoch 200, Train Loss: 36.5581, Val Loss: 36.3580\n",
      "Epoch 201, Train Loss: 0.8538, Val Loss: 0.7855\n",
      "Epoch 202, Train Loss: 6.4434, Val Loss: 6.2795\n",
      "Epoch 203, Train Loss: 1.1701, Val Loss: 1.0861\n",
      "Epoch 204, Train Loss: 0.6013, Val Loss: 0.5577\n",
      "Epoch 205, Train Loss: 2.3485, Val Loss: 2.3225\n",
      "Epoch 206, Train Loss: 0.8823, Val Loss: 0.8431\n",
      "Epoch 207, Train Loss: 6.4927, Val Loss: 6.3909\n",
      "Epoch 208, Train Loss: 4.6152, Val Loss: 4.5426\n",
      "Epoch 209, Train Loss: 1.8356, Val Loss: 1.7692\n",
      "Epoch 210, Train Loss: 0.8500, Val Loss: 0.7892\n",
      "Epoch 211, Train Loss: 5.0820, Val Loss: 4.9828\n",
      "Epoch 212, Train Loss: 0.8265, Val Loss: 0.7707\n",
      "Epoch 213, Train Loss: 5.8733, Val Loss: 5.7748\n",
      "Epoch 214, Train Loss: 1.7042, Val Loss: 1.6546\n",
      "Epoch 215, Train Loss: 6.9909, Val Loss: 6.8421\n",
      "Epoch 216, Train Loss: 0.6141, Val Loss: 0.5580\n",
      "Epoch 217, Train Loss: 0.7011, Val Loss: 0.5792\n",
      "Epoch 218, Train Loss: 1.5285, Val Loss: 1.3577\n",
      "Epoch 219, Train Loss: 1.9458, Val Loss: 1.8765\n",
      "Epoch 220, Train Loss: 29.7894, Val Loss: 29.5781\n",
      "Epoch 221, Train Loss: 0.8396, Val Loss: 0.7778\n",
      "Epoch 222, Train Loss: 6.4516, Val Loss: 6.3241\n",
      "Epoch 223, Train Loss: 1.0040, Val Loss: 0.9817\n",
      "Epoch 224, Train Loss: 2.5920, Val Loss: 2.5528\n",
      "Epoch 225, Train Loss: 56.4796, Val Loss: 56.3164\n",
      "Epoch 226, Train Loss: 6.5283, Val Loss: 6.4736\n",
      "Epoch 227, Train Loss: 6.8270, Val Loss: 6.7016\n",
      "Epoch 228, Train Loss: 1.1194, Val Loss: 1.0244\n",
      "Epoch 229, Train Loss: 5.6959, Val Loss: 5.6567\n",
      "Epoch 230, Train Loss: 1.1729, Val Loss: 1.1417\n",
      "Epoch 231, Train Loss: 1.0327, Val Loss: 0.9097\n",
      "Epoch 232, Train Loss: 0.5743, Val Loss: 0.4913\n",
      "Epoch 233, Train Loss: 1.1419, Val Loss: 1.0771\n",
      "Epoch 234, Train Loss: 2.4039, Val Loss: 2.2937\n",
      "Epoch 235, Train Loss: 0.5457, Val Loss: 0.4683\n",
      "Epoch 236, Train Loss: 1.3696, Val Loss: 1.3385\n",
      "Epoch 237, Train Loss: 0.8590, Val Loss: 0.8126\n",
      "Epoch 238, Train Loss: 0.8917, Val Loss: 0.7657\n",
      "Epoch 239, Train Loss: 1.2131, Val Loss: 1.2008\n",
      "Epoch 240, Train Loss: 4.8366, Val Loss: 4.7969\n",
      "Epoch 241, Train Loss: 22.2215, Val Loss: 22.1094\n",
      "Epoch 242, Train Loss: 1.4475, Val Loss: 1.4244\n",
      "Epoch 243, Train Loss: 6.1376, Val Loss: 6.0977\n",
      "Epoch 244, Train Loss: 1.5239, Val Loss: 1.4893\n",
      "Epoch 245, Train Loss: 7.1597, Val Loss: 7.1066\n",
      "Epoch 246, Train Loss: 0.7028, Val Loss: 0.6669\n",
      "Epoch 247, Train Loss: 7.1110, Val Loss: 7.1023\n",
      "Epoch 248, Train Loss: 3.4538, Val Loss: 3.3374\n",
      "Epoch 249, Train Loss: 2.3039, Val Loss: 2.1732\n",
      "Epoch 250, Train Loss: 6.7261, Val Loss: 6.6717\n",
      "Epoch 251, Train Loss: 1.3421, Val Loss: 1.3294\n",
      "Epoch 252, Train Loss: 1.5307, Val Loss: 1.5212\n",
      "Epoch 253, Train Loss: 0.8193, Val Loss: 0.7932\n",
      "Epoch 254, Train Loss: 1.7371, Val Loss: 1.6849\n",
      "Epoch 255, Train Loss: 6.9333, Val Loss: 6.8670\n",
      "Epoch 256, Train Loss: 0.7643, Val Loss: 0.7481\n",
      "Epoch 257, Train Loss: 5.0632, Val Loss: 5.0197\n",
      "Epoch 258, Train Loss: 0.8045, Val Loss: 0.7502\n",
      "Epoch 259, Train Loss: 5.6795, Val Loss: 5.6540\n",
      "Epoch 260, Train Loss: 46.9123, Val Loss: 46.7868\n",
      "Epoch 261, Train Loss: 7.0633, Val Loss: 6.9651\n",
      "Epoch 262, Train Loss: 1.9537, Val Loss: 1.9226\n",
      "Epoch 263, Train Loss: 6.9233, Val Loss: 6.8568\n",
      "Epoch 264, Train Loss: 5.0609, Val Loss: 5.0281\n",
      "Epoch 265, Train Loss: 1.8859, Val Loss: 1.8872\n",
      "Epoch 266, Train Loss: 0.7937, Val Loss: 0.7184\n",
      "Epoch 267, Train Loss: 6.2470, Val Loss: 6.1563\n",
      "Epoch 268, Train Loss: 4.8374, Val Loss: 4.7255\n",
      "Epoch 269, Train Loss: 0.7429, Val Loss: 0.7561\n",
      "Epoch 270, Train Loss: 2.4981, Val Loss: 2.4640\n",
      "Epoch 271, Train Loss: 50.0294, Val Loss: 49.6697\n",
      "Epoch 272, Train Loss: 83.1949, Val Loss: 83.4113\n",
      "Epoch 273, Train Loss: 0.6684, Val Loss: 0.6093\n",
      "Epoch 274, Train Loss: 3.2036, Val Loss: 3.1822\n",
      "Epoch 275, Train Loss: 4.1931, Val Loss: 4.1657\n",
      "Epoch 276, Train Loss: 2.2936, Val Loss: 2.2507\n",
      "Epoch 277, Train Loss: 41.1579, Val Loss: 41.0549\n",
      "Epoch 278, Train Loss: 0.7729, Val Loss: 0.7332\n",
      "Epoch 279, Train Loss: 0.8294, Val Loss: 0.7381\n",
      "Epoch 280, Train Loss: 0.6880, Val Loss: 0.6501\n",
      "Epoch 281, Train Loss: 0.7756, Val Loss: 0.7366\n",
      "Epoch 282, Train Loss: 3.7470, Val Loss: 3.7850\n",
      "Epoch 283, Train Loss: 0.6168, Val Loss: 0.5390\n",
      "Epoch 284, Train Loss: 18.9173, Val Loss: 18.7761\n",
      "Epoch 285, Train Loss: 0.8124, Val Loss: 0.7344\n"
     ]
    }
   ],
   "source": [
    "#attemt 2\n",
    "\n",
    "#define modell\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, input_channels, hidden_channels, output_channels, num_layers):\n",
    "        super(GCN, self).__init__()\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        self.bns = torch.nn.ModuleList()\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # input layer\n",
    "        self.convs.append(GCNConv(input_channels, hidden_channels))\n",
    "        self.bns.append(torch.nn.BatchNorm1d(hidden_channels))\n",
    "\n",
    "        # hidden layers\n",
    "        for i in range(num_layers - 2):\n",
    "            self.convs.append(GCNConv(hidden_channels, hidden_channels))\n",
    "            self.bns.append(torch.nn.BatchNorm1d(hidden_channels))\n",
    "\n",
    "        # output layer\n",
    "        self.convs.append(GCNConv(hidden_channels, output_channels))\n",
    "        self.prediction_layer = torch.nn.Linear(output_channels, num_nodes)\n",
    "\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        for i in range(self.num_layers - 1):\n",
    "            x = self.convs[i](x, edge_index)\n",
    "            x = self.bns[i](x)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, p=0.5, training=self.training)\n",
    "        # output layer  \n",
    "        x = self.convs[-1](x, edge_index)\n",
    "        x = self.prediction_layer(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "input_channels = 3  # number of features for each node\n",
    "hidden_channels = 64\n",
    "output_channels = 1  # number of output features for each node\n",
    "num_layers = 5  # number of graph convolutional layers in the network\n",
    "num_nodes = len(dataX[i])\n",
    "\n",
    "model = GCN(input_channels, hidden_channels, output_channels, num_layers)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = len(dataX)\n",
    "for i in range(num_epochs):\n",
    "    x = torch.tensor(dataX[i], dtype=torch.float)\n",
    "    y = torch.tensor(dataY[i], dtype=torch.float)\n",
    "    edge_index = torch.tensor(dataEgdeIndex, dtype=torch.long).t().contiguous()\n",
    "\n",
    "    # set model to training mode\n",
    "    model.train()\n",
    "    #print(model)\n",
    "    optimizer.zero_grad()\n",
    "    out = model(x, edge_index)\n",
    "    #print(out)\n",
    "    loss = F.mse_loss(out, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # set model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        out = model(x, edge_index)\n",
    "        val_loss = F.mse_loss(out, y)\n",
    "\n",
    "    print(f'Epoch {i}, Train Loss: {loss.item():.4f}, Val Loss: {val_loss.item():.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eliasak\\AppData\\Local\\Temp\\ipykernel_27440\\2992925218.py:52: UserWarning: Using a target size (torch.Size([20])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss = F.mse_loss(out, y)\n",
      "C:\\Users\\eliasak\\AppData\\Local\\Temp\\ipykernel_27440\\2992925218.py:61: UserWarning: Using a target size (torch.Size([20])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  val_loss = F.mse_loss(out, y)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Train Loss: 1.3782, Val Loss: 2.9060\n",
      "Epoch 1, Train Loss: 0.2650, Val Loss: 0.4255\n",
      "Epoch 2, Train Loss: 7.3466, Val Loss: 8.2165\n",
      "Epoch 3, Train Loss: 2.4732, Val Loss: 4.3371\n",
      "Epoch 4, Train Loss: 0.7961, Val Loss: 1.8438\n",
      "Epoch 5, Train Loss: 0.8936, Val Loss: 4.7732\n",
      "Epoch 6, Train Loss: 0.2893, Val Loss: 1.8559\n",
      "Epoch 7, Train Loss: 6.3535, Val Loss: 8.0194\n",
      "Epoch 8, Train Loss: 0.5843, Val Loss: 4.6098\n",
      "Epoch 9, Train Loss: 0.7314, Val Loss: 3.7344\n",
      "Epoch 10, Train Loss: 21.7123, Val Loss: 21.5878\n",
      "Epoch 11, Train Loss: 0.7372, Val Loss: 4.0146\n",
      "Epoch 12, Train Loss: 0.3344, Val Loss: 3.1113\n",
      "Epoch 13, Train Loss: 3.1212, Val Loss: 12.2694\n",
      "Epoch 14, Train Loss: 2.6678, Val Loss: 8.3494\n",
      "Epoch 15, Train Loss: 1.0060, Val Loss: 4.0696\n",
      "Epoch 16, Train Loss: 48.1068, Val Loss: 48.3546\n",
      "Epoch 17, Train Loss: 1.8044, Val Loss: 3.9336\n",
      "Epoch 18, Train Loss: 0.6775, Val Loss: 1.1006\n",
      "Epoch 19, Train Loss: 64.9093, Val Loss: 64.8838\n",
      "Epoch 20, Train Loss: 0.7408, Val Loss: 2.3524\n",
      "Epoch 21, Train Loss: 1.1733, Val Loss: 2.7398\n",
      "Epoch 22, Train Loss: 0.7722, Val Loss: 0.8254\n",
      "Epoch 23, Train Loss: 5.6156, Val Loss: 12.5299\n",
      "Epoch 24, Train Loss: 0.6781, Val Loss: 1.2326\n",
      "Epoch 25, Train Loss: 0.3823, Val Loss: 0.3201\n",
      "Epoch 26, Train Loss: 81.5643, Val Loss: 81.6777\n",
      "Epoch 27, Train Loss: 4.1852, Val Loss: 4.1594\n",
      "Epoch 28, Train Loss: 5.1123, Val Loss: 4.9160\n",
      "Epoch 29, Train Loss: 0.8400, Val Loss: 0.7909\n",
      "Epoch 30, Train Loss: 0.3188, Val Loss: 0.8475\n",
      "Epoch 31, Train Loss: 0.7804, Val Loss: 1.2024\n",
      "Epoch 32, Train Loss: 1.8128, Val Loss: 1.6757\n",
      "Epoch 33, Train Loss: 1.5430, Val Loss: 1.4269\n",
      "Epoch 34, Train Loss: 5.3257, Val Loss: 8.0744\n",
      "Epoch 35, Train Loss: 1.0377, Val Loss: 2.2179\n",
      "Epoch 36, Train Loss: 5.0349, Val Loss: 5.1970\n",
      "Epoch 37, Train Loss: 6.4423, Val Loss: 6.5472\n",
      "Epoch 38, Train Loss: 1.6824, Val Loss: 1.6764\n",
      "Epoch 39, Train Loss: 18.3276, Val Loss: 18.4155\n",
      "Epoch 40, Train Loss: 0.7423, Val Loss: 2.0357\n",
      "Epoch 41, Train Loss: 58.1343, Val Loss: 58.2627\n",
      "Epoch 42, Train Loss: 3.6900, Val Loss: 6.9852\n",
      "Epoch 43, Train Loss: 2.8521, Val Loss: 3.1113\n",
      "Epoch 44, Train Loss: 7.5222, Val Loss: 7.3977\n",
      "Epoch 45, Train Loss: 0.9093, Val Loss: 0.7866\n",
      "Epoch 46, Train Loss: 1.5514, Val Loss: 1.5541\n",
      "Epoch 47, Train Loss: 7.1852, Val Loss: 7.2630\n",
      "Epoch 48, Train Loss: 26.4985, Val Loss: 26.5320\n",
      "Epoch 49, Train Loss: 0.8230, Val Loss: 0.7417\n",
      "Epoch 50, Train Loss: 1.7093, Val Loss: 1.5048\n",
      "Epoch 51, Train Loss: 5.4773, Val Loss: 5.5616\n",
      "Epoch 52, Train Loss: 0.4700, Val Loss: 1.3905\n",
      "Epoch 53, Train Loss: 33.0478, Val Loss: 32.7367\n",
      "Epoch 54, Train Loss: 0.4938, Val Loss: 1.3366\n",
      "Epoch 55, Train Loss: 0.7736, Val Loss: 0.7750\n",
      "Epoch 56, Train Loss: 0.7843, Val Loss: 1.7349\n",
      "Epoch 57, Train Loss: 1.1132, Val Loss: 0.8533\n",
      "Epoch 58, Train Loss: 1.7032, Val Loss: 2.5404\n",
      "Epoch 59, Train Loss: 0.6477, Val Loss: 0.6709\n",
      "Epoch 60, Train Loss: 25.1953, Val Loss: 24.7972\n",
      "Epoch 61, Train Loss: 5.1760, Val Loss: 4.9011\n",
      "Epoch 62, Train Loss: 6.4592, Val Loss: 6.3008\n",
      "Epoch 63, Train Loss: 4.4299, Val Loss: 6.5760\n",
      "Epoch 64, Train Loss: 1.0172, Val Loss: 1.0699\n",
      "Epoch 65, Train Loss: 3.0586, Val Loss: 3.6205\n",
      "Epoch 66, Train Loss: 2.5251, Val Loss: 2.6423\n",
      "Epoch 67, Train Loss: 7.6125, Val Loss: 7.5143\n",
      "Epoch 68, Train Loss: 0.8580, Val Loss: 0.7622\n",
      "Epoch 69, Train Loss: 6.6622, Val Loss: 7.2451\n",
      "Epoch 70, Train Loss: 0.7781, Val Loss: 0.7147\n",
      "Epoch 71, Train Loss: 0.5377, Val Loss: 1.5764\n",
      "Epoch 72, Train Loss: 0.7467, Val Loss: 1.1168\n",
      "Epoch 73, Train Loss: 5.3687, Val Loss: 5.3834\n",
      "Epoch 74, Train Loss: 1.6377, Val Loss: 2.9067\n",
      "Epoch 75, Train Loss: 0.6559, Val Loss: 0.6221\n",
      "Epoch 76, Train Loss: 6.5730, Val Loss: 6.8854\n",
      "Epoch 77, Train Loss: 4.2259, Val Loss: 4.6629\n",
      "Epoch 78, Train Loss: 6.9829, Val Loss: 6.9645\n",
      "Epoch 79, Train Loss: 1.4853, Val Loss: 1.6653\n",
      "Epoch 80, Train Loss: 4.6986, Val Loss: 4.7152\n",
      "Epoch 81, Train Loss: 0.4453, Val Loss: 0.4919\n",
      "Epoch 82, Train Loss: 0.8125, Val Loss: 0.7669\n",
      "Epoch 83, Train Loss: 5.1204, Val Loss: 4.9726\n",
      "Epoch 84, Train Loss: 0.3439, Val Loss: 0.3455\n",
      "Epoch 85, Train Loss: 0.5291, Val Loss: 0.5364\n",
      "Epoch 86, Train Loss: 5.9492, Val Loss: 5.9998\n",
      "Epoch 87, Train Loss: 5.0682, Val Loss: 4.7498\n",
      "Epoch 88, Train Loss: 5.8952, Val Loss: 5.9286\n",
      "Epoch 89, Train Loss: 6.9288, Val Loss: 6.9286\n",
      "Epoch 90, Train Loss: 2.4718, Val Loss: 2.2777\n",
      "Epoch 91, Train Loss: 0.7845, Val Loss: 0.8725\n",
      "Epoch 92, Train Loss: 2.5289, Val Loss: 4.1154\n",
      "Epoch 93, Train Loss: 0.7519, Val Loss: 1.0609\n",
      "Epoch 94, Train Loss: 0.4823, Val Loss: 0.9255\n",
      "Epoch 95, Train Loss: 0.7500, Val Loss: 0.8556\n",
      "Epoch 96, Train Loss: 0.8455, Val Loss: 0.8188\n",
      "Epoch 97, Train Loss: 0.6435, Val Loss: 0.7948\n",
      "Epoch 98, Train Loss: 0.5660, Val Loss: 0.6860\n",
      "Epoch 99, Train Loss: 7.7743, Val Loss: 7.5472\n",
      "Epoch 100, Train Loss: 0.3074, Val Loss: 0.3175\n",
      "Epoch 101, Train Loss: 0.3949, Val Loss: 0.3938\n",
      "Epoch 102, Train Loss: 4.9843, Val Loss: 4.9876\n",
      "Epoch 103, Train Loss: 2.1635, Val Loss: 2.1198\n",
      "Epoch 104, Train Loss: 0.3157, Val Loss: 0.2935\n",
      "Epoch 105, Train Loss: 2.4240, Val Loss: 2.4601\n",
      "Epoch 106, Train Loss: 28.6153, Val Loss: 28.8408\n",
      "Epoch 107, Train Loss: 3.3917, Val Loss: 3.2515\n",
      "Epoch 108, Train Loss: 0.6453, Val Loss: 0.6655\n",
      "Epoch 109, Train Loss: 0.6818, Val Loss: 0.6638\n",
      "Epoch 110, Train Loss: 4.7091, Val Loss: 4.4952\n",
      "Epoch 111, Train Loss: 0.6303, Val Loss: 0.6083\n",
      "Epoch 112, Train Loss: 0.2832, Val Loss: 0.2589\n",
      "Epoch 113, Train Loss: 1.8374, Val Loss: 1.7432\n",
      "Epoch 114, Train Loss: 0.6307, Val Loss: 0.6635\n",
      "Epoch 115, Train Loss: 0.7739, Val Loss: 0.8699\n",
      "Epoch 116, Train Loss: 0.8953, Val Loss: 0.8926\n",
      "Epoch 117, Train Loss: 6.8051, Val Loss: 7.1689\n",
      "Epoch 118, Train Loss: 0.8022, Val Loss: 0.9984\n",
      "Epoch 119, Train Loss: 45.9431, Val Loss: 45.9937\n",
      "Epoch 120, Train Loss: 1.6056, Val Loss: 1.6060\n",
      "Epoch 121, Train Loss: 17.3103, Val Loss: 16.9697\n",
      "Epoch 122, Train Loss: 1.4038, Val Loss: 1.3837\n",
      "Epoch 123, Train Loss: 0.7103, Val Loss: 0.7118\n",
      "Epoch 124, Train Loss: 6.2023, Val Loss: 6.1870\n",
      "Epoch 125, Train Loss: 0.8431, Val Loss: 0.3890\n",
      "Epoch 126, Train Loss: 0.7231, Val Loss: 0.7618\n",
      "Epoch 127, Train Loss: 0.8810, Val Loss: 0.6963\n",
      "Epoch 128, Train Loss: 5.3738, Val Loss: 5.3418\n",
      "Epoch 129, Train Loss: 2.5962, Val Loss: 2.5088\n",
      "Epoch 130, Train Loss: 0.7251, Val Loss: 0.6836\n",
      "Epoch 131, Train Loss: 0.7878, Val Loss: 0.7890\n",
      "Epoch 132, Train Loss: 4.2158, Val Loss: 4.1770\n",
      "Epoch 133, Train Loss: 0.6116, Val Loss: 0.6144\n",
      "Epoch 134, Train Loss: 2.4937, Val Loss: 2.5016\n",
      "Epoch 135, Train Loss: 0.7459, Val Loss: 0.7236\n",
      "Epoch 136, Train Loss: 1.6263, Val Loss: 1.9240\n",
      "Epoch 137, Train Loss: 0.5208, Val Loss: 0.5888\n",
      "Epoch 138, Train Loss: 0.7598, Val Loss: 0.8477\n",
      "Epoch 139, Train Loss: 0.6876, Val Loss: 0.6073\n",
      "Epoch 140, Train Loss: 2.2487, Val Loss: 2.2886\n",
      "Epoch 141, Train Loss: 43.5999, Val Loss: 42.9915\n",
      "Epoch 142, Train Loss: 0.7303, Val Loss: 0.7517\n",
      "Epoch 143, Train Loss: 1.7787, Val Loss: 1.8045\n",
      "Epoch 144, Train Loss: 69.1100, Val Loss: 68.9752\n",
      "Epoch 145, Train Loss: 0.5596, Val Loss: 0.6107\n",
      "Epoch 146, Train Loss: 0.7375, Val Loss: 0.8048\n",
      "Epoch 147, Train Loss: 7.2883, Val Loss: 7.0933\n",
      "Epoch 148, Train Loss: 2.3082, Val Loss: 2.6167\n",
      "Epoch 149, Train Loss: 34.9188, Val Loss: 35.0182\n",
      "Epoch 150, Train Loss: 0.5945, Val Loss: 0.5991\n",
      "Epoch 151, Train Loss: 5.7379, Val Loss: 5.4053\n",
      "Epoch 152, Train Loss: 4.3083, Val Loss: 4.2832\n",
      "Epoch 153, Train Loss: 0.2956, Val Loss: 0.3151\n",
      "Epoch 154, Train Loss: 2.6912, Val Loss: 2.6882\n",
      "Epoch 155, Train Loss: 4.5412, Val Loss: 4.4896\n",
      "Epoch 156, Train Loss: 0.7765, Val Loss: 0.6468\n",
      "Epoch 157, Train Loss: 5.2699, Val Loss: 5.2922\n",
      "Epoch 158, Train Loss: 5.8712, Val Loss: 5.7183\n",
      "Epoch 159, Train Loss: 1.7786, Val Loss: 1.7799\n",
      "Epoch 160, Train Loss: 0.7165, Val Loss: 0.6770\n",
      "Epoch 161, Train Loss: 0.7827, Val Loss: 0.7813\n",
      "Epoch 162, Train Loss: 6.6325, Val Loss: 6.5940\n",
      "Epoch 163, Train Loss: 2.3417, Val Loss: 2.5931\n",
      "Epoch 164, Train Loss: 5.1947, Val Loss: 5.1849\n",
      "Epoch 165, Train Loss: 1.6867, Val Loss: 1.5889\n",
      "Epoch 166, Train Loss: 0.7646, Val Loss: 0.7550\n",
      "Epoch 167, Train Loss: 0.6320, Val Loss: 0.4408\n",
      "Epoch 168, Train Loss: 1.1552, Val Loss: 1.1283\n",
      "Epoch 169, Train Loss: 2.3403, Val Loss: 2.1437\n",
      "Epoch 170, Train Loss: 0.7367, Val Loss: 0.7203\n",
      "Epoch 171, Train Loss: 0.5854, Val Loss: 0.5544\n",
      "Epoch 172, Train Loss: 0.6348, Val Loss: 0.6390\n",
      "Epoch 173, Train Loss: 6.7168, Val Loss: 6.8277\n",
      "Epoch 174, Train Loss: 4.2454, Val Loss: 4.1852\n",
      "Epoch 175, Train Loss: 0.6598, Val Loss: 0.6690\n",
      "Epoch 176, Train Loss: 4.1813, Val Loss: 4.1316\n",
      "Epoch 177, Train Loss: 0.5436, Val Loss: 0.5555\n",
      "Epoch 178, Train Loss: 4.2306, Val Loss: 4.2157\n",
      "Epoch 179, Train Loss: 0.5483, Val Loss: 0.5258\n",
      "Epoch 180, Train Loss: 0.4519, Val Loss: 0.5045\n",
      "Epoch 181, Train Loss: 0.3405, Val Loss: 0.3469\n",
      "Epoch 182, Train Loss: 3.4795, Val Loss: 3.6962\n",
      "Epoch 183, Train Loss: 6.7962, Val Loss: 6.6819\n",
      "Epoch 184, Train Loss: 1.0376, Val Loss: 1.1135\n",
      "Epoch 185, Train Loss: 19.8190, Val Loss: 19.7632\n",
      "Epoch 186, Train Loss: 6.5670, Val Loss: 6.5555\n",
      "Epoch 187, Train Loss: 3.1281, Val Loss: 3.0594\n",
      "Epoch 188, Train Loss: 0.7672, Val Loss: 0.7430\n",
      "Epoch 189, Train Loss: 0.3773, Val Loss: 0.4158\n",
      "Epoch 190, Train Loss: 2.6608, Val Loss: 2.5379\n",
      "Epoch 191, Train Loss: 1.2443, Val Loss: 1.2488\n",
      "Epoch 192, Train Loss: 2.1332, Val Loss: 1.8911\n",
      "Epoch 193, Train Loss: 2.7059, Val Loss: 2.6916\n",
      "Epoch 194, Train Loss: 7.6658, Val Loss: 7.5639\n",
      "Epoch 195, Train Loss: 0.9375, Val Loss: 0.9841\n",
      "Epoch 196, Train Loss: 5.6022, Val Loss: 5.5806\n",
      "Epoch 197, Train Loss: 6.2718, Val Loss: 6.1359\n",
      "Epoch 198, Train Loss: 6.1385, Val Loss: 5.8227\n",
      "Epoch 199, Train Loss: 1.2709, Val Loss: 1.1684\n",
      "Epoch 200, Train Loss: 0.7353, Val Loss: 0.7346\n",
      "Epoch 201, Train Loss: 0.8279, Val Loss: 0.8462\n",
      "Epoch 202, Train Loss: 0.6296, Val Loss: 0.6560\n",
      "Epoch 203, Train Loss: 1.9478, Val Loss: 1.9497\n",
      "Epoch 204, Train Loss: 0.3687, Val Loss: 0.4106\n",
      "Epoch 205, Train Loss: 5.6121, Val Loss: 5.6164\n",
      "Epoch 206, Train Loss: 85.9292, Val Loss: 85.8557\n",
      "Epoch 207, Train Loss: 54.3694, Val Loss: 54.4952\n",
      "Epoch 208, Train Loss: 0.7649, Val Loss: 0.6087\n",
      "Epoch 209, Train Loss: 0.6228, Val Loss: 0.7606\n",
      "Epoch 210, Train Loss: 5.5051, Val Loss: 5.4905\n",
      "Epoch 211, Train Loss: 0.7596, Val Loss: 0.7784\n",
      "Epoch 212, Train Loss: 0.6660, Val Loss: 0.8539\n",
      "Epoch 213, Train Loss: 0.8257, Val Loss: 0.8579\n",
      "Epoch 214, Train Loss: 0.7358, Val Loss: 0.7631\n",
      "Epoch 215, Train Loss: 1.5914, Val Loss: 1.6992\n",
      "Epoch 216, Train Loss: 6.2458, Val Loss: 6.3696\n",
      "Epoch 217, Train Loss: 73.1875, Val Loss: 73.0410\n",
      "Epoch 218, Train Loss: 0.3900, Val Loss: 0.4239\n",
      "Epoch 219, Train Loss: 30.6623, Val Loss: 30.6840\n",
      "Epoch 220, Train Loss: 0.7666, Val Loss: 0.7768\n",
      "Epoch 221, Train Loss: 0.6773, Val Loss: 0.8015\n",
      "Epoch 222, Train Loss: 0.7928, Val Loss: 0.8942\n",
      "Epoch 223, Train Loss: 37.4451, Val Loss: 37.4295\n",
      "Epoch 224, Train Loss: 0.8012, Val Loss: 0.8216\n",
      "Epoch 225, Train Loss: 3.4609, Val Loss: 3.8062\n",
      "Epoch 226, Train Loss: 0.7050, Val Loss: 0.7424\n",
      "Epoch 227, Train Loss: 0.8389, Val Loss: 1.0465\n",
      "Epoch 228, Train Loss: 5.1607, Val Loss: 4.9565\n",
      "Epoch 229, Train Loss: 6.3433, Val Loss: 6.3056\n",
      "Epoch 230, Train Loss: 4.5255, Val Loss: 4.4743\n",
      "Epoch 231, Train Loss: 7.2189, Val Loss: 7.1047\n",
      "Epoch 232, Train Loss: 6.8905, Val Loss: 6.8659\n",
      "Epoch 233, Train Loss: 0.6713, Val Loss: 0.6715\n",
      "Epoch 234, Train Loss: 6.0005, Val Loss: 5.9406\n",
      "Epoch 235, Train Loss: 7.5993, Val Loss: 7.5877\n",
      "Epoch 236, Train Loss: 2.3787, Val Loss: 2.3727\n",
      "Epoch 237, Train Loss: 5.7727, Val Loss: 5.7715\n",
      "Epoch 238, Train Loss: 61.7701, Val Loss: 61.6124\n",
      "Epoch 239, Train Loss: 0.7539, Val Loss: 0.8959\n",
      "Epoch 240, Train Loss: 3.7633, Val Loss: 3.8335\n",
      "Epoch 241, Train Loss: 0.4086, Val Loss: 0.6449\n",
      "Epoch 242, Train Loss: 0.5929, Val Loss: 0.5243\n",
      "Epoch 243, Train Loss: 2.0222, Val Loss: 2.5237\n",
      "Epoch 244, Train Loss: 2.0189, Val Loss: 2.1177\n",
      "Epoch 245, Train Loss: 2.4431, Val Loss: 2.6293\n",
      "Epoch 246, Train Loss: 0.8470, Val Loss: 0.7895\n",
      "Epoch 247, Train Loss: 1.3928, Val Loss: 1.4009\n",
      "Epoch 248, Train Loss: 23.1356, Val Loss: 23.0648\n",
      "Epoch 249, Train Loss: 3.4649, Val Loss: 3.9007\n",
      "Epoch 250, Train Loss: 0.7466, Val Loss: 0.6769\n",
      "Epoch 251, Train Loss: 0.6878, Val Loss: 0.6425\n",
      "Epoch 252, Train Loss: 77.5711, Val Loss: 77.1476\n",
      "Epoch 253, Train Loss: 7.0782, Val Loss: 7.0608\n",
      "Epoch 254, Train Loss: 0.3697, Val Loss: 0.6717\n",
      "Epoch 255, Train Loss: 0.7564, Val Loss: 0.8329\n",
      "Epoch 256, Train Loss: 6.0265, Val Loss: 6.3882\n",
      "Epoch 257, Train Loss: 0.6341, Val Loss: 0.6379\n",
      "Epoch 258, Train Loss: 0.2806, Val Loss: 0.5743\n",
      "Epoch 259, Train Loss: 1.4938, Val Loss: 1.9391\n",
      "Epoch 260, Train Loss: 1.2550, Val Loss: 1.2583\n",
      "Epoch 261, Train Loss: 4.5942, Val Loss: 4.6033\n",
      "Epoch 262, Train Loss: 0.7430, Val Loss: 0.7285\n",
      "Epoch 263, Train Loss: 0.8769, Val Loss: 1.0306\n",
      "Epoch 264, Train Loss: 5.1154, Val Loss: 5.0494\n",
      "Epoch 265, Train Loss: 4.5977, Val Loss: 4.6011\n",
      "Epoch 266, Train Loss: 1.8826, Val Loss: 1.8769\n",
      "Epoch 267, Train Loss: 1.2946, Val Loss: 1.2752\n",
      "Epoch 268, Train Loss: 40.2460, Val Loss: 40.1678\n",
      "Epoch 269, Train Loss: 2.1326, Val Loss: 2.4306\n",
      "Epoch 270, Train Loss: 7.3438, Val Loss: 7.3810\n",
      "Epoch 271, Train Loss: 51.6261, Val Loss: 51.4778\n",
      "Epoch 272, Train Loss: 0.3373, Val Loss: 0.3188\n",
      "Epoch 273, Train Loss: 0.8653, Val Loss: 0.9730\n",
      "Epoch 274, Train Loss: 6.1694, Val Loss: 6.1726\n",
      "Epoch 275, Train Loss: 6.4254, Val Loss: 6.4249\n",
      "Epoch 276, Train Loss: 0.7757, Val Loss: 0.9682\n",
      "Epoch 277, Train Loss: 2.8296, Val Loss: 2.9344\n",
      "Epoch 278, Train Loss: 4.2926, Val Loss: 4.2819\n",
      "Epoch 279, Train Loss: 1.3021, Val Loss: 1.9063\n",
      "Epoch 280, Train Loss: 1.5224, Val Loss: 1.7056\n",
      "Epoch 281, Train Loss: 0.6712, Val Loss: 0.6707\n",
      "Epoch 282, Train Loss: 5.1833, Val Loss: 5.1820\n",
      "Epoch 283, Train Loss: 2.5186, Val Loss: 2.8982\n",
      "Epoch 284, Train Loss: 0.6807, Val Loss: 0.6734\n",
      "Epoch 285, Train Loss: 0.7898, Val Loss: 0.7709\n"
     ]
    }
   ],
   "source": [
    "#attemt 1\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, input_channels, hidden_channels, output_channels, num_layers):\n",
    "        super(GCN, self).__init__()\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        self.bns = torch.nn.ModuleList()\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # input layer\n",
    "        self.convs.append(GCNConv(input_channels, hidden_channels))\n",
    "        self.bns.append(torch.nn.BatchNorm1d(hidden_channels))\n",
    "\n",
    "        # hidden layers\n",
    "        for i in range(num_layers - 2):\n",
    "            self.convs.append(GCNConv(hidden_channels, hidden_channels))\n",
    "            self.bns.append(torch.nn.BatchNorm1d(hidden_channels))\n",
    "\n",
    "        # output layer\n",
    "        self.convs.append(GCNConv(hidden_channels, output_channels))\n",
    "        self.bns.append(torch.nn.BatchNorm1d(output_channels))\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        for i in range(self.num_layers - 1):\n",
    "            x = self.convs[i](x, edge_index)\n",
    "            x = self.bns[i](x)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, p=0.5, training=self.training)\n",
    "        # output layer\n",
    "        x = self.convs[-1](x, edge_index)\n",
    "        return global_mean_pool(x, torch.zeros(x.shape[0], dtype=torch.long))\n",
    "\n",
    "# assume dataX, dataY, dataEdgeIndex are given\n",
    "\n",
    "input_channels = 3  # number of features for each node\n",
    "hidden_channels = 64\n",
    "output_channels = 1  # number of output features for each node\n",
    "num_layers = 5  # number of graph convolutional layers in the network\n",
    "\n",
    "model = GCN(input_channels, hidden_channels, output_channels, num_layers)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "for i in range(len(dataX)):\n",
    "    x = torch.tensor(dataX[i], dtype=torch.float)\n",
    "    y = torch.tensor(dataY[i], dtype=torch.float)\n",
    "    edge_index = torch.tensor(dataEgdeIndex, dtype=torch.long).t().contiguous()\n",
    "\n",
    "    # set model to training mode\n",
    "    model.train()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    out = model(x, edge_index)\n",
    "    loss = F.mse_loss(out, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # set model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        out = model(x, edge_index)\n",
    "        val_loss = F.mse_loss(out, y)\n",
    "\n",
    "    print(f'Epoch {i}, Train Loss: {loss.item():.4f}, Val Loss: {val_loss.item():.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: (0.0, 0.0), 1: (6.2105, 0.0), 2: (12.4211, 0.0), 3: (18.6316, 0.0), 4: (24.8421, 0.0), 5: (31.0526, 0.0), 6: (37.2632, 0.0), 7: (43.4737, 0.0), 8: (49.6842, 0.0), 9: (55.8947, 0.0), 10: (62.1053, 0.0), 11: (68.3158, 0.0), 12: (74.5263, 0.0), 13: (80.7368, 0.0), 14: (86.9474, 0.0), 15: (93.1579, 0.0), 16: (99.3684, 0.0), 17: (105.5789, 0.0), 18: (111.7895, 0.0), 19: (118.0, 0.0)}\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the graph edges\n",
    "edges = [(i, i+1) for i in range(19)]\n",
    "\n",
    "# Define the node positions\n",
    "node_positions = dataX[140]\n",
    "node_positions = {i: (pos[1], pos[2]) for i, pos in enumerate(node_positions)}\n",
    "\n",
    "# Create the graph object and set node positions\n",
    "G = nx.Graph()\n",
    "G.add_edges_from(edges)\n",
    "nx.set_node_attributes(G, node_positions, 'pos')\n",
    "\n",
    "# Draw the graph\n",
    "pos = nx.get_node_attributes(G, 'pos')\n",
    "nx.draw(G, pos, with_labels=True)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
