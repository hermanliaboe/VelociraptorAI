{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Run this before starting\n",
    "!python -c \"import torch; print(torch.__version__)\"\n",
    "!python -c \"import torch; print(torch.version.cuda)\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0+cpu\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "!python -c \"import torch; print(torch.__version__)\"\n",
    "!python -c \"import torch; print(torch.version.cuda)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([[3, 4], [4, 6]], [[1, 2], [2, 3]])\n",
      "([2, 3], [1, 2])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "\n",
    "values_list = []  # create an empty list to store the values for each row\n",
    "x = []\n",
    "z = []\n",
    "m = []\n",
    "l = []\n",
    "\n",
    "with open('data.txt', \"r\") as f:\n",
    "    for line in f:  # loop over each line in the file\n",
    "        values = line.strip().split(\"\\t\")  # split the line into a list of strings\n",
    "\n",
    "        xL = []\n",
    "        zL = []\n",
    "        mL = []\n",
    "        lL = []\n",
    "\n",
    "        for i in range(20):\n",
    "            xL.append(round(float(values[i]),3))\n",
    "            zL.append(round(float(values[i+20]),3))\n",
    "            mL.append(round(float(values[i+20*2]),3))\n",
    "            try:\n",
    "                lL.append(round(float(values[i+20*3]),3))\n",
    "            except:\n",
    "                None\n",
    "        x.append(xL)\n",
    "        z.append(zL)\n",
    "        m.append(mL)\n",
    "        l.append(lL)\n",
    "\n",
    "\n",
    "dataY = []\n",
    "\n",
    "dataX = []\n",
    "for i in range(len(x)):\n",
    "    dataXs = []\n",
    "    for j in range(0,len(x[i])):\n",
    "        dataX0 = []\n",
    "        dataX0.append(j)\n",
    "        dataX0.append(x[i][j])\n",
    "        dataX0.append(z[i][j])\n",
    "        dataXs.append(dataX0)\n",
    "    dataX.append(dataXs)\n",
    "\n",
    "dataX = np.array(dataX)\n",
    "dataY = np.array(m)\n",
    "\n",
    "dataEgdeIndex = []\n",
    "for i in range(len(x[0])-1):\n",
    "    dataEgdeIndex.append([i,i+1])\n",
    "    dataEgdeIndex.append([i+1,i])  \n",
    "\n",
    "\n",
    "dataEgdeIndex = np.array(dataEgdeIndex) \n",
    "#dataEgdeIndex =np.transpose(dataEgdeIndex)\n",
    "\n",
    "\n",
    "input_data = dataX\n",
    "target_data = dataY\n",
    "edge_index = dataEgdeIndex\n",
    "\n",
    "zipped = list(zip(dataX, dataY))\n",
    "random.shuffle(zipped)\n",
    "dataX, dataY = zip(*zipped)\n",
    "\n",
    "l1 = [[[1,2],[2,3]],[[3,4],[4,6]]]\n",
    "l2 = [[1,2],[2,3]]\n",
    "\n",
    "zipped = list(zip(l1, l2))\n",
    "random.shuffle(zipped)\n",
    "l1, l2 = zip(*zipped)\n",
    "\n",
    "print(l1)\n",
    "print(l2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0.      0.      0.   ]\n",
      " [  1.      7.368   0.   ]\n",
      " [  2.     14.737   0.   ]\n",
      " [  3.     22.105   0.   ]\n",
      " [  4.     29.474   0.   ]\n",
      " [  5.     36.842   0.   ]\n",
      " [  6.     44.211   0.   ]\n",
      " [  7.     51.579   0.   ]\n",
      " [  8.     58.947   0.   ]\n",
      " [  9.     66.316   0.   ]\n",
      " [ 10.     73.684   0.   ]\n",
      " [ 11.     81.053   0.   ]\n",
      " [ 12.     88.421   0.   ]\n",
      " [ 13.     95.789   0.   ]\n",
      " [ 14.    103.158   0.   ]\n",
      " [ 15.    110.526   0.   ]\n",
      " [ 16.    117.895   0.   ]\n",
      " [ 17.    125.263   0.   ]\n",
      " [ 18.    132.632   0.   ]\n",
      " [ 19.    140.      0.   ]]\n"
     ]
    }
   ],
   "source": [
    "print(dataX[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn.conv import feast_conv\n",
    "from torch_geometric.loader import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Net.forward() takes 2 positional arguments but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[46], line 78\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[39m# Train the GNN model\u001b[39;00m\n\u001b[0;32m     77\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(dataX)):\n\u001b[1;32m---> 78\u001b[0m     loss \u001b[39m=\u001b[39m train(model, data[i], optimizer)\n\u001b[0;32m     79\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m: loss=\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(i, loss))\n",
      "Cell \u001b[1;32mIn[46], line 18\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, data, optimizer)\u001b[0m\n\u001b[0;32m     16\u001b[0m model\u001b[39m.\u001b[39mtrain()\n\u001b[0;32m     17\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m---> 18\u001b[0m output \u001b[39m=\u001b[39m model(data\u001b[39m.\u001b[39;49mx, data\u001b[39m.\u001b[39;49medge_index)\n\u001b[0;32m     19\u001b[0m loss \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mmse_loss(output, data\u001b[39m.\u001b[39my)\n\u001b[0;32m     20\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\eliasak\\OneDrive - NTNU\\Master thesis\\04_Elias\\ML1\\masters2023\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;31mTypeError\u001b[0m: Net.forward() takes 2 positional arguments but 3 were given"
     ]
    }
   ],
   "source": [
    "#Original Herman Code\n",
    "# Define a simple GNN model\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = GCNConv(3, 16)\n",
    "        self.conv2 = GCNConv(16, 1)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x.squeeze(dim=1)\n",
    "\n",
    "# Define a simple training function for the GNN model\n",
    "def train(model, data, optimizer):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(data.x, data.edge_index)\n",
    "    loss = F.mse_loss(output, data.y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "# Define a simple testing function for the GNN model\n",
    "def test(model, data):\n",
    "    model.eval()\n",
    "    output = model(data.x, data.edge_index)\n",
    "    return output\n",
    "\n",
    "\"\"\"\"\n",
    "# Set up the input data\n",
    "x = torch.tensor([\n",
    "    [0,0,1,1,1,0,0,0],\n",
    "    [10,5,0,0,0,0,5,0],\n",
    "    [20,10,0,0,0,0,5,0],\n",
    "    [30,5,0,0,0,0,5,0],\n",
    "    [40,0,1,1,1,0,0,0]\n",
    "], dtype=torch.float)\n",
    "\n",
    "edge_index = torch.tensor([\n",
    "    [0,1], [1,0], [1,2], [2,1], [2,3], [3,2], [3,4], [4,3]\n",
    "], dtype=torch.long).t()\n",
    "\n",
    "# Set up the output data (nodal forces and moments)\n",
    "y = torch.tensor([\n",
    "    [0,0,0,0,0,10,20,0],\n",
    "    [0,0,0,0,0,5,0,0],\n",
    "    [0,0,0,0,0,5,0,0],\n",
    "    [0,0,0,0,0,5,0,0],\n",
    "    [0,0,0,0,0,10,20,0]\n",
    "], dtype=torch.float)\n",
    "data = Data(x=x, edge_index=edge_index, y=y)\n",
    "\"\"\"\n",
    "\n",
    "# Create a Data object that encapsulates the input and output data\n",
    "\n",
    "data = []\n",
    "for i in range(len(dataX)):\n",
    "    x = torch.tensor(dataX[i])\n",
    "    y = torch.tensor(dataY[i])\n",
    "    edge_index = torch.tensor((dataEgdeIndex), dtype=torch.long).t()\n",
    "    data.append(Data(x=x, edge_index=edge_index, y=y))\n",
    "\n",
    "\n",
    "\n",
    "# Initialize the GNN model\n",
    "#input_dim = x.shape[1]\n",
    "#output_dim = y.shape[1]\n",
    "hidden_dim = 16\n",
    "model = Net()\n",
    "\n",
    "# Define the optimizer and the number of epochs for training\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "num_epochs = 100\n",
    "\n",
    "# Train the GNN model\n",
    "for i in range(len(dataX)):\n",
    "    loss = train(model, data[i], optimizer)\n",
    "    print('Epoch {}: loss={}'.format(i, loss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eliasak\\AppData\\Local\\Temp\\ipykernel_27440\\2682750836.py:59: UserWarning: Using a target size (torch.Size([20])) that is different to the input size (torch.Size([20, 20])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss = F.mse_loss(out, y)\n",
      "C:\\Users\\eliasak\\AppData\\Local\\Temp\\ipykernel_27440\\2682750836.py:68: UserWarning: Using a target size (torch.Size([20])) that is different to the input size (torch.Size([20, 20])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  val_loss = F.mse_loss(out, y)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Train Loss: 1.7858, Val Loss: 1.1525\n",
      "Epoch 1, Train Loss: 68.8498, Val Loss: 67.2588\n",
      "Epoch 2, Train Loss: 6.9740, Val Loss: 6.8077\n",
      "Epoch 3, Train Loss: 7.5318, Val Loss: 7.1296\n",
      "Epoch 4, Train Loss: 6.7697, Val Loss: 6.6878\n",
      "Epoch 5, Train Loss: 1.1306, Val Loss: 1.3954\n",
      "Epoch 6, Train Loss: 0.7117, Val Loss: 0.8460\n",
      "Epoch 7, Train Loss: 1.1129, Val Loss: 0.5723\n",
      "Epoch 8, Train Loss: 3.2320, Val Loss: 2.9873\n",
      "Epoch 9, Train Loss: 1.1092, Val Loss: 0.7633\n",
      "Epoch 10, Train Loss: 48.6386, Val Loss: 49.5466\n",
      "Epoch 11, Train Loss: 1.1603, Val Loss: 1.0949\n",
      "Epoch 12, Train Loss: 36.0159, Val Loss: 36.1430\n",
      "Epoch 13, Train Loss: 1.2949, Val Loss: 0.8418\n",
      "Epoch 14, Train Loss: 1.1838, Val Loss: 1.2728\n",
      "Epoch 15, Train Loss: 23.2893, Val Loss: 23.4218\n",
      "Epoch 16, Train Loss: 19.7109, Val Loss: 19.9666\n",
      "Epoch 17, Train Loss: 0.7142, Val Loss: 0.2805\n",
      "Epoch 18, Train Loss: 0.7053, Val Loss: 0.3054\n",
      "Epoch 19, Train Loss: 0.9210, Val Loss: 0.8134\n",
      "Epoch 20, Train Loss: 3.4976, Val Loss: 3.2696\n",
      "Epoch 21, Train Loss: 0.6212, Val Loss: 0.4781\n",
      "Epoch 22, Train Loss: 1.0150, Val Loss: 0.9761\n",
      "Epoch 23, Train Loss: 1.0050, Val Loss: 0.8651\n",
      "Epoch 24, Train Loss: 36.4287, Val Loss: 36.2868\n",
      "Epoch 25, Train Loss: 0.3696, Val Loss: 0.2792\n",
      "Epoch 26, Train Loss: 0.3385, Val Loss: 0.2684\n",
      "Epoch 27, Train Loss: 4.2027, Val Loss: 4.1029\n",
      "Epoch 28, Train Loss: 0.3939, Val Loss: 0.3404\n",
      "Epoch 29, Train Loss: 5.2688, Val Loss: 5.0108\n",
      "Epoch 30, Train Loss: 3.6054, Val Loss: 3.5617\n",
      "Epoch 31, Train Loss: 0.7727, Val Loss: 0.6369\n",
      "Epoch 32, Train Loss: 1.1163, Val Loss: 1.1485\n",
      "Epoch 33, Train Loss: 0.4754, Val Loss: 0.4051\n",
      "Epoch 34, Train Loss: 1.6688, Val Loss: 1.5969\n",
      "Epoch 35, Train Loss: 4.0682, Val Loss: 3.9952\n",
      "Epoch 36, Train Loss: 6.1507, Val Loss: 5.9847\n",
      "Epoch 37, Train Loss: 0.8541, Val Loss: 0.7732\n",
      "Epoch 38, Train Loss: 0.3645, Val Loss: 0.3464\n",
      "Epoch 39, Train Loss: 0.6638, Val Loss: 0.5941\n",
      "Epoch 40, Train Loss: 2.6762, Val Loss: 2.5866\n",
      "Epoch 41, Train Loss: 2.2541, Val Loss: 2.2145\n",
      "Epoch 42, Train Loss: 0.2765, Val Loss: 0.2803\n",
      "Epoch 43, Train Loss: 0.6154, Val Loss: 0.5225\n",
      "Epoch 44, Train Loss: 0.7558, Val Loss: 0.5691\n",
      "Epoch 45, Train Loss: 1.6479, Val Loss: 1.5733\n",
      "Epoch 46, Train Loss: 0.2841, Val Loss: 0.1878\n",
      "Epoch 47, Train Loss: 6.3895, Val Loss: 6.4881\n",
      "Epoch 48, Train Loss: 0.3506, Val Loss: 0.2944\n",
      "Epoch 49, Train Loss: 52.5193, Val Loss: 52.4397\n",
      "Epoch 50, Train Loss: 3.5862, Val Loss: 3.5017\n",
      "Epoch 51, Train Loss: 70.8039, Val Loss: 69.3738\n",
      "Epoch 52, Train Loss: 0.4259, Val Loss: 0.3757\n",
      "Epoch 53, Train Loss: 3.1489, Val Loss: 3.0253\n",
      "Epoch 54, Train Loss: 3.9502, Val Loss: 3.9160\n",
      "Epoch 55, Train Loss: 0.3574, Val Loss: 0.3921\n",
      "Epoch 56, Train Loss: 1.3646, Val Loss: 0.9452\n",
      "Epoch 57, Train Loss: 1.2223, Val Loss: 1.1505\n",
      "Epoch 58, Train Loss: 4.3484, Val Loss: 4.2895\n",
      "Epoch 59, Train Loss: 3.6873, Val Loss: 3.5463\n",
      "Epoch 60, Train Loss: 0.7380, Val Loss: 0.5691\n",
      "Epoch 61, Train Loss: 0.8056, Val Loss: 0.2502\n",
      "Epoch 62, Train Loss: 0.6433, Val Loss: 0.6882\n",
      "Epoch 63, Train Loss: 5.8769, Val Loss: 5.7727\n",
      "Epoch 64, Train Loss: 1.9570, Val Loss: 1.8711\n",
      "Epoch 65, Train Loss: 4.1623, Val Loss: 3.9761\n",
      "Epoch 66, Train Loss: 0.6152, Val Loss: 0.5184\n",
      "Epoch 67, Train Loss: 1.4960, Val Loss: 1.2881\n",
      "Epoch 68, Train Loss: 0.2927, Val Loss: 0.1473\n",
      "Epoch 69, Train Loss: 5.8965, Val Loss: 6.0084\n",
      "Epoch 70, Train Loss: 0.3500, Val Loss: 0.2171\n",
      "Epoch 71, Train Loss: 0.7860, Val Loss: 0.6563\n",
      "Epoch 72, Train Loss: 0.5304, Val Loss: 0.4318\n",
      "Epoch 73, Train Loss: 0.5764, Val Loss: 0.4231\n",
      "Epoch 74, Train Loss: 0.3623, Val Loss: 0.2578\n",
      "Epoch 75, Train Loss: 3.0939, Val Loss: 3.0566\n",
      "Epoch 76, Train Loss: 0.4261, Val Loss: 0.3876\n",
      "Epoch 77, Train Loss: 5.3655, Val Loss: 5.3478\n",
      "Epoch 78, Train Loss: 5.0028, Val Loss: 4.8130\n",
      "Epoch 79, Train Loss: 25.1534, Val Loss: 25.5787\n",
      "Epoch 80, Train Loss: 0.4534, Val Loss: 0.4243\n",
      "Epoch 81, Train Loss: 0.1690, Val Loss: 0.1083\n",
      "Epoch 82, Train Loss: 0.3196, Val Loss: 0.2703\n",
      "Epoch 83, Train Loss: 0.7116, Val Loss: 0.6591\n",
      "Epoch 84, Train Loss: 0.4974, Val Loss: 0.4523\n",
      "Epoch 85, Train Loss: 40.6906, Val Loss: 40.2959\n",
      "Epoch 86, Train Loss: 0.2786, Val Loss: 0.1654\n",
      "Epoch 87, Train Loss: 0.3796, Val Loss: 0.2523\n",
      "Epoch 88, Train Loss: 1.3299, Val Loss: 1.3297\n",
      "Epoch 89, Train Loss: 4.2552, Val Loss: 4.2814\n",
      "Epoch 90, Train Loss: 0.5044, Val Loss: 0.2658\n",
      "Epoch 91, Train Loss: 1.3980, Val Loss: 1.2295\n",
      "Epoch 92, Train Loss: 1.8949, Val Loss: 1.8151\n",
      "Epoch 93, Train Loss: 0.5496, Val Loss: 0.2587\n",
      "Epoch 94, Train Loss: 0.9258, Val Loss: 0.7037\n",
      "Epoch 95, Train Loss: 0.7761, Val Loss: 0.4763\n",
      "Epoch 96, Train Loss: 0.4321, Val Loss: 0.2056\n",
      "Epoch 97, Train Loss: 17.1732, Val Loss: 17.5557\n",
      "Epoch 98, Train Loss: 3.3828, Val Loss: 3.4040\n",
      "Epoch 99, Train Loss: 2.3971, Val Loss: 2.3468\n",
      "Epoch 100, Train Loss: 51.3186, Val Loss: 50.4143\n",
      "Epoch 101, Train Loss: 2.9815, Val Loss: 3.0099\n",
      "Epoch 102, Train Loss: 1.2327, Val Loss: 1.3898\n",
      "Epoch 103, Train Loss: 0.5758, Val Loss: 0.3718\n",
      "Epoch 104, Train Loss: 0.3923, Val Loss: 0.4050\n",
      "Epoch 105, Train Loss: 3.6125, Val Loss: 3.5118\n",
      "Epoch 106, Train Loss: 41.5034, Val Loss: 41.7915\n",
      "Epoch 107, Train Loss: 1.5377, Val Loss: 1.6124\n",
      "Epoch 108, Train Loss: 0.7540, Val Loss: 0.5697\n",
      "Epoch 109, Train Loss: 5.0283, Val Loss: 4.6367\n",
      "Epoch 110, Train Loss: 1.1483, Val Loss: 0.8324\n",
      "Epoch 111, Train Loss: 0.8256, Val Loss: 0.4792\n",
      "Epoch 112, Train Loss: 1.0478, Val Loss: 0.4559\n",
      "Epoch 113, Train Loss: 0.8547, Val Loss: 0.3697\n",
      "Epoch 114, Train Loss: 4.2371, Val Loss: 3.7561\n",
      "Epoch 115, Train Loss: 3.1822, Val Loss: 2.8859\n",
      "Epoch 116, Train Loss: 17.7248, Val Loss: 18.9639\n",
      "Epoch 117, Train Loss: 11.1263, Val Loss: 12.2431\n",
      "Epoch 118, Train Loss: 3.7201, Val Loss: 3.6585\n",
      "Epoch 119, Train Loss: 2.7222, Val Loss: 2.7507\n",
      "Epoch 120, Train Loss: 0.7402, Val Loss: 0.4181\n",
      "Epoch 121, Train Loss: 1.9109, Val Loss: 1.5467\n",
      "Epoch 122, Train Loss: 3.7479, Val Loss: 3.9170\n",
      "Epoch 123, Train Loss: 0.5906, Val Loss: 0.2795\n",
      "Epoch 124, Train Loss: 0.2865, Val Loss: 0.2412\n",
      "Epoch 125, Train Loss: 1.0652, Val Loss: 0.8660\n",
      "Epoch 126, Train Loss: 3.9183, Val Loss: 4.1433\n",
      "Epoch 127, Train Loss: 2.7192, Val Loss: 2.7467\n",
      "Epoch 128, Train Loss: 29.8347, Val Loss: 30.9240\n",
      "Epoch 129, Train Loss: 0.3080, Val Loss: 0.2590\n",
      "Epoch 130, Train Loss: 0.9233, Val Loss: 0.7602\n",
      "Epoch 131, Train Loss: 0.9052, Val Loss: 0.4943\n",
      "Epoch 132, Train Loss: 5.7487, Val Loss: 5.4368\n",
      "Epoch 133, Train Loss: 1.5758, Val Loss: 1.3649\n",
      "Epoch 134, Train Loss: 1.4762, Val Loss: 1.5676\n",
      "Epoch 135, Train Loss: 2.4032, Val Loss: 2.0998\n",
      "Epoch 136, Train Loss: 3.0107, Val Loss: 2.9625\n",
      "Epoch 137, Train Loss: 2.0925, Val Loss: 2.0344\n",
      "Epoch 138, Train Loss: 0.6009, Val Loss: 0.5438\n",
      "Epoch 139, Train Loss: 0.5417, Val Loss: 0.3550\n",
      "Epoch 140, Train Loss: 1.6097, Val Loss: 1.4861\n",
      "Epoch 141, Train Loss: 19.6505, Val Loss: 20.1914\n",
      "Epoch 142, Train Loss: 0.5295, Val Loss: 0.3817\n",
      "Epoch 143, Train Loss: 0.2733, Val Loss: 0.1705\n",
      "Epoch 144, Train Loss: 2.4017, Val Loss: 2.3668\n",
      "Epoch 145, Train Loss: 0.4724, Val Loss: 0.2883\n",
      "Epoch 146, Train Loss: 0.3799, Val Loss: 0.2627\n",
      "Epoch 147, Train Loss: 3.8080, Val Loss: 3.7600\n",
      "Epoch 148, Train Loss: 1.3818, Val Loss: 1.3848\n",
      "Epoch 149, Train Loss: 4.2483, Val Loss: 4.2309\n",
      "Epoch 150, Train Loss: 0.3641, Val Loss: 0.1805\n",
      "Epoch 151, Train Loss: 0.2778, Val Loss: 0.1910\n",
      "Epoch 152, Train Loss: 0.2651, Val Loss: 0.2476\n",
      "Epoch 153, Train Loss: 3.0376, Val Loss: 3.0170\n",
      "Epoch 154, Train Loss: 0.2144, Val Loss: 0.1447\n",
      "Epoch 155, Train Loss: 27.1067, Val Loss: 27.1822\n",
      "Epoch 156, Train Loss: 2.3592, Val Loss: 2.4479\n",
      "Epoch 157, Train Loss: 52.8855, Val Loss: 52.3583\n",
      "Epoch 158, Train Loss: 0.6277, Val Loss: 0.6169\n",
      "Epoch 159, Train Loss: 1.2809, Val Loss: 1.2061\n",
      "Epoch 160, Train Loss: 0.2692, Val Loss: 0.2784\n",
      "Epoch 161, Train Loss: 2.2155, Val Loss: 2.0359\n",
      "Epoch 162, Train Loss: 0.3387, Val Loss: 0.2619\n",
      "Epoch 163, Train Loss: 0.9943, Val Loss: 0.6117\n",
      "Epoch 164, Train Loss: 0.4807, Val Loss: 0.3061\n",
      "Epoch 165, Train Loss: 1.6838, Val Loss: 1.1004\n",
      "Epoch 166, Train Loss: 3.1020, Val Loss: 2.8307\n",
      "Epoch 167, Train Loss: 1.8856, Val Loss: 1.3391\n",
      "Epoch 168, Train Loss: 1.0345, Val Loss: 0.7468\n",
      "Epoch 169, Train Loss: 0.3842, Val Loss: 0.2576\n",
      "Epoch 170, Train Loss: 0.3824, Val Loss: 0.2419\n",
      "Epoch 171, Train Loss: 5.2504, Val Loss: 5.1162\n",
      "Epoch 172, Train Loss: 4.8304, Val Loss: 4.7470\n",
      "Epoch 173, Train Loss: 3.0644, Val Loss: 3.3321\n",
      "Epoch 174, Train Loss: 1.4075, Val Loss: 1.2550\n",
      "Epoch 175, Train Loss: 0.3422, Val Loss: 0.1842\n",
      "Epoch 176, Train Loss: 3.2316, Val Loss: 3.3598\n",
      "Epoch 177, Train Loss: 2.6118, Val Loss: 2.8782\n",
      "Epoch 178, Train Loss: 1.4865, Val Loss: 1.4499\n",
      "Epoch 179, Train Loss: 0.2126, Val Loss: 0.1610\n",
      "Epoch 180, Train Loss: 2.5035, Val Loss: 2.6933\n",
      "Epoch 181, Train Loss: 4.0115, Val Loss: 4.0097\n",
      "Epoch 182, Train Loss: 0.2340, Val Loss: 0.1875\n",
      "Epoch 183, Train Loss: 0.3591, Val Loss: 0.2735\n",
      "Epoch 184, Train Loss: 4.5624, Val Loss: 4.7190\n",
      "Epoch 185, Train Loss: 0.2884, Val Loss: 0.2645\n",
      "Epoch 186, Train Loss: 3.7211, Val Loss: 3.8451\n",
      "Epoch 187, Train Loss: 3.8658, Val Loss: 4.0481\n",
      "Epoch 188, Train Loss: 0.2885, Val Loss: 0.2334\n",
      "Epoch 189, Train Loss: 1.3533, Val Loss: 1.2109\n",
      "Epoch 190, Train Loss: 3.0625, Val Loss: 2.9816\n",
      "Epoch 191, Train Loss: 0.3310, Val Loss: 0.2617\n",
      "Epoch 192, Train Loss: 0.3181, Val Loss: 0.2678\n",
      "Epoch 193, Train Loss: 1.1952, Val Loss: 1.0969\n",
      "Epoch 194, Train Loss: 3.5013, Val Loss: 3.5707\n",
      "Epoch 195, Train Loss: 3.6941, Val Loss: 3.6889\n",
      "Epoch 196, Train Loss: 67.1459, Val Loss: 66.9544\n",
      "Epoch 197, Train Loss: 0.2439, Val Loss: 0.2372\n",
      "Epoch 198, Train Loss: 1.9566, Val Loss: 1.7681\n",
      "Epoch 199, Train Loss: 3.3240, Val Loss: 3.2073\n",
      "Epoch 200, Train Loss: 2.3347, Val Loss: 2.0396\n",
      "Epoch 201, Train Loss: 1.8531, Val Loss: 1.8850\n",
      "Epoch 202, Train Loss: 0.8552, Val Loss: 0.5979\n",
      "Epoch 203, Train Loss: 1.9623, Val Loss: 1.9479\n",
      "Epoch 204, Train Loss: 1.0881, Val Loss: 1.0517\n",
      "Epoch 205, Train Loss: 57.3553, Val Loss: 57.3817\n",
      "Epoch 206, Train Loss: 1.5608, Val Loss: 1.2781\n",
      "Epoch 207, Train Loss: 0.9054, Val Loss: 0.7625\n",
      "Epoch 208, Train Loss: 1.1644, Val Loss: 0.9578\n",
      "Epoch 209, Train Loss: 2.3823, Val Loss: 1.9264\n",
      "Epoch 210, Train Loss: 2.6697, Val Loss: 2.4420\n",
      "Epoch 211, Train Loss: 1.3846, Val Loss: 1.3465\n",
      "Epoch 212, Train Loss: 4.2754, Val Loss: 3.1557\n",
      "Epoch 213, Train Loss: 3.6072, Val Loss: 2.4590\n",
      "Epoch 214, Train Loss: 2.5522, Val Loss: 2.4987\n",
      "Epoch 215, Train Loss: 3.6042, Val Loss: 2.4861\n",
      "Epoch 216, Train Loss: 1.4091, Val Loss: 0.9264\n",
      "Epoch 217, Train Loss: 0.6993, Val Loss: 0.3716\n",
      "Epoch 218, Train Loss: 2.4059, Val Loss: 1.4951\n",
      "Epoch 219, Train Loss: 10.1256, Val Loss: 10.5959\n",
      "Epoch 220, Train Loss: 5.8692, Val Loss: 5.2815\n",
      "Epoch 221, Train Loss: 1.0750, Val Loss: 0.5925\n",
      "Epoch 222, Train Loss: 0.4856, Val Loss: 0.3833\n",
      "Epoch 223, Train Loss: 0.7904, Val Loss: 0.4822\n",
      "Epoch 224, Train Loss: 2.4565, Val Loss: 2.3016\n",
      "Epoch 225, Train Loss: 0.2054, Val Loss: 0.1636\n",
      "Epoch 226, Train Loss: 1.5902, Val Loss: 1.4701\n",
      "Epoch 227, Train Loss: 1.8873, Val Loss: 1.6224\n",
      "Epoch 228, Train Loss: 0.2106, Val Loss: 0.1663\n",
      "Epoch 229, Train Loss: 0.5325, Val Loss: 0.3941\n",
      "Epoch 230, Train Loss: 72.4070, Val Loss: 72.1066\n",
      "Epoch 231, Train Loss: 3.4759, Val Loss: 3.5505\n",
      "Epoch 232, Train Loss: 1.0212, Val Loss: 1.0670\n",
      "Epoch 233, Train Loss: 1.7219, Val Loss: 1.4052\n",
      "Epoch 234, Train Loss: 3.4362, Val Loss: 3.8682\n",
      "Epoch 235, Train Loss: 0.2237, Val Loss: 0.1169\n",
      "Epoch 236, Train Loss: 2.6602, Val Loss: 2.6386\n",
      "Epoch 237, Train Loss: 1.2723, Val Loss: 0.8472\n",
      "Epoch 238, Train Loss: 2.2351, Val Loss: 1.9491\n",
      "Epoch 239, Train Loss: 0.7405, Val Loss: 0.4703\n",
      "Epoch 240, Train Loss: 1.0802, Val Loss: 1.1214\n",
      "Epoch 241, Train Loss: 2.2299, Val Loss: 2.0460\n",
      "Epoch 242, Train Loss: 1.9652, Val Loss: 2.1728\n",
      "Epoch 243, Train Loss: 3.2399, Val Loss: 2.9990\n",
      "Epoch 244, Train Loss: 1.8905, Val Loss: 1.4764\n",
      "Epoch 245, Train Loss: 2.9751, Val Loss: 2.7205\n",
      "Epoch 246, Train Loss: 2.4870, Val Loss: 2.4300\n",
      "Epoch 247, Train Loss: 3.3870, Val Loss: 3.1336\n",
      "Epoch 248, Train Loss: 0.5351, Val Loss: 0.4991\n",
      "Epoch 249, Train Loss: 1.0012, Val Loss: 0.8728\n",
      "Epoch 250, Train Loss: 0.5537, Val Loss: 0.4416\n",
      "Epoch 251, Train Loss: 1.9585, Val Loss: 1.9651\n",
      "Epoch 252, Train Loss: 0.3686, Val Loss: 0.2116\n",
      "Epoch 253, Train Loss: 0.1613, Val Loss: 0.1450\n",
      "Epoch 254, Train Loss: 0.2597, Val Loss: 0.2323\n",
      "Epoch 255, Train Loss: 4.7403, Val Loss: 4.8623\n",
      "Epoch 256, Train Loss: 4.6488, Val Loss: 4.7100\n",
      "Epoch 257, Train Loss: 4.7383, Val Loss: 4.5712\n",
      "Epoch 258, Train Loss: 59.0989, Val Loss: 59.1189\n",
      "Epoch 259, Train Loss: 0.2509, Val Loss: 0.2409\n",
      "Epoch 260, Train Loss: 0.2827, Val Loss: 0.2263\n",
      "Epoch 261, Train Loss: 0.3000, Val Loss: 0.2779\n",
      "Epoch 262, Train Loss: 2.1174, Val Loss: 2.0638\n",
      "Epoch 263, Train Loss: 0.1042, Val Loss: 0.1237\n",
      "Epoch 264, Train Loss: 1.0123, Val Loss: 1.1526\n",
      "Epoch 265, Train Loss: 0.8688, Val Loss: 0.9630\n",
      "Epoch 266, Train Loss: 1.4304, Val Loss: 1.2902\n",
      "Epoch 267, Train Loss: 0.3200, Val Loss: 0.2522\n",
      "Epoch 268, Train Loss: 33.5854, Val Loss: 32.8023\n",
      "Epoch 269, Train Loss: 0.2052, Val Loss: 0.2217\n",
      "Epoch 270, Train Loss: 0.5574, Val Loss: 0.5566\n",
      "Epoch 271, Train Loss: 0.9341, Val Loss: 0.9575\n",
      "Epoch 272, Train Loss: 0.8409, Val Loss: 0.8872\n",
      "Epoch 273, Train Loss: 0.1668, Val Loss: 0.2368\n",
      "Epoch 274, Train Loss: 1.2175, Val Loss: 0.9395\n",
      "Epoch 275, Train Loss: 3.9194, Val Loss: 3.9012\n",
      "Epoch 276, Train Loss: 0.5282, Val Loss: 0.6270\n",
      "Epoch 277, Train Loss: 1.3888, Val Loss: 1.2268\n",
      "Epoch 278, Train Loss: 0.1918, Val Loss: 0.1249\n",
      "Epoch 279, Train Loss: 2.3335, Val Loss: 2.3736\n",
      "Epoch 280, Train Loss: 0.4098, Val Loss: 0.4795\n",
      "Epoch 281, Train Loss: 2.0630, Val Loss: 2.0492\n",
      "Epoch 282, Train Loss: 0.2688, Val Loss: 0.1942\n",
      "Epoch 283, Train Loss: 1.0052, Val Loss: 0.8870\n",
      "Epoch 284, Train Loss: 0.4663, Val Loss: 0.4748\n",
      "Epoch 285, Train Loss: 22.7297, Val Loss: 22.9029\n"
     ]
    }
   ],
   "source": [
    "#attemt 2\n",
    "\n",
    "#define modell\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, input_channels, hidden_channels, output_channels, num_layers):\n",
    "        super(GCN, self).__init__()\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        self.bns = torch.nn.ModuleList()\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # input layer\n",
    "        self.convs.append(GCNConv(input_channels, hidden_channels))\n",
    "        self.bns.append(torch.nn.BatchNorm1d(hidden_channels))\n",
    "\n",
    "        # hidden layers\n",
    "        for i in range(num_layers - 2):\n",
    "            self.convs.append(GCNConv(hidden_channels, hidden_channels))\n",
    "            self.bns.append(torch.nn.BatchNorm1d(hidden_channels))\n",
    "\n",
    "        # output layer\n",
    "        self.convs.append(GCNConv(hidden_channels, output_channels))\n",
    "        self.prediction_layer = torch.nn.Linear(output_channels, num_nodes)\n",
    "\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        for i in range(self.num_layers - 1):\n",
    "            x = self.convs[i](x, edge_index)\n",
    "            x = self.bns[i](x)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, p=0.5, training=self.training)\n",
    "        # output layer  \n",
    "        x = self.convs[-1](x, edge_index)\n",
    "        x = self.prediction_layer(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "input_channels = 3  # number of features for each node\n",
    "hidden_channels = 64\n",
    "output_channels = 1  # number of output features for each node\n",
    "num_layers = 5  # number of graph convolutional layers in the network\n",
    "num_nodes = len(dataX[i])\n",
    "\n",
    "model = GCN(input_channels, hidden_channels, output_channels, num_layers)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "num_epochs = len(dataX)\n",
    "for i in range(num_epochs):\n",
    "    x = torch.tensor(dataX[i], dtype=torch.float)\n",
    "    y = torch.tensor(dataY[i], dtype=torch.float)\n",
    "    edge_index = torch.tensor(dataEgdeIndex, dtype=torch.long).t().contiguous()\n",
    "\n",
    "    # set model to training mode\n",
    "    model.train()\n",
    "    #print(model)\n",
    "    optimizer.zero_grad()\n",
    "    out = model(x, edge_index)\n",
    "    #print(out)\n",
    "    loss = F.mse_loss(out, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # set model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        out = model(x, edge_index)\n",
    "        val_loss = F.mse_loss(out, y)\n",
    "\n",
    "    print(f'Epoch {i}, Train Loss: {loss.item():.4f}, Val Loss: {val_loss.item():.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eliasak\\AppData\\Local\\Temp\\ipykernel_27440\\2992925218.py:52: UserWarning: Using a target size (torch.Size([20])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss = F.mse_loss(out, y)\n",
      "C:\\Users\\eliasak\\AppData\\Local\\Temp\\ipykernel_27440\\2992925218.py:61: UserWarning: Using a target size (torch.Size([20])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  val_loss = F.mse_loss(out, y)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Train Loss: 1.3782, Val Loss: 2.9060\n",
      "Epoch 1, Train Loss: 0.2650, Val Loss: 0.4255\n",
      "Epoch 2, Train Loss: 7.3466, Val Loss: 8.2165\n",
      "Epoch 3, Train Loss: 2.4732, Val Loss: 4.3371\n",
      "Epoch 4, Train Loss: 0.7961, Val Loss: 1.8438\n",
      "Epoch 5, Train Loss: 0.8936, Val Loss: 4.7732\n",
      "Epoch 6, Train Loss: 0.2893, Val Loss: 1.8559\n",
      "Epoch 7, Train Loss: 6.3535, Val Loss: 8.0194\n",
      "Epoch 8, Train Loss: 0.5843, Val Loss: 4.6098\n",
      "Epoch 9, Train Loss: 0.7314, Val Loss: 3.7344\n",
      "Epoch 10, Train Loss: 21.7123, Val Loss: 21.5878\n",
      "Epoch 11, Train Loss: 0.7372, Val Loss: 4.0146\n",
      "Epoch 12, Train Loss: 0.3344, Val Loss: 3.1113\n",
      "Epoch 13, Train Loss: 3.1212, Val Loss: 12.2694\n",
      "Epoch 14, Train Loss: 2.6678, Val Loss: 8.3494\n",
      "Epoch 15, Train Loss: 1.0060, Val Loss: 4.0696\n",
      "Epoch 16, Train Loss: 48.1068, Val Loss: 48.3546\n",
      "Epoch 17, Train Loss: 1.8044, Val Loss: 3.9336\n",
      "Epoch 18, Train Loss: 0.6775, Val Loss: 1.1006\n",
      "Epoch 19, Train Loss: 64.9093, Val Loss: 64.8838\n",
      "Epoch 20, Train Loss: 0.7408, Val Loss: 2.3524\n",
      "Epoch 21, Train Loss: 1.1733, Val Loss: 2.7398\n",
      "Epoch 22, Train Loss: 0.7722, Val Loss: 0.8254\n",
      "Epoch 23, Train Loss: 5.6156, Val Loss: 12.5299\n",
      "Epoch 24, Train Loss: 0.6781, Val Loss: 1.2326\n",
      "Epoch 25, Train Loss: 0.3823, Val Loss: 0.3201\n",
      "Epoch 26, Train Loss: 81.5643, Val Loss: 81.6777\n",
      "Epoch 27, Train Loss: 4.1852, Val Loss: 4.1594\n",
      "Epoch 28, Train Loss: 5.1123, Val Loss: 4.9160\n",
      "Epoch 29, Train Loss: 0.8400, Val Loss: 0.7909\n",
      "Epoch 30, Train Loss: 0.3188, Val Loss: 0.8475\n",
      "Epoch 31, Train Loss: 0.7804, Val Loss: 1.2024\n",
      "Epoch 32, Train Loss: 1.8128, Val Loss: 1.6757\n",
      "Epoch 33, Train Loss: 1.5430, Val Loss: 1.4269\n",
      "Epoch 34, Train Loss: 5.3257, Val Loss: 8.0744\n",
      "Epoch 35, Train Loss: 1.0377, Val Loss: 2.2179\n",
      "Epoch 36, Train Loss: 5.0349, Val Loss: 5.1970\n",
      "Epoch 37, Train Loss: 6.4423, Val Loss: 6.5472\n",
      "Epoch 38, Train Loss: 1.6824, Val Loss: 1.6764\n",
      "Epoch 39, Train Loss: 18.3276, Val Loss: 18.4155\n",
      "Epoch 40, Train Loss: 0.7423, Val Loss: 2.0357\n",
      "Epoch 41, Train Loss: 58.1343, Val Loss: 58.2627\n",
      "Epoch 42, Train Loss: 3.6900, Val Loss: 6.9852\n",
      "Epoch 43, Train Loss: 2.8521, Val Loss: 3.1113\n",
      "Epoch 44, Train Loss: 7.5222, Val Loss: 7.3977\n",
      "Epoch 45, Train Loss: 0.9093, Val Loss: 0.7866\n",
      "Epoch 46, Train Loss: 1.5514, Val Loss: 1.5541\n",
      "Epoch 47, Train Loss: 7.1852, Val Loss: 7.2630\n",
      "Epoch 48, Train Loss: 26.4985, Val Loss: 26.5320\n",
      "Epoch 49, Train Loss: 0.8230, Val Loss: 0.7417\n",
      "Epoch 50, Train Loss: 1.7093, Val Loss: 1.5048\n",
      "Epoch 51, Train Loss: 5.4773, Val Loss: 5.5616\n",
      "Epoch 52, Train Loss: 0.4700, Val Loss: 1.3905\n",
      "Epoch 53, Train Loss: 33.0478, Val Loss: 32.7367\n",
      "Epoch 54, Train Loss: 0.4938, Val Loss: 1.3366\n",
      "Epoch 55, Train Loss: 0.7736, Val Loss: 0.7750\n",
      "Epoch 56, Train Loss: 0.7843, Val Loss: 1.7349\n",
      "Epoch 57, Train Loss: 1.1132, Val Loss: 0.8533\n",
      "Epoch 58, Train Loss: 1.7032, Val Loss: 2.5404\n",
      "Epoch 59, Train Loss: 0.6477, Val Loss: 0.6709\n",
      "Epoch 60, Train Loss: 25.1953, Val Loss: 24.7972\n",
      "Epoch 61, Train Loss: 5.1760, Val Loss: 4.9011\n",
      "Epoch 62, Train Loss: 6.4592, Val Loss: 6.3008\n",
      "Epoch 63, Train Loss: 4.4299, Val Loss: 6.5760\n",
      "Epoch 64, Train Loss: 1.0172, Val Loss: 1.0699\n",
      "Epoch 65, Train Loss: 3.0586, Val Loss: 3.6205\n",
      "Epoch 66, Train Loss: 2.5251, Val Loss: 2.6423\n",
      "Epoch 67, Train Loss: 7.6125, Val Loss: 7.5143\n",
      "Epoch 68, Train Loss: 0.8580, Val Loss: 0.7622\n",
      "Epoch 69, Train Loss: 6.6622, Val Loss: 7.2451\n",
      "Epoch 70, Train Loss: 0.7781, Val Loss: 0.7147\n",
      "Epoch 71, Train Loss: 0.5377, Val Loss: 1.5764\n",
      "Epoch 72, Train Loss: 0.7467, Val Loss: 1.1168\n",
      "Epoch 73, Train Loss: 5.3687, Val Loss: 5.3834\n",
      "Epoch 74, Train Loss: 1.6377, Val Loss: 2.9067\n",
      "Epoch 75, Train Loss: 0.6559, Val Loss: 0.6221\n",
      "Epoch 76, Train Loss: 6.5730, Val Loss: 6.8854\n",
      "Epoch 77, Train Loss: 4.2259, Val Loss: 4.6629\n",
      "Epoch 78, Train Loss: 6.9829, Val Loss: 6.9645\n",
      "Epoch 79, Train Loss: 1.4853, Val Loss: 1.6653\n",
      "Epoch 80, Train Loss: 4.6986, Val Loss: 4.7152\n",
      "Epoch 81, Train Loss: 0.4453, Val Loss: 0.4919\n",
      "Epoch 82, Train Loss: 0.8125, Val Loss: 0.7669\n",
      "Epoch 83, Train Loss: 5.1204, Val Loss: 4.9726\n",
      "Epoch 84, Train Loss: 0.3439, Val Loss: 0.3455\n",
      "Epoch 85, Train Loss: 0.5291, Val Loss: 0.5364\n",
      "Epoch 86, Train Loss: 5.9492, Val Loss: 5.9998\n",
      "Epoch 87, Train Loss: 5.0682, Val Loss: 4.7498\n",
      "Epoch 88, Train Loss: 5.8952, Val Loss: 5.9286\n",
      "Epoch 89, Train Loss: 6.9288, Val Loss: 6.9286\n",
      "Epoch 90, Train Loss: 2.4718, Val Loss: 2.2777\n",
      "Epoch 91, Train Loss: 0.7845, Val Loss: 0.8725\n",
      "Epoch 92, Train Loss: 2.5289, Val Loss: 4.1154\n",
      "Epoch 93, Train Loss: 0.7519, Val Loss: 1.0609\n",
      "Epoch 94, Train Loss: 0.4823, Val Loss: 0.9255\n",
      "Epoch 95, Train Loss: 0.7500, Val Loss: 0.8556\n",
      "Epoch 96, Train Loss: 0.8455, Val Loss: 0.8188\n",
      "Epoch 97, Train Loss: 0.6435, Val Loss: 0.7948\n",
      "Epoch 98, Train Loss: 0.5660, Val Loss: 0.6860\n",
      "Epoch 99, Train Loss: 7.7743, Val Loss: 7.5472\n",
      "Epoch 100, Train Loss: 0.3074, Val Loss: 0.3175\n",
      "Epoch 101, Train Loss: 0.3949, Val Loss: 0.3938\n",
      "Epoch 102, Train Loss: 4.9843, Val Loss: 4.9876\n",
      "Epoch 103, Train Loss: 2.1635, Val Loss: 2.1198\n",
      "Epoch 104, Train Loss: 0.3157, Val Loss: 0.2935\n",
      "Epoch 105, Train Loss: 2.4240, Val Loss: 2.4601\n",
      "Epoch 106, Train Loss: 28.6153, Val Loss: 28.8408\n",
      "Epoch 107, Train Loss: 3.3917, Val Loss: 3.2515\n",
      "Epoch 108, Train Loss: 0.6453, Val Loss: 0.6655\n",
      "Epoch 109, Train Loss: 0.6818, Val Loss: 0.6638\n",
      "Epoch 110, Train Loss: 4.7091, Val Loss: 4.4952\n",
      "Epoch 111, Train Loss: 0.6303, Val Loss: 0.6083\n",
      "Epoch 112, Train Loss: 0.2832, Val Loss: 0.2589\n",
      "Epoch 113, Train Loss: 1.8374, Val Loss: 1.7432\n",
      "Epoch 114, Train Loss: 0.6307, Val Loss: 0.6635\n",
      "Epoch 115, Train Loss: 0.7739, Val Loss: 0.8699\n",
      "Epoch 116, Train Loss: 0.8953, Val Loss: 0.8926\n",
      "Epoch 117, Train Loss: 6.8051, Val Loss: 7.1689\n",
      "Epoch 118, Train Loss: 0.8022, Val Loss: 0.9984\n",
      "Epoch 119, Train Loss: 45.9431, Val Loss: 45.9937\n",
      "Epoch 120, Train Loss: 1.6056, Val Loss: 1.6060\n",
      "Epoch 121, Train Loss: 17.3103, Val Loss: 16.9697\n",
      "Epoch 122, Train Loss: 1.4038, Val Loss: 1.3837\n",
      "Epoch 123, Train Loss: 0.7103, Val Loss: 0.7118\n",
      "Epoch 124, Train Loss: 6.2023, Val Loss: 6.1870\n",
      "Epoch 125, Train Loss: 0.8431, Val Loss: 0.3890\n",
      "Epoch 126, Train Loss: 0.7231, Val Loss: 0.7618\n",
      "Epoch 127, Train Loss: 0.8810, Val Loss: 0.6963\n",
      "Epoch 128, Train Loss: 5.3738, Val Loss: 5.3418\n",
      "Epoch 129, Train Loss: 2.5962, Val Loss: 2.5088\n",
      "Epoch 130, Train Loss: 0.7251, Val Loss: 0.6836\n",
      "Epoch 131, Train Loss: 0.7878, Val Loss: 0.7890\n",
      "Epoch 132, Train Loss: 4.2158, Val Loss: 4.1770\n",
      "Epoch 133, Train Loss: 0.6116, Val Loss: 0.6144\n",
      "Epoch 134, Train Loss: 2.4937, Val Loss: 2.5016\n",
      "Epoch 135, Train Loss: 0.7459, Val Loss: 0.7236\n",
      "Epoch 136, Train Loss: 1.6263, Val Loss: 1.9240\n",
      "Epoch 137, Train Loss: 0.5208, Val Loss: 0.5888\n",
      "Epoch 138, Train Loss: 0.7598, Val Loss: 0.8477\n",
      "Epoch 139, Train Loss: 0.6876, Val Loss: 0.6073\n",
      "Epoch 140, Train Loss: 2.2487, Val Loss: 2.2886\n",
      "Epoch 141, Train Loss: 43.5999, Val Loss: 42.9915\n",
      "Epoch 142, Train Loss: 0.7303, Val Loss: 0.7517\n",
      "Epoch 143, Train Loss: 1.7787, Val Loss: 1.8045\n",
      "Epoch 144, Train Loss: 69.1100, Val Loss: 68.9752\n",
      "Epoch 145, Train Loss: 0.5596, Val Loss: 0.6107\n",
      "Epoch 146, Train Loss: 0.7375, Val Loss: 0.8048\n",
      "Epoch 147, Train Loss: 7.2883, Val Loss: 7.0933\n",
      "Epoch 148, Train Loss: 2.3082, Val Loss: 2.6167\n",
      "Epoch 149, Train Loss: 34.9188, Val Loss: 35.0182\n",
      "Epoch 150, Train Loss: 0.5945, Val Loss: 0.5991\n",
      "Epoch 151, Train Loss: 5.7379, Val Loss: 5.4053\n",
      "Epoch 152, Train Loss: 4.3083, Val Loss: 4.2832\n",
      "Epoch 153, Train Loss: 0.2956, Val Loss: 0.3151\n",
      "Epoch 154, Train Loss: 2.6912, Val Loss: 2.6882\n",
      "Epoch 155, Train Loss: 4.5412, Val Loss: 4.4896\n",
      "Epoch 156, Train Loss: 0.7765, Val Loss: 0.6468\n",
      "Epoch 157, Train Loss: 5.2699, Val Loss: 5.2922\n",
      "Epoch 158, Train Loss: 5.8712, Val Loss: 5.7183\n",
      "Epoch 159, Train Loss: 1.7786, Val Loss: 1.7799\n",
      "Epoch 160, Train Loss: 0.7165, Val Loss: 0.6770\n",
      "Epoch 161, Train Loss: 0.7827, Val Loss: 0.7813\n",
      "Epoch 162, Train Loss: 6.6325, Val Loss: 6.5940\n",
      "Epoch 163, Train Loss: 2.3417, Val Loss: 2.5931\n",
      "Epoch 164, Train Loss: 5.1947, Val Loss: 5.1849\n",
      "Epoch 165, Train Loss: 1.6867, Val Loss: 1.5889\n",
      "Epoch 166, Train Loss: 0.7646, Val Loss: 0.7550\n",
      "Epoch 167, Train Loss: 0.6320, Val Loss: 0.4408\n",
      "Epoch 168, Train Loss: 1.1552, Val Loss: 1.1283\n",
      "Epoch 169, Train Loss: 2.3403, Val Loss: 2.1437\n",
      "Epoch 170, Train Loss: 0.7367, Val Loss: 0.7203\n",
      "Epoch 171, Train Loss: 0.5854, Val Loss: 0.5544\n",
      "Epoch 172, Train Loss: 0.6348, Val Loss: 0.6390\n",
      "Epoch 173, Train Loss: 6.7168, Val Loss: 6.8277\n",
      "Epoch 174, Train Loss: 4.2454, Val Loss: 4.1852\n",
      "Epoch 175, Train Loss: 0.6598, Val Loss: 0.6690\n",
      "Epoch 176, Train Loss: 4.1813, Val Loss: 4.1316\n",
      "Epoch 177, Train Loss: 0.5436, Val Loss: 0.5555\n",
      "Epoch 178, Train Loss: 4.2306, Val Loss: 4.2157\n",
      "Epoch 179, Train Loss: 0.5483, Val Loss: 0.5258\n",
      "Epoch 180, Train Loss: 0.4519, Val Loss: 0.5045\n",
      "Epoch 181, Train Loss: 0.3405, Val Loss: 0.3469\n",
      "Epoch 182, Train Loss: 3.4795, Val Loss: 3.6962\n",
      "Epoch 183, Train Loss: 6.7962, Val Loss: 6.6819\n",
      "Epoch 184, Train Loss: 1.0376, Val Loss: 1.1135\n",
      "Epoch 185, Train Loss: 19.8190, Val Loss: 19.7632\n",
      "Epoch 186, Train Loss: 6.5670, Val Loss: 6.5555\n",
      "Epoch 187, Train Loss: 3.1281, Val Loss: 3.0594\n",
      "Epoch 188, Train Loss: 0.7672, Val Loss: 0.7430\n",
      "Epoch 189, Train Loss: 0.3773, Val Loss: 0.4158\n",
      "Epoch 190, Train Loss: 2.6608, Val Loss: 2.5379\n",
      "Epoch 191, Train Loss: 1.2443, Val Loss: 1.2488\n",
      "Epoch 192, Train Loss: 2.1332, Val Loss: 1.8911\n",
      "Epoch 193, Train Loss: 2.7059, Val Loss: 2.6916\n",
      "Epoch 194, Train Loss: 7.6658, Val Loss: 7.5639\n",
      "Epoch 195, Train Loss: 0.9375, Val Loss: 0.9841\n",
      "Epoch 196, Train Loss: 5.6022, Val Loss: 5.5806\n",
      "Epoch 197, Train Loss: 6.2718, Val Loss: 6.1359\n",
      "Epoch 198, Train Loss: 6.1385, Val Loss: 5.8227\n",
      "Epoch 199, Train Loss: 1.2709, Val Loss: 1.1684\n",
      "Epoch 200, Train Loss: 0.7353, Val Loss: 0.7346\n",
      "Epoch 201, Train Loss: 0.8279, Val Loss: 0.8462\n",
      "Epoch 202, Train Loss: 0.6296, Val Loss: 0.6560\n",
      "Epoch 203, Train Loss: 1.9478, Val Loss: 1.9497\n",
      "Epoch 204, Train Loss: 0.3687, Val Loss: 0.4106\n",
      "Epoch 205, Train Loss: 5.6121, Val Loss: 5.6164\n",
      "Epoch 206, Train Loss: 85.9292, Val Loss: 85.8557\n",
      "Epoch 207, Train Loss: 54.3694, Val Loss: 54.4952\n",
      "Epoch 208, Train Loss: 0.7649, Val Loss: 0.6087\n",
      "Epoch 209, Train Loss: 0.6228, Val Loss: 0.7606\n",
      "Epoch 210, Train Loss: 5.5051, Val Loss: 5.4905\n",
      "Epoch 211, Train Loss: 0.7596, Val Loss: 0.7784\n",
      "Epoch 212, Train Loss: 0.6660, Val Loss: 0.8539\n",
      "Epoch 213, Train Loss: 0.8257, Val Loss: 0.8579\n",
      "Epoch 214, Train Loss: 0.7358, Val Loss: 0.7631\n",
      "Epoch 215, Train Loss: 1.5914, Val Loss: 1.6992\n",
      "Epoch 216, Train Loss: 6.2458, Val Loss: 6.3696\n",
      "Epoch 217, Train Loss: 73.1875, Val Loss: 73.0410\n",
      "Epoch 218, Train Loss: 0.3900, Val Loss: 0.4239\n",
      "Epoch 219, Train Loss: 30.6623, Val Loss: 30.6840\n",
      "Epoch 220, Train Loss: 0.7666, Val Loss: 0.7768\n",
      "Epoch 221, Train Loss: 0.6773, Val Loss: 0.8015\n",
      "Epoch 222, Train Loss: 0.7928, Val Loss: 0.8942\n",
      "Epoch 223, Train Loss: 37.4451, Val Loss: 37.4295\n",
      "Epoch 224, Train Loss: 0.8012, Val Loss: 0.8216\n",
      "Epoch 225, Train Loss: 3.4609, Val Loss: 3.8062\n",
      "Epoch 226, Train Loss: 0.7050, Val Loss: 0.7424\n",
      "Epoch 227, Train Loss: 0.8389, Val Loss: 1.0465\n",
      "Epoch 228, Train Loss: 5.1607, Val Loss: 4.9565\n",
      "Epoch 229, Train Loss: 6.3433, Val Loss: 6.3056\n",
      "Epoch 230, Train Loss: 4.5255, Val Loss: 4.4743\n",
      "Epoch 231, Train Loss: 7.2189, Val Loss: 7.1047\n",
      "Epoch 232, Train Loss: 6.8905, Val Loss: 6.8659\n",
      "Epoch 233, Train Loss: 0.6713, Val Loss: 0.6715\n",
      "Epoch 234, Train Loss: 6.0005, Val Loss: 5.9406\n",
      "Epoch 235, Train Loss: 7.5993, Val Loss: 7.5877\n",
      "Epoch 236, Train Loss: 2.3787, Val Loss: 2.3727\n",
      "Epoch 237, Train Loss: 5.7727, Val Loss: 5.7715\n",
      "Epoch 238, Train Loss: 61.7701, Val Loss: 61.6124\n",
      "Epoch 239, Train Loss: 0.7539, Val Loss: 0.8959\n",
      "Epoch 240, Train Loss: 3.7633, Val Loss: 3.8335\n",
      "Epoch 241, Train Loss: 0.4086, Val Loss: 0.6449\n",
      "Epoch 242, Train Loss: 0.5929, Val Loss: 0.5243\n",
      "Epoch 243, Train Loss: 2.0222, Val Loss: 2.5237\n",
      "Epoch 244, Train Loss: 2.0189, Val Loss: 2.1177\n",
      "Epoch 245, Train Loss: 2.4431, Val Loss: 2.6293\n",
      "Epoch 246, Train Loss: 0.8470, Val Loss: 0.7895\n",
      "Epoch 247, Train Loss: 1.3928, Val Loss: 1.4009\n",
      "Epoch 248, Train Loss: 23.1356, Val Loss: 23.0648\n",
      "Epoch 249, Train Loss: 3.4649, Val Loss: 3.9007\n",
      "Epoch 250, Train Loss: 0.7466, Val Loss: 0.6769\n",
      "Epoch 251, Train Loss: 0.6878, Val Loss: 0.6425\n",
      "Epoch 252, Train Loss: 77.5711, Val Loss: 77.1476\n",
      "Epoch 253, Train Loss: 7.0782, Val Loss: 7.0608\n",
      "Epoch 254, Train Loss: 0.3697, Val Loss: 0.6717\n",
      "Epoch 255, Train Loss: 0.7564, Val Loss: 0.8329\n",
      "Epoch 256, Train Loss: 6.0265, Val Loss: 6.3882\n",
      "Epoch 257, Train Loss: 0.6341, Val Loss: 0.6379\n",
      "Epoch 258, Train Loss: 0.2806, Val Loss: 0.5743\n",
      "Epoch 259, Train Loss: 1.4938, Val Loss: 1.9391\n",
      "Epoch 260, Train Loss: 1.2550, Val Loss: 1.2583\n",
      "Epoch 261, Train Loss: 4.5942, Val Loss: 4.6033\n",
      "Epoch 262, Train Loss: 0.7430, Val Loss: 0.7285\n",
      "Epoch 263, Train Loss: 0.8769, Val Loss: 1.0306\n",
      "Epoch 264, Train Loss: 5.1154, Val Loss: 5.0494\n",
      "Epoch 265, Train Loss: 4.5977, Val Loss: 4.6011\n",
      "Epoch 266, Train Loss: 1.8826, Val Loss: 1.8769\n",
      "Epoch 267, Train Loss: 1.2946, Val Loss: 1.2752\n",
      "Epoch 268, Train Loss: 40.2460, Val Loss: 40.1678\n",
      "Epoch 269, Train Loss: 2.1326, Val Loss: 2.4306\n",
      "Epoch 270, Train Loss: 7.3438, Val Loss: 7.3810\n",
      "Epoch 271, Train Loss: 51.6261, Val Loss: 51.4778\n",
      "Epoch 272, Train Loss: 0.3373, Val Loss: 0.3188\n",
      "Epoch 273, Train Loss: 0.8653, Val Loss: 0.9730\n",
      "Epoch 274, Train Loss: 6.1694, Val Loss: 6.1726\n",
      "Epoch 275, Train Loss: 6.4254, Val Loss: 6.4249\n",
      "Epoch 276, Train Loss: 0.7757, Val Loss: 0.9682\n",
      "Epoch 277, Train Loss: 2.8296, Val Loss: 2.9344\n",
      "Epoch 278, Train Loss: 4.2926, Val Loss: 4.2819\n",
      "Epoch 279, Train Loss: 1.3021, Val Loss: 1.9063\n",
      "Epoch 280, Train Loss: 1.5224, Val Loss: 1.7056\n",
      "Epoch 281, Train Loss: 0.6712, Val Loss: 0.6707\n",
      "Epoch 282, Train Loss: 5.1833, Val Loss: 5.1820\n",
      "Epoch 283, Train Loss: 2.5186, Val Loss: 2.8982\n",
      "Epoch 284, Train Loss: 0.6807, Val Loss: 0.6734\n",
      "Epoch 285, Train Loss: 0.7898, Val Loss: 0.7709\n"
     ]
    }
   ],
   "source": [
    "#attemt 1\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, input_channels, hidden_channels, output_channels, num_layers):\n",
    "        super(GCN, self).__init__()\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        self.bns = torch.nn.ModuleList()\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # input layer\n",
    "        self.convs.append(GCNConv(input_channels, hidden_channels))\n",
    "        self.bns.append(torch.nn.BatchNorm1d(hidden_channels))\n",
    "\n",
    "        # hidden layers\n",
    "        for i in range(num_layers - 2):\n",
    "            self.convs.append(GCNConv(hidden_channels, hidden_channels))\n",
    "            self.bns.append(torch.nn.BatchNorm1d(hidden_channels))\n",
    "\n",
    "        # output layer\n",
    "        self.convs.append(GCNConv(hidden_channels, output_channels))\n",
    "        self.bns.append(torch.nn.BatchNorm1d(output_channels))\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        for i in range(self.num_layers - 1):\n",
    "            x = self.convs[i](x, edge_index)\n",
    "            x = self.bns[i](x)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, p=0.5, training=self.training)\n",
    "        # output layer\n",
    "        x = self.convs[-1](x, edge_index)\n",
    "        return global_mean_pool(x, torch.zeros(x.shape[0], dtype=torch.long))\n",
    "\n",
    "# assume dataX, dataY, dataEdgeIndex are given\n",
    "\n",
    "input_channels = 3  # number of features for each node\n",
    "hidden_channels = 64\n",
    "output_channels = 1  # number of output features for each node\n",
    "num_layers = 5  # number of graph convolutional layers in the network\n",
    "\n",
    "model = GCN(input_channels, hidden_channels, output_channels, num_layers)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "for i in range(len(dataX)):\n",
    "    x = torch.tensor(dataX[i], dtype=torch.float)\n",
    "    y = torch.tensor(dataY[i], dtype=torch.float)\n",
    "    edge_index = torch.tensor(dataEgdeIndex, dtype=torch.long).t().contiguous()\n",
    "\n",
    "    # set model to training mode\n",
    "    model.train()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    out = model(x, edge_index)\n",
    "    loss = F.mse_loss(out, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # set model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        out = model(x, edge_index)\n",
    "        val_loss = F.mse_loss(out, y)\n",
    "\n",
    "    print(f'Epoch {i}, Train Loss: {loss.item():.4f}, Val Loss: {val_loss.item():.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the graph edges\n",
    "edges = [(i, i+1) for i in range(19)]\n",
    "\n",
    "# Define the node positions\n",
    "node_positions = dataX[250]\n",
    "node_positions = {i: (pos[1], pos[2]) for i, pos in enumerate(node_positions)}\n",
    "\n",
    "# Create the graph object and set node positions\n",
    "G = nx.Graph()\n",
    "G.add_edges_from(edges)\n",
    "nx.set_node_attributes(G, node_positions, 'pos')\n",
    "\n",
    "# Draw the graph\n",
    "pos = nx.get_node_attributes(G, 'pos')\n",
    "nx.draw(G, pos, with_labels=True)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
