{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch-geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch.nn import Parameter\n",
    "from torch_geometric.nn.conv import MessagePassing\n",
    "from torch_geometric.utils import remove_self_loops, add_self_loops, softmax\n",
    "import torch.optim as optim\n",
    "from torch_geometric.nn.conv import feast_conv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc --version\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device_name = torch.device(\"cuda\")\n",
    "else:\n",
    "    device_name = torch.device('cpu')\n",
    "\n",
    "print(\"Using {}.\".format(device_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "\n",
    "#Import data function\n",
    "def get_data(filepath):\n",
    "    data = np.loadtxt(filepath)\n",
    "    x = data[:, :20]\n",
    "    z = data[:, 20:40]\n",
    "    m = data[:, 40:60]\n",
    "\n",
    "    dataX = []\n",
    "    for i in range(x.shape[0]):\n",
    "        dataXs = np.column_stack((np.arange(20), x[i], z[i]))\n",
    "        dataX.append(dataXs)\n",
    "\n",
    "    dataY = m\n",
    "    zipped = list(zip(dataX, dataY))\n",
    "    np.random.shuffle(zipped)\n",
    "    dataX, dataY = zip(*zipped)\n",
    "\n",
    "    dataX = np.array(dataX)\n",
    "    dataY = np.array(dataY)\n",
    "\n",
    "    dataEdgeIndex = np.column_stack((np.arange(20)[:-1], np.arange(20)[1:]))\n",
    "    dataEdgeIndex = np.vstack((dataEdgeIndex, dataEdgeIndex[:, ::-1])).T\n",
    "\n",
    "    dataset = [Data(x=torch.from_numpy(x).float(), edge_index=torch.from_numpy(dataEdgeIndex).long(), y=torch.from_numpy(y).float()) for x, y in zip(dataX, dataY)]\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "#Getting data\n",
    "dataset = get_data(\"/content/drive/MyDrive/datasets/data1.txt\")\n",
    "print(len(dataset))\n",
    "#Splitting dataset\n",
    "datasetRun = dataset[:int(len(dataset)*0.90)]\n",
    "datasetTest = dataset[int(len(dataset)*0.90):]\n",
    "\n",
    "train_loader = datasetRun[:int(len(datasetRun)*0.75)]\n",
    "test_loader = datasetRun[int(len(datasetRun)*0.75):]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Checks\n",
    "\n",
    "print(len(train_loader))\n",
    "print(len(test_loader))\n",
    "\n",
    "# one data object from train_loader:\n",
    "graph = train_loader[0]\n",
    "print(graph.x.shape)\n",
    "print(graph.edge_index.shape)\n",
    "print(graph.y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ArchNN\n",
    "\n",
    "class ArchNN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, num_classes, heads, t_inv = True, l1_lambda=0.001):\n",
    "        super(ArchNN, self).__init__()\n",
    "        self.fc0 = nn.Linear(in_channels, 16) \n",
    "        #self.bn0 = nn.BatchNorm1d(16)\n",
    "        self.conv1 = feast_conv.FeaStConv(16, 32, heads=heads, t_inv=t_inv)\n",
    "        self.conv2 = feast_conv.FeaStConv(32, 64, heads=heads, t_inv=t_inv)\n",
    "        self.conv3 = feast_conv.FeaStConv(64, 128, heads=heads, t_inv=t_inv)\n",
    "        #self.conv4 = feast_conv.FeaStConv(128, 256, heads=heads, t_inv=t_inv)\n",
    "        self.fc1 = nn.Linear(128, 256)\n",
    "        #self.bn1 = nn.BatchNorm1d(256) \n",
    "        self.fc2 = nn.Linear(256, 1)\n",
    "\n",
    "        self.reset_parameters()\n",
    "        self.l1_lambda = l1_lambda\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.conv1.reset_parameters()\n",
    "        self.conv2.reset_parameters()\n",
    "        self.conv3.reset_parameters()\n",
    "        #self.conv4.reset_parameters()\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = F.elu(self.fc0(x))\n",
    "        x = F.elu(self.conv1(x, edge_index))\n",
    "        x = F.elu(self.conv2(x, edge_index))\n",
    "        x = F.elu(self.conv3(x, edge_index))\n",
    "        #x = F.elu(self.conv4(x, edge_index))\n",
    "        x = F.elu(self.fc1(x))\n",
    "        #x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        #F.log_softmax(x, dijm=1)\n",
    "        x = torch.squeeze(x, dim=1)\n",
    "        return x\n",
    "\n",
    "    \"\"\"def compute_loss(self, outputs, targets):\n",
    "      # Define the loss function\n",
    "      mse_loss = nn.MSELoss()\n",
    "      l1_loss = torch.tensor(0.0, requires_grad=True).to(outputs.device)  # initialize L1 loss to 0\n",
    "      for name, param in self.named_parameters():\n",
    "          if 'weight' in name:  # apply L1 regularization only to weight parameters\n",
    "              l1_loss += torch.norm(param, p=1)  # compute element-wise L1 norm\n",
    "      loss = mse_loss(outputs, targets) + self.l1_lambda * l1_loss  # add L1 regularization term to the loss\n",
    "      return loss\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import time\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def print_info(info):\n",
    "    message = ('Epoch: {}/{}, Duration: {:.3f}s,'\n",
    "               'Train Loss: {:.4f}, Test Loss:{:.4f}').format(\n",
    "                   info['current_epoch'], info['epochs'], info['t_duration'],\n",
    "                   info['train_loss'], info['test_loss'])\n",
    "    print(message)\n",
    "\n",
    "\n",
    "def run(model, train_loader, test_loader, num_nodes, epochs, optimizer, device):\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        #random.shuffle(train_loader) #added this this see if it helps, tihihi\n",
    "        #random.shuffle(test_loader)  #added this this see if it helps, tihihi\n",
    "        t = time.time()\n",
    "        train_loss = train(model, train_loader, optimizer, device)\n",
    "        t_duration = time.time() - t\n",
    "        test_loss = test(model, test_loader, num_nodes, device)\n",
    "        eval_info = {\n",
    "            'train_loss': train_loss,\n",
    "            'test_loss': test_loss,\n",
    "            'current_epoch': epoch,\n",
    "            'epochs': epochs,\n",
    "            't_duration': t_duration\n",
    "        }\n",
    "\n",
    "        print_info(eval_info)\n",
    "\n",
    "\n",
    "def train(model, train_loader, optimizer, device):\n",
    "    model.train()\n",
    "\n",
    "    total_loss = 0\n",
    "    for idx, data in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data.to(device))\n",
    "        #log_probs = F.log_softmax(output, dim=1).double()\n",
    "        #print(log_probs.dtype)\n",
    "        #print(data.y.dtype)\n",
    "        loss = F.mse_loss(output, data.y.to(device))\n",
    "\n",
    "        #for i in range(data.num_nodes):\n",
    "        #    loss = F.mse_loss(output[i], data.y[i].to(device))\n",
    "        #    loss.backward(retain_graph=True)\n",
    "\n",
    "        #loss = model.compute_loss(output, data.y)  # compute loss with L1 regularization\n",
    "        #loss = F.nll_loss(log_probs, data.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "\n",
    "def test(model, test_loader, num_nodes, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total_loss = 0\n",
    "    n_graphs = 0\n",
    "    with torch.no_grad():\n",
    "        for idx, data in enumerate(test_loader):\n",
    "            out = model(data.to(device))\n",
    "            total_loss += F.mse_loss(out, data.y.to(device)).item()\n",
    "            #pred = out.max(1)[1]\n",
    "            #correct += pred.eq(data.y).sum().item()\n",
    "            #n_graphs += data.num_graphs\n",
    "    return total_loss / len(test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#runner\n",
    "num_nodes = train_loader[0].x.shape[0]\n",
    "num_features = train_loader[0].x.shape[1]\n",
    "\n",
    "model = ArchNN(num_features, num_nodes, heads=8).to(device_name)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0005, weight_decay=0.0005)\n",
    "\n",
    "\n",
    "run(model, train_loader, test_loader, num_nodes, 50, optimizer, device_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#More epochs per favoure?\n",
    "num_nodes = train_loader[0].x.shape[0]\n",
    "num_features = train_loader[0].x.shape[1]\n",
    "\n",
    "run(model, train_loader, test_loader, num_nodes, 25, optimizer, device_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test for new dataline\n",
    "\n",
    "def testData(line):\n",
    "    real = datasetTest[line].y\n",
    "    out = model(datasetTest[line].to(device_name))\n",
    "    Bool = ''\n",
    "    print('{:<10}{:<10}{:<10}{:<10}'.format('real', 'out', '%error', 'Sign'))\n",
    "    for i in range(len(out)):\n",
    "        error = ((out-real[line])/out)*100\n",
    "        if torch.sign(real[i]) == torch.sign(datasetTest[line].y[i]):\n",
    "            Bool = 'True'\n",
    "        else:\n",
    "            Bool = 'False'\n",
    "        real_formatted = '{:<10.2f}'.format(round(real[i].item(),2))\n",
    "        out_formatted = '{:<10.2f}'.format(round(out[i].item(),2))\n",
    "        error_formatted = '{:<10.2f}'.format(round(error[i].item(),2))\n",
    "        list_formatted = [real_formatted, out_formatted, error_formatted, Bool]\n",
    "        print('{:<10}{:<10}{:<10}{:<10}'.format(*list_formatted))\n",
    "\n",
    "#Test\n",
    "testData(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test for new dataset\n",
    "def testFinal(model, indata, device):\n",
    "    model.eval()\n",
    "    #correct = 0\n",
    "   # total_loss = 0\n",
    "   # n_graphs = 0\n",
    "    errorAVG = 0\n",
    "    with torch.no_grad():\n",
    "        for id, data in enumerate(indata):\n",
    "          out = model(data.to(device))\n",
    "          real = data.y.to(device)\n",
    "          errorList = []\n",
    "          error = 0\n",
    "          for i in range(len(out)):\n",
    "            errorList.append(round(((out[i].item() - real[i].item() ) / out[i].item())*100, 2))\n",
    "            error += abs(((out[i].item() - real[i].item() ) / out[i].item())*100)\n",
    "\n",
    "          errorAVG += round(error/len(out),2)\n",
    "          print(errorList)\n",
    "          print('Avarage error: '+ str(round(error/len(out),2))+'%')\n",
    "          print('Mean Square: '+ str(round(F.mse_loss(out, data.y.to(device)).item(),2))+'\\n')\n",
    "    \n",
    "    print('The avarage error is ' + str(errorAVG/len(indata))+'%')\n",
    "\n",
    "\n",
    "#Load test data set, if this is not done from beginning!\n",
    "#datasetTest = get_data(\"/content/drive/MyDrive/datasets/data1test.txt\")\n",
    "\n",
    "#Run testFinal\n",
    "testFinal(model, datasetTest,  device_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test runner\n",
    "num_nodes = train_loader[0].x.shape[0]\n",
    "num_features = train_loader[0].x.shape[1]\n",
    "\n",
    "modelTest = ArchNN(num_features, num_nodes, heads=8).to(device_name)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.005)\n",
    "\n",
    "\n",
    "run(modelTest, train_loader, test_loader, num_nodes, 5, optimizer, device_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
